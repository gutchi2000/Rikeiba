# keiba_web_app_final.py
# ã‚µã‚¤ãƒ‰ãƒãƒ¼äº’æ›ï¼ˆexpanderï¼‰ã€ç¸¦è»¸å …ç‰¢åŒ–ã€å¹´é½¢/æ é‡ã¿ãƒ»MCãƒ»è¡€çµ±HTML å®Œå‚™
# + å³/å·¦å›ã‚Š è‡ªå‹•åˆ¤å®š&åŠ ç‚¹ + AR100/Bandã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆBä¸­å¿ƒåŒ–ï¼‰+ è¦‹é€ã‚Šãƒ­ã‚¸ãƒƒã‚¯
# + â˜… å…ˆé ­ã®ç©ºè¡Œã‚’ç¢ºå®Ÿã«é™¤å» / 4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã®å®‰å…¨åŒ–
# + â˜… è„šè³ªã‚¨ãƒ‡ã‚£ã‚¿ã®å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ãƒ»å¾©å…ƒï¼ˆãƒªãƒ©ãƒ³ã—ã¦ã‚‚æ¶ˆãˆãªã„ï¼‰
# + â˜… è„šè³ªã®è¡¨è¨˜ã‚†ã‚Œã‚’å¸åï¼ˆã€Œè¿½ã„è¾¼ã¿ã€ã€Œå·®è¾¼ã€ãªã©â†’æ­£è¦åŒ–ï¼‰
# + â˜… ndcg_by_race ã‚’å …ç‰¢å®Ÿè£…ã§ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«å®šç¾©ï¼ˆé‡è¤‡import/ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆå´©ã‚Œä¿®æ­£ï¼‰
# + â˜… 18æ¡ race_id â†’ NetKeiba 12æ¡ ã«è‡ªå‹•å¤‰æ›ï¼ˆå­¦ç¿’å‰å‡¦ç†ï¼‰
# + â˜… ã€ç½®ãæ›ãˆã€‘å‹ç‡ã¯ Plackettâ€“Luceï¼ˆsoftmaxï¼‰ã§è§£æè¨ˆç®—ã€Top3ã¯è»½é‡MCã€‚ç­‰æ¸©å›å¸°ã§ç¢ºç‡æ ¡æ­£ï¼ˆä»»æ„ï¼‰ã€‚
# + â˜… ã€å‰Šé™¤äºˆå®šã€‘LightGBMå­¦ç¿’UI/ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾ŒåŠã§å®Œå…¨å‰Šé™¤ï¼†è©•ä¾¡ã‚¿ãƒ–ã¸å·®ã—æ›¿ãˆï¼‰
# app.pyï¼ˆã‚ãªãŸã®æ—¢å­˜ã‚¢ãƒ—ãƒªï¼‰


import streamlit as st
import pandas as pd
import numpy as np
import re, io, json
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib.patches import Wedge, Rectangle, Circle
from itertools import combinations

# ===== Optional: Altairï¼ˆæ•£å¸ƒå›³ï¼‰ =====
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

# ===== Optional: HTML åŸ‹ã‚è¾¼ã¿ï¼ˆè¡€çµ±è¡¨ç¤ºç”¨ï¼‰ =====
try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

# ===== Optional: ç¢ºç‡æ ¡æ­£ï¼ˆç­‰æ¸©å›å¸° / Plattç”¨ãƒ­ã‚¸ãƒƒãƒˆï¼‰ =====
try:
    from sklearn.isotonic import IsotonicRegression
    SK_ISO = True
except Exception:
    SK_ISO = False

try:
    from sklearn.linear_model import LogisticRegression
    SK_PLATT = True
except Exception:
    SK_PLATT = False

# ---- åŸºæœ¬è¨­å®šã¨ãƒ•ã‚©ãƒ³ãƒˆ ----
# ---- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆç½®ãæ›ãˆï¼‰----
import os
from matplotlib import font_manager

@st.cache_resource
def get_jp_font():
    # åŒæ¢± or OSã®ä»£è¡¨çš„ãªå ´æ‰€ã‚’é †ã«æ¢ç´¢
    candidates = [
        "ipaexg.ttf",  # ãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã«ç½®ã„ãŸå ´åˆï¼ˆæ¨å¥¨ï¼šIPAexGothicï¼‰
        "/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        "/System/Library/Fonts/Hiragino Sans W3.ttc",
        "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
        "C:/Windows/Fonts/meiryo.ttc",
    ]
    for p in candidates:
        if os.path.exists(p):
            try:
                font_manager.fontManager.addfont(p)  # â† å®Ÿä½“ã‚’ç™»éŒ²
            except Exception:
                pass
            return font_manager.FontProperties(fname=p)
    return None

jp_font = get_jp_font()

import matplotlib.pyplot as plt
plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®è±†è…å¯¾ç­–
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'IPAexGothic', 'IPAGothic', 'Noto Sans CJK JP',
    'Yu Gothic UI', 'Meiryo', 'Hiragino Sans', 'MS Gothic'
]

# è¦‹ã¤ã‹ã£ãŸå®Ÿãƒ•ã‚©ãƒ³ãƒˆåã‚’å„ªå…ˆã§é©ç”¨
if jp_font is not None:
    try:
        plt.rcParams['font.family'] = jp_font.get_name()
    except Exception:
        pass

st.set_page_config(page_title="ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰", layout="wide")

# ---- ä¾¿åˆ©CSSï¼ˆsidebar å¹…ã ã‘èª¿æ•´ï¼‰----
# æ â†’HEX
# â”€â”€ æ ã®è‰²ï¼ˆè¿‘ä¼¼ï¼šJRA/NetKeibaã®æ è‰²ï¼‰
def _waku_hex(v: int) -> str:
    palette = {1:"#ffffff",2:"#000000",3:"#e6002b",4:"#1560bd",5:"#ffd700",6:"#00a04b",7:"#ff7f27",8:"#f19ec2"}
    return palette.get(int(v), "#ffffff")

def _style_waku(s: pd.Series):
    out = []
    for v in s:
        if pd.isna(v):
            out.append("")
        else:
            v = int(v)
            bg = _waku_hex(v)
            fg = "#000" if v == 1 else "#fff"   # â† 1æ ã¯é»’æ–‡å­—ã€ãã‚Œä»¥å¤–ã¯ç™½æ–‡å­—
            out.append(f"background-color:{bg}; color:{fg}; font-weight:700; text-align:center;")
    return out

STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fwid = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

# === NEW: è„šè³ªã®è¡¨è¨˜ã‚†ã‚Œå¸åï¼ˆæ­£è¦åŒ–é–¢æ•°ï¼‰ ===
STYLE_ALIASES = {
    'è¿½ã„è¾¼ã¿':'è¿½è¾¼','è¿½è¾¼ã¿':'è¿½è¾¼','ãŠã„ã“ã¿':'è¿½è¾¼','ãŠã„è¾¼ã¿':'è¿½è¾¼',
    'ã•ã—':'å·®ã—','å·®è¾¼':'å·®ã—','å·®è¾¼ã¿':'å·®ã—',
    'ã›ã‚“ã“ã†':'å…ˆè¡Œ','å…ˆè¡Œ ':'å…ˆè¡Œ','å…ˆè¡Œã€€':'å…ˆè¡Œ',
    'ã«ã’':'é€ƒã’','é€ƒã’ ':'é€ƒã’','é€ƒã’ã€€':'é€ƒã’'
}
def normalize_style(s: str) -> str:
    s = str(s).replace('ã€€','').strip().translate(_fwid)
    s = STYLE_ALIASES.get(s, s)
    return s if s in STYLES else ''

# ======================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========================
def season_of(month: int) -> str:
    if 3 <= month <= 5: return 'æ˜¥'
    if 6 <= month <= 8: return 'å¤'
    if 9 <= month <= 11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    std = s.std(ddof=0)
    if std == 0 or pd.isna(std):
        return pd.Series([50]*len(s), index=s.index)
    return 50 + 10 * (s - s.mean()) / std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + float('0.'+m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + int(m.group(3))/10
    try:  return float(s)
    except: return np.nan

def normalize_grade_text(x: str | None) -> str | None:
    if x is None or (isinstance(x, float) and np.isnan(x)): 
        return None
    s = str(x).translate(_fwid)
    s = (s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
         .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s = re.sub(r'G\s*III', 'G3', s, flags=re.I)
    s = re.sub(r'G\s*II',  'G2', s, flags=re.I)
    s = re.sub(r'G\s*I',   'G1', s, flags=re.I)
    s = re.sub(r'ï¼ªï¼°ï¼®', 'Jpn', s, flags=re.I)
    s = re.sub(r'JPN',    'Jpn', s, flags=re.I)
    s = re.sub(r'Jpn\s*III', 'Jpn3', s, flags=re.I)
    s = re.sub(r'Jpn\s*II',  'Jpn2', s, flags=re.I)
    s = re.sub(r'Jpn\s*I',   'Jpn1', s, flags=re.I)
    m = re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def safe_take(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    return df[[c for c in cols if c in df.columns]].copy()

def _trim_name(x):
    try:
        return str(x).replace('\u3000',' ').strip()
    except Exception:
        return str(x)

# å®‰å®šã—ãŸé‡ã¿ä»˜ãæ¨™æº–åå·®ï¼ˆä¸åè£œæ­£ã¤ãï¼‰
def w_std_unbiased(x, w, ddof=1):
    x = np.asarray(x, dtype=float)
    w = np.asarray(w, dtype=float)
    sw = w.sum()
    if not np.isfinite(sw) or sw <= 0: return np.nan
    m = np.sum(w * x) / sw
    var = np.sum(w * (x - m)**2) / sw
    n_eff = (sw**2) / np.sum(w**2) if np.sum(w**2) > 0 else 0
    if ddof and n_eff > ddof:
        var *= n_eff / (n_eff - ddof)
    return float(np.sqrt(max(var, 0.0)))
# --- è·é›¢Ã—å›ã‚Š è¿‘å‚åŒ–ã®è£œåŠ© ---
def _kish_neff(w: np.ndarray) -> float:
    w = np.asarray(w, float)
    sw = w.sum()
    s2 = np.sum(w ** 2)
    return float((sw * sw) / s2) if s2 > 0 else 0.0

def _nw_mean(x, y, w, h):
    # Nadarayaâ€“Watson (Gaussian kernel)
    x = np.asarray(x, float); y = np.asarray(y, float); w = np.asarray(w, float)
    if len(x) == 0: return np.nan
    K = np.exp(-0.5 * (x / max(1e-9, h)) ** 2) * w
    sK = K.sum()
    return float((K * y).sum() / sK) if sK > 0 else np.nan

def _dist_turn_profile_for_horse(
    name: str, df_hist: pd.DataFrame, target_distance: int, target_turn: str, surface: str,
    h_m: float, opp_turn_w: float, prior_mode: str, tau: float,
    grid_step: int, grid_span_m: int, df_agg_for_prior: pd.DataFrame
):
    g = df_hist[df_hist['é¦¬å'].astype(str).str.strip() == str(name)].copy()
    # è·é›¢/å›ã‚Š/é¦¬å ´ã®å‰å‡¦ç†
    g['è·é›¢'] = pd.to_numeric(g['è·é›¢'], errors='coerce')
    g = g[g['è·é›¢'].notna() & (g['è·é›¢'] > 0)]
    if 'é¦¬å ´' in g.columns:
        g = g[g['é¦¬å ´'].astype(str) == str(surface)]
    if 'å›ã‚Š' not in g.columns or g.empty:
        return {'DistTurnZ': np.nan, 'n_eff_turn': 0.0, 'BestDist_turn': np.nan, 'DistTurnZ_best': np.nan}

    # åŸºæœ¬é‡ã¿ï¼šæ™‚é–“æ¸›è¡°ï¼ˆ_wï¼‰Ã— å›ã‚Šä¿‚æ•° Ã— 1
    w_time = pd.to_numeric(g.get('_w', 1.0), errors='coerce').fillna(1.0).to_numpy(float)
    w_turn = np.where(g['å›ã‚Š'].astype(str) == str(target_turn), 1.0, float(opp_turn_w))
    w0 = w_time * w_turn

    x_dist = g['è·é›¢'].to_numpy(float)
    y_z    = pd.to_numeric(g['score_norm'], errors='coerce').to_numpy(float)
    msk    = np.isfinite(x_dist) & np.isfinite(y_z) & np.isfinite(w0)
    x_dist, y_z, w0 = x_dist[msk], y_z[msk], w0[msk]
    if x_dist.size == 0:
        return {'DistTurnZ': np.nan, 'n_eff_turn': 0.0, 'BestDist_turn': np.nan, 'DistTurnZ_best': np.nan}

    # äº‹å‰å¹³å‡ Î¼0
    mu0 = np.nan
    if prior_mode == "WAvgZãƒ™ãƒ¼ã‚¹":
        mu0 = float(df_agg_for_prior.set_index('é¦¬å').get('WAvgZ', pd.Series()).get(name, np.nan))
    elif prior_mode == "Right/LeftZãƒ™ãƒ¼ã‚¹":
        if str(target_turn) == 'å³':
            mu0 = float(df_agg_for_prior.set_index('é¦¬å').get('RightZ', pd.Series()).get(name, np.nan))
        else:
            mu0 = float(df_agg_for_prior.set_index('é¦¬å').get('LeftZ', pd.Series()).get(name, np.nan))
        if not np.isfinite(mu0):
            mu0 = float(df_agg_for_prior.set_index('é¦¬å').get('WAvgZ', pd.Series()).get(name, np.nan))

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè·é›¢ã§ã®å¾Œå¹³å‡ï¼ˆæ“¬ä¼¼ãƒ™ã‚¤ã‚º: Ï„ ã§äº‹å‰ã‚’æ··åˆï¼‰
    z_hat_t = _nw_mean(x_dist - float(target_distance), y_z, w0, h_m)
    w_eff_t = _kish_neff(np.exp(-0.5 * ((x_dist - float(target_distance))/max(1e-9, h_m))**2) * w0)

    if np.isfinite(mu0) and tau > 0:
        if np.isfinite(z_hat_t):
            z_hat_t = (w_eff_t * z_hat_t + tau * mu0) / max(1e-9, (w_eff_t + tau))
        else:
            z_hat_t = mu0

    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èµ°æŸ»ï¼ˆãƒ™ã‚¹ãƒˆè·é›¢ã®å‚è€ƒç”¨ï¼‰
    ds = np.arange(int(target_distance - grid_span_m),
                   int(target_distance + grid_span_m) + 1, int(grid_step))
    best_d, best_val = np.nan, -1e18
    for d0 in ds:
        z = _nw_mean(x_dist - float(d0), y_z, w0, h_m)
        if np.isfinite(mu0) and tau > 0:
            # ãƒ™ã‚¤ã‚ºæ··åˆï¼ˆæœ‰åŠ¹æœ¬æ•°ã‚’ç–‘ä¼¼ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦åŠ ç®—ï¼‰
            we = _kish_neff(np.exp(-0.5 * ((x_dist - float(d0))/max(1e-9, h_m))**2) * w0)
            z = (we * z + tau * mu0) / max(1e-9, (we + tau)) if np.isfinite(z) else mu0
        if np.isfinite(z) and z > best_val:
            best_val, best_d = float(z), int(d0)

    return {
        'DistTurnZ': float(z_hat_t) if np.isfinite(z_hat_t) else np.nan,
        'n_eff_turn': float(w_eff_t),
        'BestDist_turn': float(best_d) if np.isfinite(best_d) else np.nan,
        'DistTurnZ_best': float(best_val) if np.isfinite(best_val) else np.nan
    }

@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    return s0, s1

def validate_inputs(df_score: pd.DataFrame, horses: pd.DataFrame):
    problems = []
    req = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
    for c in req:
        if c not in df_score.columns:
            problems.append(f"sheet0 å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {c}")
    if 'æ–¤é‡' in horses:
        bad = horses['æ–¤é‡'].dropna()
        if len(bad)>0 and ((bad<45)|(bad>65)).any():
            problems.append("sheet1 æ–¤é‡ãŒãƒ¬ãƒ³ã‚¸å¤–ï¼ˆ45â€“65ï¼‰")
    if {'é€šé4è§’','é ­æ•°'}.issubset(df_score.columns):
        tmp = df_score[['é€šé4è§’','é ­æ•°']].dropna()
        if len(tmp)>0 and ((tmp['é€šé4è§’']<1) | (tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
            problems.append("sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–")
    if problems: st.warning("âš  å…¥åŠ›ãƒã‚§ãƒƒã‚¯ï¼š\n- " + "\n- ".join(problems))

# ===== å³/å·¦å›ã‚Šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
DEFAULT_VENUE_TURN = {
    'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦',
    'ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'
}
VENUE_SYNONYM_TO_VENUE = {'åºœä¸­':'æ±äº¬','æ·€':'äº¬éƒ½'}

def _normalize_turn_table(df: pd.DataFrame) -> pd.DataFrame:
    cols = { 'å ´å':None, 'ç«¶èµ°å':None, 'å›ã‚Š':None, 'æ­£è¦è¡¨ç¾':None }
    for c in df.columns:
        s = str(c).strip()
        if s in ['å ´å','ç«¶é¦¬å ´','ã‚³ãƒ¼ã‚¹','é–‹å‚¬','é–‹å‚¬å ´']: cols['å ´å']=c
        if s in ['ç«¶èµ°å','ãƒ¬ãƒ¼ã‚¹å','åç§°']:              cols['ç«¶èµ°å']=c
        if s in ['å›ã‚Š','å³å·¦','å‘ã']:                    cols['å›ã‚Š']=c
        if s.lower() in ['regex','æ­£è¦è¡¨ç¾','æ­£è¦è¡¨ç¾?']:   cols['æ­£è¦è¡¨ç¾']=c
    out = pd.DataFrame()
    if cols['å ´å'] is not None:    out['å ´å'] = df[cols['å ´å']].astype(str).str.strip()
    if cols['ç«¶èµ°å'] is not None:  out['ç«¶èµ°å'] = df[cols['ç«¶èµ°å']].astype(str).str.strip()
    if cols['å›ã‚Š'] is not None:
        out['å›ã‚Š'] = (df[cols['å›ã‚Š']].astype(str)
                       .str.replace('å›ã‚Š','').str.replace('å‘¨ã‚Š','').str.strip().str[:1])
    else:
        out['å›ã‚Š'] = None
    out['æ­£è¦è¡¨ç¾'] = (df[cols['æ­£è¦è¡¨ç¾']] if cols['æ­£è¦è¡¨ç¾'] is not None else False).astype(bool)
    out = out[(out['å›ã‚Š'].isin(['å³','å·¦']))]
    return out

def _infer_venue_from_racename(name: str) -> str | None:
    s = str(name)
    for v in DEFAULT_VENUE_TURN.keys():
        if v in s: return v
    for syn, venue in VENUE_SYNONYM_TO_VENUE.items():
        if syn in s: return venue
    return None

def _attach_turn_to_scores(df_score: pd.DataFrame,
                           turn_df: pd.DataFrame | None,
                           use_default: bool=True) -> pd.DataFrame:
    df = df_score.copy()
    if 'å ´å' in df.columns:
        df['_å ´åæ¨å®š'] = df['å ´å'].astype(str).apply(_infer_venue_from_racename)
    else:
        df['_å ´åæ¨å®š'] = df['ç«¶èµ°å'].astype(str).apply(_infer_venue_from_racename)
    venue_map = {}
    if use_default:
        venue_map.update(DEFAULT_VENUE_TURN)
    if turn_df is not None and 'å ´å' in turn_df.columns:
        for v, t in turn_df[['å ´å','å›ã‚Š']].dropna().values:
            if str(v).strip():
                venue_map[str(v).strip()] = t
    df['å›ã‚Š'] = df['_å ´åæ¨å®š'].map(venue_map)
    if turn_df is not None and 'ç«¶èµ°å' in turn_df.columns:
        patt = turn_df.dropna(subset=['ç«¶èµ°å'])
        for _, row in patt.iterrows():
            pat = str(row['ç«¶èµ°å']).strip()
            trn = row['å›ã‚Š']
            is_re = bool(row.get('æ­£è¦è¡¨ç¾', False))
            mask = (df['ç«¶èµ°å'].astype(str).str.contains(pat, regex=is_re, na=False)
                    if is_re else
                    df['ç«¶èµ°å'].astype(str).str.contains(re.escape(pat), regex=True, na=False))
            df.loc[mask, 'å›ã‚Š'] = trn
    return df

# ===== NDCGï¼ˆãƒ¬ãƒ¼ã‚¹å˜ä½ï¼‰å®‰å…¨å®Ÿè£… =====
def ndcg_by_race(frame: pd.DataFrame, scores, k: int = 3) -> float:
    """ãƒ¬ãƒ¼ã‚¹å˜ä½ã§ NDCG@k ã‚’è¨ˆç®—ï¼ˆå®‰å…¨ç‰ˆï¼‰ã€‚"""
    f = frame[['race_id', 'y']].copy().reset_index(drop=True)
    s = np.asarray(scores, dtype=float)
    if len(s) != len(f):
        s = s[:len(f)]
    vals = []
    for _, idx in f.groupby('race_id').groups.items():
        idx = np.asarray(list(idx), dtype=int)
        y_true = np.nan_to_num(f.loc[idx, 'y'].values.astype(float), nan=0.0, neginf=0.0, posinf=0.0)
        y_pred = np.nan_to_num(s[idx].astype(float),                nan=0.0, neginf=0.0, posinf=0.0)
        m = len(idx)
        if m == 0:
            continue
        if m == 1:
            vals.append(1.0 if y_true[0] > 0 else 0.0)
            continue
        kk = int(min(max(1, k), m))
        order = np.argsort(-y_pred)
        gains = (2.0 ** y_true[order] - 1.0)
        discounts = 1.0 / np.log2(np.arange(2, m + 2))
        dcg = float(np.sum(gains[:kk] * discounts[:kk]))
        order_best = np.argsort(-y_true)
        gains_best = (2.0 ** y_true[order_best] - 1.0)
        idcg = float(np.sum(gains_best[:kk] * discounts[:kk]))
        vals.append(dcg / idcg if idcg > 0 else 0.0)
    return float(np.mean(vals)) if vals else float('nan')

# ======================== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ========================
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®š")

with st.sidebar.expander("ğŸ”° ã‚ˆãä½¿ã†ï¼ˆåŸºæœ¬ï¼‰", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    besttime_w   = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿", 0.0, 2.0, 1.0)
    with st.expander("æˆ¦ç¸¾ç‡ã®é‡ã¿ï¼ˆå½“è©²é¦¬å ´ï¼‰", expanded=False):
        win_w  = st.slider("å‹ç‡ã®é‡ã¿",   0.0, 5.0, 1.0, 0.1, key="w_win")
        quin_w = st.slider("é€£å¯¾ç‡ã®é‡ã¿", 0.0, 5.0, 0.7, 0.1, key="w_quin")
        plc_w  = st.slider("è¤‡å‹ç‡ã®é‡ã¿", 0.0, 5.0, 0.5, 0.1, key="w_plc")
    with st.expander("å„ç¨®ãƒœãƒ¼ãƒŠã‚¹è¨­å®š", expanded=False):
        grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
        agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
        agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
        agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)
        bw_bonus     = st.slider("é¦¬ä½“é‡é©æ­£ãƒœãƒ¼ãƒŠã‚¹(Â±10kg)", 0, 10, 2)
    with st.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆBTé‡ã¿ç”¨ï¼‰", expanded=True):
        TARGET_GRADE    = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", ["G1", "G2", "G3", "L", "OP"], index=4)
        TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ", "ãƒ€"], index=0)
        TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", min_value=1000, max_value=3600, value=1800, step=100)

with st.sidebar.expander("ğŸ›  è©³ç´°ï¼ˆè£œæ­£/è„šè³ª/ãƒšãƒ¼ã‚¹ï¼‰", expanded=False):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)

    with st.expander("æ–¤é‡ãƒ™ãƒ¼ã‚¹ï¼ˆWFAç°¡ç•¥ï¼‰", expanded=False):
        race_date = pd.to_datetime(st.date_input("é–‹å‚¬æ—¥", value=pd.Timestamp.today().date()))
        use_wfa_base = st.checkbox("WFAåŸºæº–ã‚’ä½¿ã†ï¼ˆæ¨å¥¨ï¼‰", value=True)
        wfa_2_early_m = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 55.0, 0.5)
        wfa_2_early_f = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 54.0, 0.5)
        wfa_2_late_m  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 56.0, 0.5)
        wfa_2_late_f  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 55.0, 0.5)
        wfa_3p_m      = st.number_input("3æ­³ä»¥ä¸Š ç‰¡/ã›ã‚“ [kg]" , 50.0, 62.0, 57.0, 0.5)
        wfa_3p_f      = st.number_input("3æ­³ä»¥ä¸Š ç‰ [kg]"     , 48.0, 60.0, 55.0, 0.5)

    with st.expander("å±æ€§é‡ã¿ï¼ˆ1èµ°ã‚¹ã‚³ã‚¢ä¿‚æ•°ï¼‰", expanded=False):
        gender_w = {g: st.slider(f"{g}", 0.0, 2.0, 1.0) for g in ['ç‰¡','ç‰','ã‚»']}
        style_w  = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in STYLES}
        season_w = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in ['æ˜¥','å¤','ç§‹','å†¬']}
        age_w    = {str(age): st.slider(f"{age}æ­³", 0.0, 2.0, 1.0, 0.05) for age in range(3, 11)}
        frame_w  = {str(i): st.slider(f"{i}æ ", 0.0, 2.0, 1.0) for i in range(1,8+1)}

    with st.expander("è„šè³ªè‡ªå‹•æ¨å®š / ãƒšãƒ¼ã‚¹æ¨å®š", expanded=False):
        auto_style_on   = st.checkbox("è„šè³ªã®è‡ªå‹•æ¨å®šã‚’ä½¿ã†ï¼ˆç©ºæ¬„ã‚’åŸ‹ã‚ã‚‹ï¼‰", True)
        AUTO_OVERWRITE  = st.checkbox("æ‰‹å…¥åŠ›ã‚ˆã‚Šè‡ªå‹•ã‚’å„ªå…ˆã—ã¦ä¸Šæ›¸ã", False)
        NRECENT         = st.slider("ç›´è¿‘ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆè„šè³ªæ¨å®šï¼‰", 1, 10, 5)
        HL_DAYS_STYLE   = st.slider("åŠæ¸›æœŸï¼ˆæ—¥ãƒ»è„šè³ªç”¨ï¼‰", 30, 365, 180, 15)
        pace_mc_draws   = st.slider("ãƒšãƒ¼ã‚¹MCå›æ•°", 500, 30000, 5000, 500)
        pace_mode = st.radio("ãƒšãƒ¼ã‚¹ã®æ‰±ã„", ["è‡ªå‹•ï¼ˆMCï¼‰", "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰"], index=0)
        pace_fixed = st.selectbox("å›ºå®šãƒšãƒ¼ã‚¹", ["ãƒã‚¤ãƒšãƒ¼ã‚¹","ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹","ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹","ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"],
                                  index=1, disabled=(pace_mode=="è‡ªå‹•ï¼ˆMCï¼‰"))
        epi_alpha = st.slider("é€ƒã’ä¿‚æ•° Î±", 0.0, 2.0, 1.0, 0.05)
        epi_beta  = st.slider("å…ˆè¡Œä¿‚æ•° Î²", 0.0, 2.0, 0.60, 0.05)
        thr_hi    = st.slider("é–¾å€¤: ãƒã‚¤ãƒšãƒ¼ã‚¹ â‰¥", 0.30, 1.00, 0.52, 0.01)
        thr_mid   = st.slider("é–¾å€¤: ãƒŸãƒ‰ãƒ« â‰¥",    0.10, 0.99, 0.30, 0.01)
        thr_slow  = st.slider("é–¾å€¤: ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ â‰¥",0.00, 0.98, 0.18, 0.01)

# === NEW: å›ã‚Šï¼ˆå³/å·¦ï¼‰è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ ===
with st.sidebar.expander("ğŸ”„ å›ã‚Šï¼ˆå³/å·¦ï¼‰", expanded=False):
    TARGET_TURN = st.radio("æœ¬ãƒ¬ãƒ¼ã‚¹ã®å›ã‚Š", ["å³","å·¦"], index=0, horizontal=True)
    turn_gain   = st.slider("å›ã‚Šé©æ€§ ä¿‚æ•°ï¼ˆFinalRawã¸åŠ ç‚¹ï¼‰", 0.0, 3.0, 1.0, 0.1)
    turn_gap_thr= st.slider("å¾—æ„åˆ¤å®šã®é–¾å€¤ï¼ˆRightZâˆ’LeftZ ã®æœ€å°å·®ï¼‰", 0.0, 10.0, 1.0, 0.1)
    use_default_venue_map = st.checkbox("JRAæ¨™æº–ã®ã€å ´åâ†’å›ã‚Šã€ã§è£œå®Œã™ã‚‹", True)
    st.caption("â€» å ´åæ—¢å®šè¡¨ï¼‹ç«¶èµ°åã‹ã‚‰è‡ªå‹•æ¨å®šã€‚")

# === NEW: è·é›¢Ã—å›ã‚Šï¼ˆç²¾å¯†ï¼‰ ===
with st.sidebar.expander("ğŸ¯ è·é›¢Ã—å›ã‚Šï¼ˆç²¾å¯†ï¼‰", expanded=False):
    USE_DIST_TURN     = st.checkbox("æœ‰åŠ¹åŒ–ï¼ˆè·é›¢Ã—å³å·¦ã§è¿‘å‚åŒ–ï¼‰", True)
    dist_bw_m         = st.slider("è·é›¢å¸¯ã®å¹… [m]", 50, 600, 200, 25)
    opp_turn_discount = st.slider("é€†å›ã‚Šã®é‡ã¿ä¿‚æ•°", 0.0, 1.0, 0.5, 0.05)
    dist_prior_mode   = st.selectbox("è·é›¢é©æ€§ã®äº‹å‰åˆ†å¸ƒ",
                                     ["ç„¡ã—","WAvgZãƒ™ãƒ¼ã‚¹","Right/LeftZãƒ™ãƒ¼ã‚¹"], index=1)
    dist_tau          = st.slider("äº‹å‰é‡ã¿ Ï„ï¼ˆå°=äº‹å¾Œé‡è¦–ï¼‰", 0.0, 5.0, 1.0, 0.1)
    dist_turn_gain    = st.slider("è·é›¢Ã—å›ã‚Š ä¿‚æ•°ï¼ˆFinalRawã¸ï¼‰", 0.0, 3.0, 1.0, 0.1)
    grid_step         = st.slider("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è·é›¢åˆ»ã¿ [m]", 50, 400, 100, 50)
    grid_span_m       = st.slider("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®Â±ç¯„å›² [m]", 200, 1200, 600, 50)
    st.caption("è¿‘å‚åŒ–: Nadarayaâ€“Watsonï¼ˆã‚¬ã‚¦ã‚¹æ ¸ï¼‰ / æœ‰åŠ¹æœ¬æ•°: Kish ã®å¼")

# === ãƒãƒ³ãƒ‰æ ¡æ­£ï¼ˆBä¸­å¿ƒåŒ–ï¼‰ ===
with st.sidebar.expander("ğŸ· ãƒãƒ³ãƒ‰æ ¡æ­£ï¼ˆBä¸­å¿ƒåŒ–ï¼‰", expanded=True):
    band_mid_target = st.slider("ä¸­å¤®å€¤â†’ä½•ç‚¹ã«åˆã‚ã›ã‚‹ï¼Ÿ", 40, 80, 65, 1,
                                help="AR100ã§ãƒ¬ãƒ¼ã‚¹ã®çœŸã‚“ä¸­ã‚’ä½•ç‚¹ã«ç½®ãã‹ï¼ˆBã®çœŸã‚“ä¸­=65æ¨å¥¨ï¼‰")
    band_A_share = st.slider("Aä»¥ä¸Šã®ç›®æ¨™å‰²åˆ(%)", 1, 60, 25, 1,
                             help="AR100ãŒ70ä»¥ä¸Šï¼ˆA, S, SSï¼‰ã«ãªã‚‹é ­æ•°ã®å‰²åˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€‚Bã‚’åšãã—ãŸã„ãªã‚‰å°ã•ã‚ã«ã€‚")
    band_clip_lo = st.slider("ä¸‹é™ã‚¯ãƒªãƒƒãƒ—", 0, 60, 40, 1)
    band_clip_hi = st.slider("ä¸Šé™ã‚¯ãƒªãƒƒãƒ—", 80, 100, 100, 1)

# === NEW: PL/Top3ã¨ä¿å­˜ç³» ===
with st.sidebar.expander("ğŸ§ª å‹ç‡åŒ– / ä¿å­˜", expanded=False):
    mc_iters   = st.slider("Top3è¿‘ä¼¼ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆè»½é‡MCï¼‰", 1000, 50000, 8000, 1000)
    mc_beta    = st.slider("PLæ¸©åº¦ Î²ï¼ˆå¤§=é‹­ã„ï¼‰", 0.3, 5.0, 1.4, 0.1)
    mc_seed    = st.number_input("ä¹±æ•°Seed", 0, 999999, 42, 1)
    st.markdown("---")
    total_budget = st.slider("åˆè¨ˆäºˆç®—", 500, 50000, 10000, 100)
    min_unit     = st.selectbox("æœ€å°è³­ã‘å˜ä½", [100, 200, 300, 500], index=0)
    max_lines    = st.slider("æœ€å¤§ç‚¹æ•°(é€£ç³»)", 1, 60, 20, 1)
    st.markdown("---")

    # ---- JSONä¿å­˜ã‚’å®‰å…¨åŒ–ï¼ˆéã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå“ã‚’æ–‡å­—åˆ—åŒ–ï¼‰ ----
    def _jsonable(x):
        import numpy as _np
        if isinstance(x, (str, int, float, bool)) or x is None:
            return x
        if isinstance(x, (_np.integer,)):
            return int(x)
        if isinstance(x, (_np.floating,)):
            return float(x)
        if isinstance(x, (_np.bool_,)):
            return bool(x)
        if isinstance(x, (list, tuple, set)):
            return [_jsonable(i) for i in x]
        if isinstance(x, dict):
            return {str(k): _jsonable(v) for k, v in x.items()}
        return str(x)

    col_a, col_b = st.columns(2)
    if col_a.button("è¨­å®šã‚’ä¿å­˜"):
        try:
            cfg_dict = {}
            for k, v in st.session_state.items():
                if str(k).startswith('_'):
                    continue
                if k in ('excel_up','cfg_up','pedi_html_up'):
                    continue
                cfg_dict[k] = _jsonable(v)
            cfg = json.dumps(cfg_dict, ensure_ascii=False, indent=2)
            st.download_button("JSONã‚’DL", data=cfg, file_name="keiba_config.json", mime="application/json")
        except Exception as e:
            st.error(f"è¨­å®šä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    cfg_file = col_b.file_uploader("è¨­å®šèª­ã¿è¾¼ã¿", type=["json"], key="cfg_up")
    if cfg_file is not None:
        try:
            cfg = json.loads(cfg_file.read().decode("utf-8"))
            for k,v in cfg.items():
                st.session_state[k]=v
            st.success("è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆå¿…è¦ãªã‚‰å†å®Ÿè¡Œï¼‰ã€‚")
        except Exception as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# === NEW: ç¢ºç‡æ ¡æ­£ã®è¨­å®š ===
with st.sidebar.expander("ğŸ“ ç¢ºç‡æ ¡æ­£ï¼ˆä»»æ„ï¼‰", expanded=False):
    do_calib = st.checkbox("å‹ç‡ã‚’æ ¡æ­£ã™ã‚‹ï¼ˆç­‰æ¸©å›å¸°/Plattï¼‰", value=False)
    calib_method = st.radio("æ ¡æ­£æ³•", ["Isotonicï¼ˆæ¨å¥¨ï¼‰","Plattï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰"], index=0, horizontal=True, disabled=not do_calib)
    st.caption("â€» sheet0ã®éå»èµ°ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹å†…softmaxâ†’1ç€ãƒ©ãƒ™ãƒ«ã§å­¦ç¿’ã€‚èŠ/ãƒ€åˆ†å‰²ãªã©ã¯å¾ŒåŠã®è©•ä¾¡ã‚¿ãƒ–ã§ã€‚")

# === è¡¨ç¤ºè¨­å®š ===
with st.sidebar.expander("ğŸ–¥ è¡¨ç¤ºè¨­å®š", expanded=True):
    FULL_TABLE_VIEW = st.checkbox("è¡¨ã¯å…¨é ­è¡¨ç¤ºï¼ˆå†…éƒ¨ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ç„¡ãã™ï¼‰", value=True)
    MAX_TABLE_HEIGHT = st.slider("å…¨é ­è¡¨ç¤ºã®æœ€å¤§é«˜ã•(px)", 800, 10000, 5000, 200)

def auto_table_height(n_rows: int, row_px: int = 35, header_px: int = 38, pad_px: int = 28) -> int:
    h = int(header_px + row_px * max(1, int(n_rows)) + pad_px)
    return min(h, int(MAX_TABLE_HEIGHT))

def H(df, default_px: int) -> int:
    try:
        return auto_table_height(len(df)) if FULL_TABLE_VIEW else int(default_px)
    except Exception:
        return int(default_px)

# === ä¾¿åˆ©ãƒªã‚»ãƒƒãƒˆ ===
with st.sidebar.expander("ğŸ§¹ ãƒˆãƒ©ãƒ–ãƒ«æ™‚ã®ãƒªã‚»ãƒƒãƒˆ", expanded=False):
    if st.button("åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        for key in list(st.session_state.keys()):
            if key.startswith("s0:") or key.startswith("map:s0:") or key.startswith("s1:") or key.startswith("map:s1:"):
                del st.session_state[key]
        st.success("åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚Excelã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")

# ======================== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ========================
st.title("ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰")
st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
excel_file = st.file_uploader("Excelï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

sheet0, sheet1 = load_excel_bytes(excel_file.getvalue())

# === sheet0 / sheet1 ãƒãƒƒãƒ”ãƒ³ã‚° ===
def _norm_col(s: str) -> str:
    s = str(s).strip()
    s = re.sub(r'\s+', '', s)
    s = s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789'))
    s = s.replace('ï¼ˆ','(').replace('ï¼‰',')').replace('ï¼…','%')
    return s
def _auto_guess(col_map, pats):
    for orig, normed in col_map.items():
        for p in pats:
            if re.search(p, normed, flags=re.I):
                return orig
    return None

def _interactive_map(df, patterns, required_keys, title, state_key, show_ui=False):
    cols = list(df.columns)
    cmap = {c: _norm_col(c) for c in cols}

    auto = {k: (st.session_state.get(f"{state_key}:{k}") or _auto_guess(cmap, pats))
            for k, pats in patterns.items()}

    for k, v in list(auto.items()):
        if v not in cols:
            auto[k] = None

    if not show_ui:
        missing = [k for k in required_keys if not auto.get(k)]
        if not missing:
            for k, v in auto.items():
                if v:
                    st.session_state[f"{state_key}:{k}"] = v
            return auto
        else:
            st.warning(f"{title} ã®å¿…é ˆåˆ—ãŒè‡ªå‹•èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ: " + ", ".join(missing))
            show_ui = True

    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=True):
        mapping = {}
        for key, pats in patterns.items():
            default = st.session_state.get(f"{state_key}:{key}") or auto.get(key)
            options = ['<æœªé¸æŠ>'] + cols
            idx = options.index(default) if (default in cols) else 0
            mapping[key] = st.selectbox(key, options=options, index=idx, key=f"map:{state_key}:{key}")
            if mapping[key] != '<æœªé¸æŠ>':
                st.session_state[f"{state_key}:{key}"] = mapping[key]

    missing = [k for k in required_keys if mapping.get(k) in (None, '<æœªé¸æŠ>')]
    if missing:
        st.stop()

    return {k: (None if v=='<æœªé¸æŠ>' else v) for k, v in mapping.items()}

# ãƒ‘ã‚¿ãƒ¼ãƒ³
PAT_S0 = {
    'é¦¬å'         : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥'     : [r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜(?!S)|å¹´æœˆæ—¥|æ–½è¡Œæ—¥|é–‹å‚¬æ—¥'],
    'ç«¶èµ°å'       : [r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å'     : [r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°'         : [r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †'     : [r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ '           : [r'æ \b|æ ç•ª'],
    'ç•ª'           : [r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡'         : [r'æ–¤é‡'],
    'é¦¬ä½“é‡'       : [r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ': [r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3Fã‚¿ã‚¤ãƒ |ä¸Š3F|Ave-?3F'],
    'ä¸Š3Fé †ä½'     : [r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’'      : [r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼.*|4è§’é †ä½|4è§’é€šéé †'],
    'æ€§åˆ¥'         : [r'æ€§åˆ¥'],
    'å¹´é½¢'         : [r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' : [r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢'         : [r'è·é›¢'],
    'é¦¬å ´'         : [r'é¦¬å ´(?!.*æŒ‡æ•°)|é¦¬å ´çŠ¶æ…‹'],
    'å ´å'         : [r'å ´å|å ´æ‰€|ç«¶é¦¬å ´|é–‹å‚¬(åœ°|å ´|å ´æ‰€)'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
MAP_S0 = _interactive_map(sheet0, PAT_S0, REQ_S0, "sheet0ï¼ˆéå»èµ°ï¼‰", "s0", show_ui=False)

df_score = pd.DataFrame()
for k, col in MAP_S0.items():
    if (col is None) or (col not in sheet0.columns):
        continue
    df_score[k] = sheet0[col]

df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
for c in ['é ­æ•°','ç¢ºå®šç€é †','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½','é€šé4è§’','è·é›¢']:
    if c in df_score: df_score[c] = pd.to_numeric(df_score[c], errors='coerce')
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' in df_score: df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].apply(_parse_time_to_sec)
if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in df_score: df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# é€šé4è§’/é ­æ•°ã®æ•´å½¢
if 'é ­æ•°' in df_score.columns:
    df_score['é ­æ•°'] = (
        df_score['é ­æ•°'].astype(str).str.extract(r'(\d+)')[0]
        .apply(pd.to_numeric, errors='coerce')
    )
if 'é€šé4è§’' in df_score.columns:
    s = df_score['é€šé4è§’']
    if s.dtype.kind not in 'iu':
        last_num = s.astype(str).str.extract(r'(\d+)(?!.*\d)')[0]
        df_score['é€šé4è§’'] = pd.to_numeric(last_num, errors='coerce')
    ok = df_score['é ­æ•°'].notna() & df_score['é€šé4è§’'].notna()
    bad = ok & ((df_score['é€šé4è§’'] < 1) | (df_score['é€šé4è§’'] > df_score['é ­æ•°']))
    df_score.loc[df_score['é€šé4è§’'].eq(0), 'é€šé4è§’'] = np.nan
    df_score.loc[bad, 'é€šé4è§’'] = np.nan

# === sheet1ï¼ˆå½“æ—¥å‡ºèµ°è¡¨ï¼‰ ===
PAT_S1 = {
    'é¦¬å'   : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ '     : [r'æ \b|æ ç•ª'],
    'ç•ª'     : [r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥'   : [r'æ€§åˆ¥'],
    'å¹´é½¢'   : [r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡'   : [r'æ–¤é‡'],
    'é¦¬ä½“é‡' : [r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª'   : [r'è„šè³ª'],
    'å‹ç‡'   : [r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡' : [r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡' : [r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ': [r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1 = ['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢']
MAP_S1 = _interactive_map(sheet1, PAT_S1, REQ_S1, "sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰", "s1", show_ui=False)

attrs = pd.DataFrame()
for k, col in MAP_S1.items():
    if (col is None) or (col not in sheet1.columns):
        continue
    attrs[k] = sheet1[col]
for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in attrs: attrs[c] = pd.to_numeric(attrs[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in attrs: attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] = attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# â˜…â˜…â˜… å…ˆé ­ã®ç©ºè¡Œï¼ˆå…¨åˆ—NaNï¼‰ã‚„é¦¬åãŒç©ºã®è¡Œã‚’é™¤å»ã—ã¦ã‹ã‚‰ã‚¨ãƒ‡ã‚£ã‚¿ã¸ â˜…â˜…â˜…
attrs = attrs.replace(r'^\s*$', np.nan, regex=True)   # ç©ºæ–‡å­—â†’NaN
attrs = attrs.dropna(how='all')                       # å…¨åˆ—NaNè¡Œã‚’å‰Šé™¤
if 'é¦¬å' in attrs.columns:
    attrs['é¦¬å'] = attrs['é¦¬å'].astype(str).str.replace('\u3000',' ').str.strip()
    attrs = attrs[attrs['é¦¬å'].ne('')]
attrs = attrs.reset_index(drop=True)

# å…¥åŠ›UIï¼ˆè„šè³ªãƒ»æ–¤é‡ãƒ»é¦¬ä½“é‡ç·¨é›†ï¼‰
if 'è„šè³ª' not in attrs.columns: attrs['è„šè³ª'] = ''
if 'æ–¤é‡' not in attrs.columns: attrs['æ–¤é‡'] = np.nan
if 'é¦¬ä½“é‡' not in attrs.columns: attrs['é¦¬ä½“é‡'] = np.nan

# === NEW: ã‚¨ãƒ‡ã‚£ã‚¿ã®å†…å®¹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å¾©å…ƒï¼ˆãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰ ===
if 'horses_df' in st.session_state and isinstance(st.session_state['horses_df'], pd.DataFrame) and not st.session_state['horses_df'].empty:
    prev = st.session_state['horses_df'][['é¦¬å','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy()
    attrs = attrs.merge(prev, on='é¦¬å', how='left', suffixes=('','_prev'))
    for c in ['è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']:
        attrs[c] = attrs[c].where(attrs[c].notna() & (attrs[c] != ''), attrs.get(f'{c}_prev'))
    drop_cols = [f'{c}_prev' for c in ['è„šè³ª','æ–¤é‡','é¦¬ä½“é‡'] if f'{c}_prev' in attrs.columns]
    if drop_cols:
        attrs.drop(columns=drop_cols, inplace=True)

st.subheader("é¦¬ä¸€è¦§ãƒ»è„šè³ªãƒ»æ–¤é‡ãƒ»å½“æ—¥é¦¬ä½“é‡å…¥åŠ›")

with st.form("horses_form", clear_on_submit=False):
    edited = st.data_editor(
        attrs[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
        column_config={
            'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
            'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
            'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1)
        },
        use_container_width=True,
        num_rows='static',
        height=auto_table_height(len(attrs)) if st.session_state.get('FULL_TABLE_VIEW', True) else 420,
        hide_index=True,
        key="horses_editor",
    )

    # å…¨é ­åŸ‹ã¾ã£ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆç©ºæ–‡å­—ãƒ»NaNã‚’è¨±ã•ãªã„ï¼‰
    need_cols = ['è„šè³ª']  # æ–¤é‡ã‚„é¦¬ä½“é‡ã‚‚å¿…é ˆã«ã™ã‚‹ãªã‚‰è¿½åŠ : 'æ–¤é‡','é¦¬ä½“é‡'
    def _all_filled(df, cols):
        ok = True
        for c in cols:
            s = df[c].astype(str).str.strip()
            ok &= s.ne('').all()
        return bool(ok)

    submitted = st.form_submit_button("å…¨é ­å…¥åŠ›å®Œäº† â†’ è¨ˆç®—")

if not submitted:
    st.info("è„šè³ªã‚’å…¨é ­å…¥åŠ›ã—ã¦ã€å…¨é ­å…¥åŠ›å®Œäº† â†’ è¨ˆç®—ã€ã‚’æŠ¼ã™ã¨é›†è¨ˆã—ã¾ã™ã€‚")
    st.stop()

horses = edited.copy()
# ã“ã®å¾Œã¯å¾“æ¥ã©ãŠã‚Š
horses['è„šè³ª'] = horses['è„šè³ª'].map(normalize_style)
st.session_state['horses_df'] = horses.copy()


validate_inputs(df_score, horses)

# --- è„šè³ª è‡ªå‹•æ¨å®šï¼ˆç•¥ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç¶­æŒï¼‰ ---
# ï¼ˆã“ã“ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒä¸€ã®ãŸã‚çœç•¥ã›ãšæ®‹ã—ã¦ã„ã¾ã™ï¼‰
df_style = pd.DataFrame({'é¦¬å': [], 'p_é€ƒã’': [], 'p_å…ˆè¡Œ': [], 'p_å·®ã—': [], 'p_è¿½è¾¼': [], 'æ¨å®šè„šè³ª': []})
need_cols = {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
if auto_style_on and need_cols.issubset(df_score.columns):
    tmp = (
        df_score[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’','ä¸Š3Fé †ä½']].copy()
        .dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'])
        .sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    )
    tmp['_rn'] = tmp.groupby('é¦¬å').cumcount()+1
    tmp = tmp[tmp['_rn'] <= int(NRECENT)].copy()
    today = pd.Timestamp.today()
    tmp['_days'] = (today - pd.to_datetime(tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    tmp['_w'] = 0.5 ** (tmp['_days'] / float(HL_DAYS_STYLE))

    denom = (pd.to_numeric(tmp['é ­æ•°'], errors='coerce') - 1).replace(0, np.nan)
    pos_ratio = (pd.to_numeric(tmp['é€šé4è§’'], errors='coerce') - 1) / denom
    pos_ratio = pos_ratio.clip(0, 1).fillna(0.5)

    if 'ä¸Š3Fé †ä½' in tmp.columns:
        ag = pd.to_numeric(tmp['ä¸Š3Fé †ä½'], errors='coerce')
        close_strength = ((3.5 - ag) / 3.5).clip(lower=0, upper=1).fillna(0.0)
    else:
        close_strength = pd.Series(0.0, index=tmp.index)

    b_nige, b_sengo, b_sashi, b_oikomi = -1.2, 0.6, 0.3, -0.7
    tmp['L_nige']   = b_nige  + 1.6*(1 - pos_ratio) - 1.2*close_strength
    tmp['L_sengo']  = b_sengo + 1.1*(1 - pos_ratio) - 0.1*close_strength
    tmp['L_sashi']  = b_sashi + 1.1*(pos_ratio)     + 0.9*close_strength
    tmp['L_oikomi'] = b_oikomi+ 1.6*(pos_ratio)     + 0.5*close_strength

    rows = []
    for name, g in tmp.groupby('é¦¬å'):
        w  = g['_w'].to_numpy(); sw = w.sum()
        if sw <= 0: continue
        def wavg(v): return float((v*w).sum()/sw)
        vec = np.array([wavg(g['L_nige']), wavg(g['L_sengo']), wavg(g['L_sashi']), wavg(g['L_oikomi'])], dtype=float)
        vec = vec - vec.max()
        p = np.exp(vec); p = p / p.sum()
        pred = STYLES[int(np.argmax(p))]

        pr = (pd.to_numeric(g['é€šé4è§’'], errors='coerce') - 1) / (pd.to_numeric(g['é ­æ•°'], errors='coerce') - 1)
        pr = pr.clip(0,1).fillna(0.5)
        wpr = float((pr*w).sum()/sw)
        if pred == 'é€ƒã’' and not (wpr <= 0.22 or ((pr <= 0.15)*w).sum()/sw >= 0.25): pred='å…ˆè¡Œ'
        if pred == 'è¿½è¾¼' and not (wpr >= 0.78 or ((pr >= 0.85)*w).sum()/sw >= 0.25): pred='å·®ã—'

        rows.append([name, *p.tolist(), pred])
    if rows:
        df_style = pd.DataFrame(rows, columns=['é¦¬å','p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼','æ¨å®šè„šè³ª'])
        manual_has_nige = ('è„šè³ª' in horses.columns) and horses['è„šè³ª'].eq('é€ƒã’').any()
        if (df_style['æ¨å®šè„šè³ª'].eq('é€ƒã’').sum() == 0) and (not manual_has_nige):
            early = tmp.assign(early=(1 - pos_ratio).clip(0, 1), w=tmp['_w'].values)\
                      .groupby('é¦¬å').apply(lambda g: float((g['early']*g['w']).sum()/g['w'].sum()))
            nige_cand = early.idxmax()
            df_style.loc[df_style['é¦¬å'] == nige_cand, 'æ¨å®šè„šè³ª'] = 'é€ƒã’'

# --- æˆ¦ç¸¾ç‡ãƒ»ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ  ---
rate_cols = [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡'] if c in attrs.columns]
if rate_cols:
    rate = attrs[['é¦¬å'] + rate_cols].copy()
    for c in rate_cols:
        rate[c] = rate[c].astype(str).str.replace('%','', regex=False).str.replace('ï¼…','', regex=False)
        rate[c] = pd.to_numeric(rate[c], errors='coerce')
    mx = pd.concat([rate[c] for c in rate_cols], axis=1).max().max()
    if pd.notna(mx) and mx <= 1.0:
        for c in rate_cols: rate[c] *= 100.0
    if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in attrs:
        rate = rate.merge(attrs[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
else:
    rate = pd.DataFrame({'é¦¬å':[]})

# === éå»èµ°ã‹ã‚‰ã€Œé©æ­£é¦¬ä½“é‡ã€æ¨å®š ===
best_bw_map = {}
if {'é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †'}.issubset(df_score.columns):
    _bw = df_score[['é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †']].dropna()
    _bw['ç¢ºå®šç€é †'] = pd.to_numeric(_bw['ç¢ºå®šç€é †'], errors='coerce')
    _bw = _bw[_bw['ç¢ºå®šç€é †'].notna()]
    try:
        best_idx = _bw.groupby('é¦¬å')['ç¢ºå®šç€é †'].idxmin()
        best_bw_map = _bw.loc[best_idx].set_index('é¦¬å')['é¦¬ä½“é‡'].astype(float).to_dict()
    except Exception:
        best_bw_map = {}

# é‡è¤‡ã‚¬ãƒ¼ãƒ‰
try:
    horses.drop_duplicates('é¦¬å', keep='first', inplace=True)
except Exception:
    pass
try:
    df_score = df_score.drop_duplicates(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å'], keep='first')
except Exception:
    pass

# ===== ãƒãƒ¼ã‚¸ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    df_score.drop(columns=[dup], errors='ignore', inplace=True)
df_score = df_score.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')
if len(rate) > 0:
    use_cols = ['é¦¬å'] + [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] if c in rate.columns]
    df_score = df_score.merge(rate[use_cols], on='é¦¬å', how='left')

# ===== ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ =====
bt_min = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_max = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_span = (bt_max - bt_min) if pd.notna(bt_min) and pd.notna(bt_max) and (bt_max > bt_min) else 1.0
CLASS_BASE_BT = {"OP": 1.50, "L": 1.38, "G3": 1.19, "G2": 1.00, "G1": 0.80}
def besttime_weight_final(grade: str, surface: str, distance_m: int, user_scale: float) -> float:
    base = CLASS_BASE_BT.get(str(grade), CLASS_BASE_BT["OP"])
    s = 1.10 if str(surface) == "ãƒ€" else 1.00
    try:
        d = int(distance_m)
        if d <= 1400:           dfac = 1.20
        elif d == 1600:         dfac = 1.10
        elif 1800 <= d <= 2200: dfac = 1.00
        elif d >= 2400:         dfac = 0.85
        else:                   dfac = 1.00
    except: dfac = 1.00
    w = base * s * dfac * float(user_scale)
    return float(np.clip(w, 0.0, 2.0))

CLASS_PTS = {'G1':10, 'G2':8, 'G3':6, 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5, 'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}
def class_points(row) -> int:
    g = normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row:
        g = normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name = str(row.get('ã‚¯ãƒ©ã‚¹å','')) + ' ' + str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

def wfa_base_for(sex: str, age: int | None, dt: pd.Timestamp) -> float:
    try:
        a = int(age) if age is not None and not pd.isna(age) else None
    except Exception:
        a = None
    m = int(dt.month) if isinstance(dt, pd.Timestamp) else 1
    if a == 2:
        male = wfa_2_early_m if m <= 9 else wfa_2_late_m
        filly = wfa_2_early_f if m <= 9 else wfa_2_late_f
    elif a is not None and a >= 3:
        male, filly = wfa_3p_m, wfa_3p_f
    else:
        male, filly = wfa_3p_m, wfa_3p_f
    return male if sex in ("ç‰¡", "ã‚»") else filly

# ===== è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰èª­ã‚€ =====
pedi_bonus_pts: float = float(st.session_state.get('pedi:points', 0.0))
_pedi_map_session = st.session_state.get('pedi:map', {})
pedi_bonus_map = { _trim_name(k): bool(v) for k, v in dict(_pedi_map_session).items() }
def _pedi_bonus_for(name: str) -> float:
    nm = _trim_name(name)
    return float(pedi_bonus_pts) if pedi_bonus_map.get(nm, False) else 0.0

def calc_score(r):
    g = class_points(r)
    raw = g * (r['é ­æ•°'] + 1 - r['ç¢ºå®šç€é †']) + lambda_part * g

    sw  = season_w[season_of(pd.to_datetime(r['ãƒ¬ãƒ¼ã‚¹æ—¥']).month)]
    gw  = gender_w.get(r.get('æ€§åˆ¥'), 1)
    stw = style_w.get(r.get('è„šè³ª'), 1)
    fw  = frame_w.get(str(r.get('æ ')), 1)
    aw  = age_w.get(str(r.get('å¹´é½¢')), 1.0)

    gnorm = normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))
    grade_point = grade_bonus if gnorm in ['G1','G2','G3'] else 0

    agari_bonus = 0
    try:
        ao = int(r.get('ä¸Š3Fé †ä½', np.nan))
        if   ao == 1: agari_bonus = agari1_bonus
        elif ao == 2: agari_bonus = agari2_bonus
        elif ao == 3: agari_bonus = agari3_bonus
    except: pass

    body_bonus = 0
    try:
        name = r['é¦¬å']
        now_bw = float(r.get('é¦¬ä½“é‡', np.nan))
        tekitai = float(best_bw_map.get(name, np.nan))
        if not np.isnan(now_bw) and not np.isnan(tekitai) and abs(now_bw - tekitai) <= 10:
            body_bonus = bw_bonus
    except Exception: pass

    rate_bonus = 0.0
    try:
        if 'å‹ç‡' in r and pd.notna(r.get('å‹ç‡', np.nan)):   rate_bonus += win_w  * (float(r['å‹ç‡'])  / 100.0)
        if 'é€£å¯¾ç‡' in r and pd.notna(r.get('é€£å¯¾ç‡', np.nan)): rate_bonus += quin_w * (float(r['é€£å¯¾ç‡']) / 100.0)
        if 'è¤‡å‹ç‡' in r and pd.notna(r.get('è¤‡å‹ç‡', np.nan)): rate_bonus += plc_w  * (float(r['è¤‡å‹ç‡'])  / 100.0)
    except: pass

    bt_bonus = 0.0
    try:
        if pd.notna(r.get('ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’', np.nan)):
            bt_norm = (bt_max - float(r['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])) / bt_span
            bt_norm = max(0.0, min(1.0, bt_norm))
            bt_w_final = besttime_weight_final(
                grade=TARGET_GRADE, surface=TARGET_SURFACE, distance_m=int(TARGET_DISTANCE), user_scale=besttime_w
            )
            bt_bonus = bt_w_final * bt_norm
    except Exception:
        pass

    kg_pen = 0.0
    try:
        kg = float(r.get('æ–¤é‡', np.nan))
        if not np.isnan(kg):
            base = wfa_base_for(r.get('æ€§åˆ¥',''), int(r.get('å¹´é½¢')) if pd.notna(r.get('å¹´é½¢',np.nan)) else None, race_date) if use_wfa_base else 56.0
            delta = kg - float(base)
            kg_pen = (-max(0.0,  delta) * float(weight_coeff)
                      + 0.5 * max(0.0, -delta) * float(weight_coeff))
    except Exception:
        pass

    pedi_bonus = _pedi_bonus_for(r['é¦¬å'])

    total_bonus = grade_point + agari_bonus + body_bonus + rate_bonus + bt_bonus + kg_pen + pedi_bonus
    return raw * sw * gw * stw * fw * aw + total_bonus

# 1èµ°â†’æ­£è¦åŒ–
if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df_score.columns:
    st.error("ãƒ¬ãƒ¼ã‚¹æ—¥ åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Excelã®1æšç›®ã«å«ã‚ã¦ãã ã•ã„ã€‚")
    st.stop()
df_score['score_raw']  = df_score.apply(calc_score, axis=1)
if df_score['score_raw'].max() == df_score['score_raw'].min():
    df_score['score_norm'] = 50.0
else:
    df_score['score_norm'] = (
        (df_score['score_raw'] - df_score['score_raw'].min()) /
        (df_score['score_raw'].max() - df_score['score_raw'].min()) * 100
    )

# ===== æ™‚ç³»åˆ—åŠ é‡ =====
now = pd.Timestamp.today()
df_score['_days_ago'] = (now - df_score['ãƒ¬ãƒ¼ã‚¹æ—¥']).dt.days
df_score['_w'] = 0.5 ** (df_score['_days_ago'] / (half_life_m * 30.4375)) if half_life_m > 0 else 1.0

# ===== å›ã‚Šæƒ…å ±ã®ä»˜ä¸ =====
turn_df = None
df_score = _attach_turn_to_scores(df_score, turn_df, use_default=use_default_venue_map)

# ===== å³/å·¦å›ã‚Š é©æ€§ã®é›†è¨ˆ =====
g_turn = df_score[['é¦¬å','score_norm','_w','å›ã‚Š']].dropna(subset=['é¦¬å','score_norm','_w'])
g_turn['é¦¬å'] = g_turn['é¦¬å'].map(_trim_name)

def _wavg_row(s: pd.DataFrame) -> float:
    sw = float(pd.to_numeric(s['_w'], errors='coerce').sum())
    if sw <= 0:
        return float('nan')
    num = (pd.to_numeric(s['score_norm'], errors='coerce') * pd.to_numeric(s['_w'], errors='coerce')).sum()
    return float(num / sw)

def _make_weighted(df_sub: pd.DataFrame, col_name: str) -> pd.DataFrame:
    if df_sub.empty:
        return pd.DataFrame(columns=[col_name])
    res = df_sub.groupby('é¦¬å', sort=False).apply(_wavg_row)
    if isinstance(res, pd.Series):
        return res.rename(col_name).to_frame()
    elif isinstance(res, pd.DataFrame):
        if res.shape[1] == 0:
            return pd.DataFrame(columns=[col_name])
        return res.iloc[:, 0].rename(col_name).to_frame()
    else:
        return pd.DataFrame({col_name: pd.Series(res, index=pd.Index([], name='é¦¬å'))})

right = _make_weighted(g_turn[g_turn['å›ã‚Š'].astype(str) == 'å³'], 'RightZ')
left  = _make_weighted(g_turn[g_turn['å›ã‚Š'].astype(str) == 'å·¦'], 'LeftZ')

cnts = (
    g_turn.pivot_table(index='é¦¬å', columns='å›ã‚Š', values='score_norm', aggfunc='count')
         .rename(columns={'å³':'nR','å·¦':'nL'})
) if not g_turn.empty else pd.DataFrame()

turn_pref = pd.concat([right,left,cnts], axis=1).reset_index() \
    if len(right)+len(left)>0 else pd.DataFrame(columns=['é¦¬å','RightZ','LeftZ','nR','nL'])

if len(turn_pref) > 0:
    turn_pref['é¦¬å'] = turn_pref['é¦¬å'].map(_trim_name)

for c in ['RightZ','LeftZ','nR','nL']:
    if c not in turn_pref.columns:
        turn_pref[c] = np.nan

def _pref_label(row):
    R, L = row['RightZ'], row['LeftZ']
    nR, nL = row['nR'], row['nL']
    if pd.notna(R) and pd.notna(L):
        if (R - L) >= turn_gap_thr: return 'å³'
        if (L - R) >= turn_gap_thr: return 'å·¦'
        return 'ä¸­ç«‹'
    if pd.notna(R) and (nR >= 1): return 'å³?'
    if pd.notna(L) and (nL >= 1): return 'å·¦?'
    return 'ä¸æ˜'

turn_pref['TurnPref'] = turn_pref.apply(_pref_label, axis=1) if len(turn_pref)>0 else []
turn_pref['TurnGap']  = (turn_pref['RightZ'].fillna(0) - turn_pref['LeftZ'].fillna(0)) if len(turn_pref)>0 else []

def _pref_pts(row):
    lab = str(row['TurnPref'])
    if TARGET_TURN == 'å³':
        if lab == 'å³':  return 1.0
        if lab == 'å·¦':  return -1.0
        if lab == 'å³?': return 0.5
        if lab == 'å·¦?': return -0.5
        return 0.0
    else:
        if lab == 'å·¦':  return 1.0
        if lab == 'å³':  return -1.0
        if lab == 'å·¦?': return 0.5
        if lab == 'å³?': return -0.5
        return 0.0

if len(turn_pref)>0:
    turn_pref['TurnPrefPts'] = turn_pref.apply(_pref_pts, axis=1)
else:
    turn_pref['TurnPrefPts'] = []

# ===== é¦¬ã”ã¨ã®é›†è¨ˆ =====
def w_mean(x, w):
    x = np.asarray(x, dtype=float); w = np.asarray(w, dtype=float)
    s = w.sum()
    return float((x*w).sum()/s) if s>0 else np.nan

agg = []
for name, g in df_score.groupby('é¦¬å'):
    avg  = g['score_norm'].mean()
    std  = g['score_norm'].std(ddof=0)
    wavg = w_mean(g['score_norm'], g['_w'])
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g) >= 2 else np.nan
    agg.append({'é¦¬å':name,'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg = pd.DataFrame(agg)

# â€”â€” WStd ã®ç©´åŸ‹ã‚
wstd_nontrivial = df_agg.loc[df_agg['Nrun']>=2, 'WStd']
default_std = float(wstd_nontrivial.median()) if (wstd_nontrivial.notna().any()) else 6.0
df_agg['WStd'] = df_agg['WStd'].fillna(default_std)
min_floor = max(1.0, default_std*0.6)
df_agg.loc[df_agg['WStd'] < min_floor, 'WStd'] = min_floor

# ===== è„šè³ªçµ±åˆï¼ˆæ‰‹å…¥åŠ›å„ªå…ˆâ†’è‡ªå‹•ï¼‰ =====
for df in [horses, df_agg]:
    if 'é¦¬å' in df.columns: df['é¦¬å'] = df['é¦¬å'].map(_trim_name)

if 'è„šè³ª' in horses.columns:
    horses['è„šè³ª'] = horses['è„šè³ª'].map(normalize_style)

name_list = df_agg['é¦¬å'].tolist()
combined_style = pd.Series(index=name_list, dtype=object)
if 'è„šè³ª' in horses.columns:
    combined_style.update(horses.set_index('é¦¬å')['è„šè³ª'])
if not df_style.empty and auto_style_on:
    pred_series = df_style.set_index('é¦¬å')['æ¨å®šè„šè³ª']
    if AUTO_OVERWRITE:
        combined_style.update(pred_series)
    else:
        mask_blank = combined_style.isna() | combined_style.astype(str).str.strip().eq('')
        combined_style.loc[mask_blank] = pred_series.reindex(combined_style.index)[mask_blank]
combined_style = combined_style.fillna('')
df_agg['è„šè³ª'] = df_agg['é¦¬å'].map(combined_style)

# ===== Pè¡Œåˆ—ï¼ˆè„šè³ªç¢ºç‡ï¼‰â†’ ãƒšãƒ¼ã‚¹MC =====
Hn = len(name_list)
P = np.zeros((Hn, 4), dtype=float)
pmap = None
if not df_style.empty and 'é¦¬å' in df_style.columns:
    df_prob = df_style.rename(columns={
        'p_å·®è¾¼': 'p_å·®ã—','p_è¿½ã„è¾¼ã¿': 'p_è¿½è¾¼','p_è¿½è¾¼ã¿': 'p_è¿½è¾¼',
        'p_é€ƒã’ç‡': 'p_é€ƒã’','p_å…ˆè¡Œç‡': 'p_å…ˆè¡Œ','p_å·®ã—ç‡': 'p_å·®ã—','p_è¿½è¾¼ç‡': 'p_è¿½è¾¼',
    })
    need_cols = ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']
    if set(need_cols).issubset(df_prob.columns):
        for c in need_cols:
            df_prob[c] = pd.to_numeric(df_prob[c], errors='coerce').fillna(0.0)
        pmap = (df_prob[['é¦¬å'] + need_cols].set_index('é¦¬å')[need_cols])
        row_sums = pmap.sum(axis=1).replace(0, np.nan)
        pmap = pmap.div(row_sums, axis=0).fillna(0.0)

for i, nm in enumerate(name_list):
    stl = combined_style.get(nm, '')
    if not AUTO_OVERWRITE and (stl in STYLES):
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0; continue
    if pmap is not None and nm in pmap.index:
        P[i, :] = pmap.loc[nm, ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']].to_numpy(dtype=float)
        if P[i, :].sum() == 0:
            if stl in STYLES: P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
            else: P[i, :] = np.array([0.25,0.25,0.25,0.25])
    elif stl in STYLES:
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
    else:
        P[i, :] = np.array([0.25,0.25,0.25,0.25])

mark_rule = {
    'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
    'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
    'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
}
mark_to_pts = {'â—':2, 'ã€‡':1, 'â—‹':1, 'â–³':0, 'Ã—':-1}

rng_pace = np.random.default_rng(int(mc_seed) + 12345)
sum_pts = np.zeros(Hn, dtype=float)
pace_counter = {'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
for _ in range(int(pace_mc_draws)):
    sampled = [rng_pace.choice(4, p=P[i]) for i in range(Hn)]
    nige  = sum(1 for s in sampled if s==0)
    sengo = sum(1 for s in sampled if s==1)
    epi = (epi_alpha*nige + epi_beta*sengo) / max(1, Hn)
    if   epi >= thr_hi:   pace_t = "ãƒã‚¤ãƒšãƒ¼ã‚¹"
    elif epi >= thr_mid:  pace_t = "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
    elif epi >= thr_slow: pace_t = "ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    else:                 pace_t = "ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    pace_counter[pace_t] += 1
    mk = mark_rule[pace_t]
    for i, s in enumerate(sampled):
        sum_pts[i] += mark_to_pts[ mk[STYLES[s]] ]
df_agg['PacePts'] = sum_pts / max(1, int(pace_mc_draws))
pace_type = max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
if pace_mode == "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰":
    pace_type = pace_fixed
    v_pts = np.array([mark_to_pts[ mark_rule[pace_type][st] ] for st in STYLES], dtype=float)
    df_agg['PacePts'] = (P @ v_pts)

# ===== å›ã‚Šé©æ€§ã‚’ df_agg ã¸ãƒãƒ¼ã‚¸ =====
if len(turn_pref)>0:
    df_agg = df_agg.merge(
        turn_pref[['é¦¬å','RightZ','LeftZ','nR','nL','TurnGap','TurnPref','TurnPrefPts']],
        on='é¦¬å', how='left'
    )
else:
    for c in ['RightZ','LeftZ','nR','nL','TurnGap','TurnPref','TurnPrefPts']:
        df_agg[c] = np.nan
    df_agg['TurnPrefPts'] = df_agg['TurnPrefPts'].fillna(0.0)
df_agg['TurnPrefPts'] = df_agg['TurnPrefPts'].fillna(0.0)

# ===== è·é›¢Ã—å›ã‚Šï¼ˆç²¾å¯†ï¼‰ã‚’ df_agg ã«ä»˜ä¸ =====
if USE_DIST_TURN and {'è·é›¢','å›ã‚Š','score_norm','_w'}.issubset(df_score.columns):
    rows = []
    for nm in df_agg['é¦¬å'].astype(str):
        prof = _dist_turn_profile_for_horse(
            name=nm, df_hist=df_score,
            target_distance=int(TARGET_DISTANCE),
            target_turn=str(TARGET_TURN),
            surface=str(TARGET_SURFACE),
            h_m=float(dist_bw_m), opp_turn_w=float(opp_turn_discount),
            prior_mode=str(dist_prior_mode), tau=float(dist_tau),
            grid_step=int(grid_step), grid_span_m=int(grid_span_m),
            df_agg_for_prior=df_agg
        )
        rows.append({'é¦¬å': nm, **prof})
    _disttbl = pd.DataFrame(rows)
    df_agg = df_agg.merge(_disttbl, on='é¦¬å', how='left')
else:
    for c in ['DistTurnZ','n_eff_turn','BestDist_turn','DistTurnZ_best']:
        df_agg[c] = np.nan

# ===== æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆæ­£è¦åŒ–ã¯å†…éƒ¨æŒ‡æ¨™ï¼‰ =====
# ã“ã“ã¯ RecencyZ / StabZ / PacePts / TurnPrefPts / DistTurnZ ãŒæƒã†â€œå¾Œâ€ã«ç½®ã

# 1) å¿…è¦åˆ—ã®ç”¨æ„ï¼ˆæœªç”Ÿæˆã§ã‚‚0ã§åŸ‹ã‚ã¦è½ã¡ãªã„ã‚ˆã†ã«ï¼‰
for c in ("PacePts", "TurnPrefPts", "DistTurnZ"):
    if c not in df_agg.columns:
        df_agg[c] = 0.0
df_agg[["PacePts", "TurnPrefPts", "DistTurnZ"]] = df_agg[["PacePts", "TurnPrefPts", "DistTurnZ"]].fillna(0.0)

# 2) RecencyZ / StabZ ã‚’ã“ã“ã§ç”Ÿæˆ
#    - RecencyZ ã¯ WAvgZï¼ˆãªã‘ã‚Œã° AvgZï¼‰ã‚’ZåŒ–
base_for_recency = (
    df_agg.get("WAvgZ", pd.Series(np.nan, index=df_agg.index))
      .fillna(df_agg.get("AvgZ", pd.Series(0.0, index=df_agg.index)))
)
df_agg["RecencyZ"] = z_score(pd.to_numeric(base_for_recency, errors="coerce").fillna(0.0))

#    - StabZ ã¯ã€Œå°ã•ã„ã»ã©â—ã€ãªã®ã§ WStd ã«ãƒã‚¤ãƒŠã‚¹ã‚’æ›ã‘ã¦ZåŒ–
if "WStd" not in df_agg.columns:
    df_agg["WStd"] = 6.0
wstd_fill = pd.to_numeric(df_agg["WStd"], errors="coerce")
if not np.isfinite(wstd_fill).any():
    wstd_fill = pd.Series(6.0, index=df_agg.index)
df_agg["StabZ"] = z_score(-(wstd_fill.fillna(wstd_fill.median())))

# 3) è·é›¢Ã—å›ã‚Š ä¿‚æ•°ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ãŒç„¡ã„/æœªåæ˜ ã§ã‚‚è½ã¡ãªã„ã‚ˆã†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
_dist_gain_from_state = st.session_state.get("dist_turn_gain", None)
if _dist_gain_from_state is None:
    # ãƒ­ãƒ¼ã‚«ãƒ«ã«å¤‰æ•°ãŒã‚ã‚‹ãªã‚‰ãã‚Œã‚’ä½¿ã†
    _dist_gain_from_state = float(locals().get("dist_turn_gain", 0.0))
dist_turn_gain_val = float(_dist_gain_from_state)

# 4) æœ€çµ‚ã‚¹ã‚³ã‚¢
df_agg["FinalRaw"] = (
    df_agg["RecencyZ"]
    + float(stab_weight) * df_agg["StabZ"]
    + float(pace_gain)   * df_agg["PacePts"]
    + float(turn_gain)   * df_agg["TurnPrefPts"]
    + float(dist_turn_gain_val) * df_agg["DistTurnZ"]
)
df_agg["FinalZ"] = z_score(df_agg["FinalRaw"])

# ===== NEW: AR100ï¼ˆBä¸­å¿ƒåŒ–ã®ç·šå½¢ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ =====
S = df_agg['FinalRaw'].to_numpy(dtype=float)
if np.all(~np.isfinite(S)) or len(S)==0:
    med = 0.0; qa = 1.0
else:
    med = float(np.nanmedian(S))
    p = max(0.01, min(0.99, 1.0 - float(band_A_share)/100.0))
    qa = float(np.nanquantile(S, p))
denom = (qa - med)
a = (70.0 - float(band_mid_target)) / (denom if abs(denom) > 1e-8 else 1.0)
df_agg['AR100'] = band_mid_target + a * (df_agg['FinalRaw'] - med)
df_agg['AR100'] = df_agg['AR100'].clip(band_clip_lo, band_clip_hi)

def to_band(x):
    try:
        v = float(x)
    except:
        return ''
    if v >= 90: return 'SS'
    if v >= 80: return 'S'
    if v >= 70: return 'A'
    if v >= 60: return 'B'
    if v >= 50: return 'C'
    return 'E'
df_agg['Band'] = df_agg['AR100'].map(to_band)

# ============================================================
# ========== ã“ã“ã‹ã‚‰ï¼šå‹ç‡ã¯ PLï¼ˆè§£æè§£ï¼‰ã«ç½®ãæ›ãˆ ==========
# ============================================================

def _softmax_vec(v: np.ndarray) -> np.ndarray:
    v = np.asarray(v, float)
    v = v - np.max(v)
    ev = np.exp(v)
    s = ev.sum()
    return ev / s if (s > 0 and np.isfinite(s)) else np.ones_like(ev)/len(ev)

# --- ï¼ˆä»»æ„ï¼‰éå»èµ°ã‹ã‚‰ã®ç­‰æ¸©å›å¸°/Plattæ ¡æ­£å™¨ã®å­¦ç¿’ ---
def _make_race_id_for_hist(df: pd.DataFrame) -> pd.Series:
    d = pd.to_datetime(df['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d').fillna('00000000')
    n = df['ç«¶èµ°å'].astype(str).fillna('')
    return d + '_' + n

def fit_isotonic_from_history(df_hist: pd.DataFrame, beta: float = 1.4):
    if not SK_ISO: 
        return None
    need = {'ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_norm','ç¢ºå®šç€é †'}
    if not need.issubset(df_hist.columns):
        return None
    df = df_hist.dropna(subset=['score_norm','ç¢ºå®šç€é †']).copy()
    df['race_id'] = _make_race_id_for_hist(df)
    X_list, y_list = [], []
    for rid, g in df.groupby('race_id'):
        s = g['score_norm'].astype(float).to_numpy()
        p = _softmax_vec(beta * s)
        y = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
        if len(p) == len(y) and len(p) >= 2:
            X_list.append(p); y_list.append(y)
    if not X_list:
        return None
    X = np.concatenate(X_list)
    Y = np.concatenate(y_list)
    ir = IsotonicRegression(out_of_bounds='clip')
    ir.fit(X, Y)
    return ir

def fit_platt_from_history(df_hist: pd.DataFrame, beta: float = 1.4):
    if not SK_PLATT:
        return None
    need = {'ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_norm','ç¢ºå®šç€é †'}
    if not need.issubset(df_hist.columns):
        return None
    df = df_hist.dropna(subset=['score_norm','ç¢ºå®šç€é †']).copy()
    df['race_id'] = _make_race_id_for_hist(df)
    X_list, y_list = [], []
    for rid, g in df.groupby('race_id'):
        s = g['score_norm'].astype(float).to_numpy()
        p = _softmax_vec(beta * s)
        y = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
        if len(p) == len(y) and len(p) >= 2:
            X_list.append(p.reshape(-1,1)); y_list.append(y)
    if not X_list:
        return None
    X = np.vstack(X_list)
    Y = np.concatenate(y_list)
    lr = LogisticRegression(max_iter=1000, solver='lbfgs')
    lr.fit(X, Y)
    return lr

# --- æœ¬ãƒ¬ãƒ¼ã‚¹ï¼šPLï¼ˆsoftmaxï¼‰ã§å‹ç‡ã€Top3ã¯è»½é‡MC ---
abilities = df_agg['FinalRaw'].to_numpy(float)
m = np.nanmean(abilities); s = np.nanstd(abilities) + 1e-9
abilities = (abilities - m) / s  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ã¿ï¼ˆé †ä½ã¯ä¸å¤‰ï¼‰

beta_pl = float(mc_beta)
x = beta_pl * (abilities - np.max(abilities))
ex = np.exp(x)
p_win_pl = ex / np.sum(ex)

# ï¼ˆä»»æ„ï¼‰æ ¡æ­£
calibrator = None
if do_calib:
    if calib_method.startswith("Isotonic") and SK_ISO:
        with st.spinner("æ ¡æ­£ï¼ˆIsotonicï¼‰ã‚’å­¦ç¿’ä¸­â€¦"):
            calibrator = fit_isotonic_from_history(df_score.copy(), beta=beta_pl)
    elif calib_method.startswith("Platt") and SK_PLATT:
        with st.spinner("æ ¡æ­£ï¼ˆPlattï¼‰ã‚’å­¦ç¿’ä¸­â€¦"):
            calibrator = fit_platt_from_history(df_score.copy(), beta=beta_pl)

if calibrator is not None:
    try:
        if isinstance(calibrator, IsotonicRegression):
            p_win_cal = calibrator.predict(p_win_pl)
        else:
            p_win_cal = calibrator.predict_proba(p_win_pl.reshape(-1,1))[:,1]
        p_win_pl = np.clip(p_win_cal, 1e-6, 1-1e-6)
    except Exception:
        pass

df_agg['å‹ç‡%_PL'] = (100 * p_win_pl).round(2)

# ---- Top3ã¯PLã®é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿‘ä¼¼ï¼ˆè»½é‡MCï¼‰ã€‚å‹ç‡ã¯PLãã®ã‚‚ã®ãªã®ã§MCã¯Top3ç”¨ã ã‘ã€‚
draws_top3 = int(mc_iters)
rng_top3 = np.random.default_rng(int(mc_seed) + 24601)
U = beta_pl * abilities[None, :] + rng_top3.gumbel(size=(draws_top3, len(abilities)))
rank_idx = np.argsort(-U, axis=1)
top3_counts = np.zeros(len(abilities), float)
for k in range(3):
    top3_counts += np.bincount(rank_idx[:, k], minlength=len(abilities)).astype(float)
df_agg['è¤‡å‹ç‡%_PL'] = (100 * (top3_counts / draws_top3)).round(2)

# ------------------------------------------------------------
# ï¼ˆã“ã®å…ˆï¼šæ•£å¸ƒå›³ã€ã‚³ãƒ¡ãƒ³ãƒˆã€ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã€ã‚¿ãƒ–UIã€EV/ã‚±ãƒªãƒ¼ã€è©•ä¾¡ã‚¿ãƒ–ç­‰ã¯ã€å¾ŒåŠã€‘ã§å·®ã—æ›¿ãˆï¼‰
# ------------------------------------------------------------

# ------------------------------------------------------------
# ã“ã“ã‹ã‚‰å¾ŒåŠï¼šUIã‚¿ãƒ–ï¼å¯è¦–åŒ–ï¼è©•ä¾¡ï¼æ ¡æ­£ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼è²·ã„ç›®ï¼ˆEV>1 + åˆ†æ•°ã‚±ãƒªãƒ¼ï¼‰
# ------------------------------------------------------------
# --- è¿½åŠ ï¼šæœ€çµ‚çµæœï¼ˆAR100 ãªã©ï¼‰ã®åˆ†å¸ƒã‚°ãƒ©ãƒ• ---
def render_result_distribution(df, col="AR100"):
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    s = pd.to_numeric(df[col], errors="coerce").dropna()
    if s.empty:
        st.info(f"{col} ã®å€¤ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœ€çµ‚çµæœã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚")
        return

    mean_v = float(s.mean())
    bins = min(30, max(10, int(len(s) ** 0.5) * 2))

    fig, ax = plt.subplots(figsize=(7, 3))
    ax.hist(s, bins=bins)
    ax.axvline(mean_v, linestyle="--")
    ax.set_xlabel(col)
    ax.set_ylabel("ä»¶æ•°")
    ax.grid(alpha=.3)
    st.pyplot(fig, use_container_width=True)

    st.caption(f"å¹³å‡ {col}: {mean_v:.2f}")
  

# â†‘ ã“ã“ãŒ render_result_distribution ã®æœ€å¾Œã®è¡Œ

# â–¼â–¼â–¼ ã“ã“ã«è¿½è¨˜ï¼šå‡ºèµ°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®å°é–¢æ•°ã¨ãƒ¬ãƒ³ãƒ€ãƒ© â–¼â–¼â–¼
def _pill(mark: str) -> str:
    color = {'â—':'#4f46e5','ã€‡':'#14b8a6','â—‹':'#14b8a6','â–²':'#3b82f6','â–³':'#94a3b8','Ã—':'#ef4444'}.get(str(mark), '#94a3b8')
    return (
        f"<span style='display:inline-flex;align-items:center;gap:6px;padding:3px 10px;"
        f"border-radius:999px;border:1px solid rgba(255,255,255,.12);"
        f"background:rgba(255,255,255,.05);color:{color};font-weight:700;'>{mark}</span>"
    )

def _bar(v, max_v=100):
    try:
        pct = max(0.0, min(100.0, float(v)/float(max_v)*100.0))
    except Exception:
        pct = 0.0
    return (
        "<div style='height:10px;border-radius:999px;background:#0f1830;"
        "border:1px solid rgba(255,255,255,.08);overflow:hidden'>"
        f"<span style='display:block;height:100%;width:{pct:.1f}%;"
        "background:linear-gradient(90deg,#4f46e5,#14b8a6)'></span></div>"
    )

def render_final_preview(df):
    """df ã¯ df_disp ã‚’æ¸¡ã™æƒ³å®šï¼ˆåˆ—ï¼šé¦¬åãƒ»å°ãƒ»AR100ãƒ»æ ãƒ»ç•ª ãªã©ï¼‰"""
    import pandas as pd, numpy as np
    for _, r in df.iterrows():
        name = str(r.get('é¦¬å',''))
        mark = str(r.get('å°',''))
        ar   = r.get('AR100', np.nan)
        ar_s = f"{float(ar):.1f}" if pd.notna(ar) else "-"
        info = []
        if pd.notna(r.get('æ ')): info.append(f"æ  {int(r['æ '])}")
        if pd.notna(r.get('ç•ª')): info.append(f"é¦¬ç•ª {int(r['ç•ª'])}")
        info = " / ".join(info)

        st.markdown(
            f"""
<div style='background:#121a2d;border:1px solid rgba(255,255,255,.06);border-radius:18px;
            padding:14px 16px;margin:8px 0;box-shadow:0 10px 25px rgba(0,0,0,.25)'>
  <div style='display:flex;justify-content:space-between;align-items:center;gap:12px;'>
    <div>
      <div style='display:flex;align-items:center;gap:10px;'>
        {_pill(mark)}
        <strong style='font-size:16px'>{name}</strong>
        <span style='color:#94a3b8;font-size:12px'>{info}</span>
      </div>
    </div>
    <div style='min-width:240px;text-align:right'>
      <div style='margin-bottom:6px'><span style='color:#94a3b8;font-size:12px'>AR100</span>
        <span style='font-variant-numeric:tabular-nums;font-weight:700;'>{ar_s}</span>
      </div>
      {_bar(ar, 100)}
    </div>
  </div>
</div>
""",
            unsafe_allow_html=True,
        )
# â–²â–²â–² ã“ã“ã¾ã§è¿½è¨˜ â–²â–²â–²

st.subheader("æœ¬æ—¥ã®è¦‹ç«‹ã¦")

# è¡¨ç¤ºç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
# è¡¨ç¤ºç”¨ã‚«ãƒ©ãƒ 
disp_cols = ['æ ','ç•ª','é¦¬å','è„šè³ª','PacePts','TurnPref','RightZ','LeftZ',
             'RecencyZ','StabZ','FinalRaw','FinalZ','AR100','Band','å‹ç‡%_PL','è¤‡å‹ç‡%_PL',
             # è¿½åŠ 
             'DistTurnZ','n_eff_turn','BestDist_turn'
]

# ...ï¼ˆä¸­ç•¥ï¼‰...

show_cols = ['é †ä½','å°','æ ','ç•ª','é¦¬å','è„šè³ª','AR100','Band',
             'å‹ç‡%_PL','è¤‡å‹ç‡%_PL','TurnPref','PacePts','RightZ','LeftZ','FinalZ',
             # è¿½åŠ 
             'DistTurnZ','n_eff_turn','BestDist_turn'
]

# === è¡¨ç¤ºç”¨ df_disp æ§‹ç¯‰ï¼ˆshow_cols å®šç¾©ã®ç›´å¾Œã«è¿½åŠ ï¼‰ ===

def _fmt_int(x):
    try:
        return "" if pd.isna(x) else f"{int(x)}"
    except Exception:
        return ""

# ä¸‹æµã§å‚ç…§ã™ã‚‹åˆ—ãŒæ¬ ã‘ã¦ã„ã¦ã‚‚è½ã¡ãªã„ã‚ˆã†ã«ä½œã£ã¦ãŠã
for c in ['PacePts','TurnPref','RightZ','LeftZ','RecencyZ','StabZ','FinalRaw','FinalZ',
          'AR100','Band','å‹ç‡%_PL','è¤‡å‹ç‡%_PL','DistTurnZ','n_eff_turn','BestDist_turn']:
    if c not in df_agg.columns:
        df_agg[c] = np.nan

# ä»Šæ—¥ã®æ ãƒ»é¦¬ç•ªãƒ»ï¼ˆæ‰‹å…¥åŠ›ã®ï¼‰è„šè³ªã‚’ãƒãƒ¼ã‚¸
df_disp = df_agg.merge(
    horses[['é¦¬å','æ ','ç•ª','è„šè³ª']],
    on='é¦¬å', how='left', suffixes=('', '_today')
)

# ã‚‚ã— df_agg å´ã«ã‚‚è„šè³ªãŒã‚ã‚‹ãªã‚‰ã€æ‰‹å…¥åŠ›ï¼ˆtodayï¼‰ã‚’å„ªå…ˆ
if 'è„šè³ª_today' in df_disp.columns:
    df_disp['è„šè³ª'] = df_disp['è„šè³ª_today'].where(
        df_disp['è„šè³ª_today'].astype(str).str.strip().ne(''),
        df_disp.get('è„šè³ª')
    )
    df_disp.drop(columns=['è„šè³ª_today'], inplace=True, errors='ignore')

# æ ãƒ»ç•ªã¯æ•°å€¤åŒ–ï¼ˆè¡¨ç¤ºæ™‚ã¯ _fmt_int ã§æ•´å½¢ï¼‰
for c in ['æ ','ç•ª']:
    df_disp[c] = pd.to_numeric(df_disp.get(c), errors='coerce')

# ä¸¦ã¹æ›¿ãˆï¼ˆAR100 â†’ å‹ç‡ï¼‰ã—ã¦é †ä½ã‚’ä»˜ä¸
df_disp = df_disp.sort_values(['AR100','å‹ç‡%_PL'], ascending=[False, False]).reset_index(drop=True)
df_disp['é †ä½'] = np.arange(1, len(df_disp)+1)

# å°ï¼ˆâ—â—‹â–²â–³ï¼‰ã‚’ä»˜ä¸ï¼š1ä½=â—ã€2ä½=â—‹ã€3ä½=â–²ã€4ï½6ä½=â–³
def _assign_mark(rank: int) -> str:
    if rank == 1: return 'â—'
    if rank == 2: return 'ã€‡'
    if rank == 3: return 'â–²'
    if 4 <= rank <= 6: return 'â–³'
    return ''
df_disp['å°'] = df_disp['é †ä½'].apply(_assign_mark)

# ä¸‹æµã®è¡¨ç¤º/å¯è¦–åŒ–ãŒæƒ³å®šã™ã‚‹åˆ—é †ã¸æ•´å½¢ï¼ˆå­˜åœ¨ã—ãªã„åˆ—ã¯ä¸Šã§è£œå®Œæ¸ˆã¿ï¼‰
df_disp = df_disp[['é †ä½','å°','æ ','ç•ª','é¦¬å','è„šè³ª','PacePts','TurnPref','RightZ','LeftZ',
                   'RecencyZ','StabZ','FinalRaw','FinalZ','AR100','Band','å‹ç‡%_PL','è¤‡å‹ç‡%_PL',
                   'DistTurnZ','n_eff_turn','BestDist_turn']].copy()

# æ•´å½¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ã«ã‚‚è¿½åŠ 
styled = (
    df_disp[show_cols]
      .style
      .apply(_style_waku, subset=['æ '])
      .format({
          'æ ': _fmt_int, 'ç•ª': _fmt_int,
          'AR100':'{:.1f}','å‹ç‡%_PL':'{:.2f}','è¤‡å‹ç‡%_PL':'{:.2f}',
          'FinalZ':'{:.2f}','RightZ':'{:.1f}','LeftZ':'{:.1f}','PacePts':'{:.2f}',
          # è¿½åŠ 
          'DistTurnZ':'{:.2f}','n_eff_turn':'{:.1f}','BestDist_turn': _fmt_int
      }, na_rep="")
)

st.dataframe(styled, use_container_width=True, height=H(df_disp, 560))


# ====================== ã‚¿ãƒ– ======================
tab_main, tab_vis, tab_eval, tab_calib, tab_bet = st.tabs(["ğŸ æœ¬å‘½","ğŸ“Š å¯è¦–åŒ–","ğŸ“ˆ è©•ä¾¡","ğŸ“ æ ¡æ­£","ğŸ’¸ è²·ã„ç›®"])

with tab_main:
    st.markdown("#### æœ¬å‘½ãƒ»å¯¾æŠ—ï¼ˆTopå€™è£œï¼‰")
    st.markdown("---")
    st.markdown("#### å…¨ä½“ã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆçµæœï¼‰")
    # AR100 ã®åˆ†å¸ƒã‚°ãƒ©ãƒ•ï¼ˆâ†’ å‹ç‡ã§è¦‹ãŸã‘ã‚Œã° col="å‹ç‡%_PL"ï¼‰
    render_result_distribution(df_disp, col="AR100")


    # è¡¨ç¤ºç”¨ï¼šä¸Šä½ã‚’æŠœç²‹
    top_cols = ['é †ä½','å°','æ ','ç•ª','é¦¬å','AR100','Band','å‹ç‡%_PL','è¤‡å‹ç‡%_PL','PacePts','TurnPref']
    top_view = df_disp[top_cols].head(6).copy()
    st.dataframe(
        top_view.style.format({'AR100':'{:.1f}','å‹ç‡%_PL':'{:.2f}','è¤‡å‹ç‡%_PL':'{:.2f}','PacePts':'{:.2f}'}),
        use_container_width=True, height=H(top_view, 360)
    )

    # è¦‹é€ã‚Šãƒ«ãƒ¼ãƒ«ã®ãƒ¡ãƒ¢
    if not (df_disp['AR100'] >= 70).any():
        st.warning("ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã¯ã€è¦‹é€ã‚Šã€ï¼šAä»¥ä¸Šï¼ˆAR100â‰¥70ï¼‰ãŒä¸åœ¨ã€‚")
    else:
        lead = top_view.iloc[0] if len(top_view)>0 else None
        if lead is not None:
            st.info(f"æœ¬å‘½å€™è£œï¼š**{int(lead['æ '])}-{int(lead['ç•ª'])} {lead['é¦¬å']}** / å‹ç‡{lead['å‹ç‡%_PL']:.2f}% / AR100 {lead['AR100']:.1f}")

    # 4ã‚³ãƒ¼ãƒŠãƒ¼é…ç½®å›³ï¼ˆè„šè³ªÃ—æ ã‹ã‚‰ã®æƒ³å®šï¼‰
    try:
        fig = render_corner_positions_nowrace(horses, combined_style, title="4ã‚³ãƒ¼ãƒŠãƒ¼æƒ³å®šãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆæœ¬ãƒ¬ãƒ¼ã‚¹ï¼‰")
        st.pyplot(fig)
    except Exception as e:
        st.caption(f"4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼š{e}")

with tab_vis:
    st.markdown("#### æ•£å¸ƒå›³ï¼ˆAR100 Ã— ãƒšãƒ¼ã‚¹é©æ€§ï¼‰")
    df_plot = df_disp[['é¦¬å','AR100','PacePts','å‹ç‡%_PL','è„šè³ª','æ ','ç•ª']].copy()
    df_plot = df_plot.dropna(subset=['AR100','PacePts'])
    if ALT_AVAILABLE and not df_plot.empty:
        ch = (
            alt.Chart(df_plot)
            .mark_circle()
            .encode(
                x=alt.X('PacePts:Q', title='PacePtsï¼ˆï¼‹ãŒè¿½è¾¼/å·®ã—æœ‰åˆ©ãƒ»âˆ’ãŒé€ƒã’/å…ˆè¡Œæœ‰åˆ©ï¼‰'),
                y=alt.Y('AR100:Q', title='AR100ï¼ˆBä¸­å¿ƒåŒ–æŒ‡æ•°ï¼‰'),
                size=alt.Size('å‹ç‡%_PL:Q', title='å‹ç‡%ï¼ˆPLï¼‰', scale=alt.Scale(range=[40, 600])),
                tooltip=['æ ','ç•ª','é¦¬å','è„šè³ª','AR100','å‹ç‡%_PL','PacePts']
            )
            .properties(height=480)
        )
        st.altair_chart(ch, use_container_width=True)
    elif not df_plot.empty:
        fig = plt.figure(figsize=(8,6))
        ax = fig.add_subplot(111)
        s = 40 + (df_plot['å‹ç‡%_PL'].fillna(0).to_numpy())*8
        ax.scatter(df_plot['PacePts'], df_plot['AR100'], s=s, alpha=0.6)
        for _, r in df_plot.iterrows():
            ax.annotate(str(r['ç•ª']), (r['PacePts'], r['AR100']), xytext=(3,3), textcoords="offset points", fontsize=8)
        ax.set_xlabel('PacePts')
        ax.set_ylabel('AR100')
        ax.grid(True, ls='--', alpha=0.3)
        st.pyplot(fig)
    else:
        st.info("å¯è¦–åŒ–ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

with tab_eval:
    st.markdown("#### éå»èµ°ã§ã®å¦¥å½“æ€§è©•ä¾¡ï¼ˆãƒ¬ãƒ¼ã‚¹å†…softmaxï¼‰")

    def history_probs_and_labels(df_hist: pd.DataFrame, beta: float = 1.4, calibrator_obj=None):
        need = {'ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_norm','ç¢ºå®šç€é †'}
        if not need.issubset(df_hist.columns): 
            return np.array([]), np.array([])
        dfh = df_hist.dropna(subset=['score_norm','ç¢ºå®šç€é †']).copy()
        dfh['race_id'] = (pd.to_datetime(dfh['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d').fillna('00000000')
                          + '_' + dfh['ç«¶èµ°å'].astype(str).fillna(''))
        probs = []; labels = []
        for rid, g in dfh.groupby('race_id'):
            s = g['score_norm'].astype(float).to_numpy()
            p = _softmax_vec(beta * s)
            y = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
            if len(p)==len(y) and len(p)>=2:
                if calibrator_obj is not None:
                    try:
                        if isinstance(calibrator_obj, IsotonicRegression):
                            p = calibrator_obj.predict(p)
                        else:
                            p = calibrator_obj.predict_proba(p.reshape(-1,1))[:,1]
                        p = np.clip(p, 1e-6, 1-1e-6)
                    except Exception:
                        pass
                probs.append(p); labels.append(y)
        if not probs:
            return np.array([]), np.array([])
        return np.concatenate(probs), np.concatenate(labels)

    def brier_score(p, y):
        p = np.clip(p, 1e-9, 1-1e-9); y = np.asarray(y, float)
        return float(np.mean((p - y)**2))

    def log_loss_bin(p, y):
        p = np.clip(p, 1e-9, 1-1e-9); y = np.asarray(y, float)
        return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))

    # Raw
    p_raw, y = history_probs_and_labels(df_score.copy(), beta=beta_pl, calibrator_obj=None)
    # Calibratedï¼ˆIf availableï¼‰
    p_cal, _ = history_probs_and_labels(df_score.copy(), beta=beta_pl, calibrator_obj=calibrator) if (calibrator is not None) else (np.array([]), None)

    if p_raw.size == 0:
        st.info("è©•ä¾¡ã«ååˆ†ãªéå»èµ°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        bs_raw = brier_score(p_raw, y)
        ll_raw = log_loss_bin(p_raw, y)
        st.write(f"**Brier**: {bs_raw:.4f}ã€€/ã€€**LogLoss**: {ll_raw:.4f}ï¼ˆæœªæ ¡æ­£ï¼‰")
        if p_cal.size > 0:
            bs_cal = brier_score(p_cal, y)
            ll_cal = log_loss_bin(p_cal, y)
            st.write(f"**Brier**: {bs_cal:.4f}ã€€/ã€€**LogLoss**: {ll_cal:.4f}ï¼ˆæ ¡æ­£å¾Œï¼‰")

        # Reliability Diagram
        def reliability_points(p, y, n_bins=10):
            bins = np.linspace(0,1,n_bins+1)
            idx = np.digitize(p, bins)-1
            xs, ys, ns = [], [], []
            for b in range(n_bins):
                m = idx==b
                if m.sum() == 0: 
                    continue
                xs.append(p[m].mean())
                ys.append(y[m].mean())
                ns.append(int(m.sum()))
            return np.array(xs), np.array(ys), np.array(ns)

        fig = plt.figure(figsize=(6,6))
        ax = fig.add_subplot(111)
        ax.plot([0,1],[0,1], ls='--', alpha=0.5)
        x1,y1,n1 = reliability_points(p_raw, y, n_bins=12)
        ax.plot(x1, y1, marker='o', label='Raw')
        if p_cal.size > 0:
            x2,y2,n2 = reliability_points(p_cal, y, n_bins=12)
            ax.plot(x2, y2, marker='o', label='Calibrated')
        ax.set_xlabel('å¹³å‡ äºˆæ¸¬å‹ç‡')
        ax.set_ylabel('å®Ÿæ¸¬ å‹ç‡ï¼ˆTop1ï¼‰')
        ax.grid(True, ls=':', alpha=0.3)
        ax.legend()
        st.pyplot(fig)

        # NDCG@3ï¼ˆå‚è€ƒï¼‰
        try:
            df_tmp = df_score[['ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_norm','ç¢ºå®šç€é †']].dropna().copy()
            df_tmp['race_id'] = (pd.to_datetime(df_tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d') + '_' + df_tmp['ç«¶èµ°å'].astype(str))
            df_tmp['y'] = (pd.to_numeric(df_tmp['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int)
            ndcg_raw = ndcg_by_race(df_tmp[['race_id','y']], p_raw, k=3)
            st.caption(f"NDCG@3ï¼ˆæœªæ ¡æ­£softmaxï¼‰ï¼š{ndcg_raw:.4f}")
        except Exception:
            pass

with tab_calib:
    st.markdown("#### ç¢ºç‡æ ¡æ­£ã®çŠ¶æ³")
    if not do_calib:
        st.info("ç¾åœ¨ã¯**æœªæ ¡æ­£**ã§ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒğŸ“ ç¢ºç‡æ ¡æ­£ã€ã‚’ONã«ã™ã‚‹ã¨ã€ç­‰æ¸©å›å¸°ï¼ˆæ¨å¥¨ï¼‰ã¾ãŸã¯Plattã§æ ¡æ­£ã—ã¾ã™ã€‚")
    else:
        if calibrator is None:
            st.warning("æ ¡æ­£å™¨ã®å­¦ç¿’ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚sheet0ã«éå»èµ°ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã®ç€é †ãƒ»score_normï¼‰ãŒå¿…è¦ã§ã™ã€‚")
        else:
            method = "IsotonicRegression" if isinstance(calibrator, IsotonicRegression) else "Platt (Logistic)"
            st.success(f"æ ¡æ­£å™¨ï¼š**{method}** ã‚’ä½¿ç”¨ä¸­ã€‚")
            st.write("- è©•ä¾¡ã‚¿ãƒ–ã§Brier/LogLossã¨ä¿¡é ¼åº¦æ›²ç·šï¼ˆReliabilityï¼‰ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
            st.write("- ç¾åœ¨ã®å‹ç‡åˆ—ï¼š**å‹ç‡%_PL**ï¼ˆæ ¡æ­£å¾ŒãŒã‚ã‚Œã°ãã‚Œã‚’åæ˜ ï¼‰")

with tab_bet:
    st.markdown("#### ã‚ªãƒƒã‚ºå…¥åŠ›ï¼ˆå°æ•°ï¼æ‰•ã„æˆ»ã—ç·é¡å€ç‡ï¼‰")
    st.caption("ä¾‹ï¼šå˜å‹3.5å€ â†’ 3.5ã€è¤‡å‹2.1å€ â†’ 2.1ã€‚æœªå…¥åŠ›ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚")
    # ã‚ªãƒƒã‚ºç·¨é›†ãƒ†ãƒ¼ãƒ–ãƒ«
    if 'odds_df' not in st.session_state:
        st.session_state['odds_df'] = df_disp[['æ ','ç•ª','é¦¬å']].copy().assign(å˜ã‚ªãƒƒã‚º=np.nan, è¤‡ã‚ªãƒƒã‚º=np.nan)
    # ãƒãƒ¼ã‚¸ã§æ–°é¦¬è¿½åŠ /é †åºåŒæœŸ
    odds_merge = df_disp[['æ ','ç•ª','é¦¬å']].merge(st.session_state['odds_df'], on=['æ ','ç•ª','é¦¬å'], how='left')
    odds_merge[['å˜ã‚ªãƒƒã‚º','è¤‡ã‚ªãƒƒã‚º']] = odds_merge[['å˜ã‚ªãƒƒã‚º','è¤‡ã‚ªãƒƒã‚º']].astype(float)
    odds_edit = st.data_editor(
        odds_merge,
        column_config={
            'å˜ã‚ªãƒƒã‚º': st.column_config.NumberColumn('å˜ã‚ªãƒƒã‚º', min_value=1.01, step=0.01),
            'è¤‡ã‚ªãƒƒã‚º': st.column_config.NumberColumn('è¤‡ã‚ªãƒƒã‚º', min_value=1.01, step=0.01),
        },
        hide_index=True,
        height=H(odds_merge, 460),
        use_container_width=True,
        key="odds_editor",
    )
    st.session_state['odds_df'] = odds_edit[['æ ','ç•ª','é¦¬å','å˜ã‚ªãƒƒã‚º','è¤‡ã‚ªãƒƒã‚º']].copy()

    st.markdown("---")
    kelly_frac = st.slider("åˆ†æ•°ã‚±ãƒªãƒ¼ä¿‚æ•°", 0.05, 1.0, 0.5, 0.05, help="0.5=ãƒãƒ¼ãƒ•ã‚±ãƒªãƒ¼ã€‚éç©ã¿ã‚’é˜²ãä¿‚æ•°ã€‚")
    alloc_mode = st.selectbox("é…åˆ†å¯¾è±¡", ["å˜å‹ã®ã¿","è¤‡å‹ã®ã¿","å˜å‹ï¼‹è¤‡å‹"], index=2)
    st.caption(f"ç·äºˆç®—: **{total_budget} å††** / æœ€å°å˜ä½: **{min_unit} å††** / æœ€å¤§ç‚¹æ•°: **{max_lines}**")

    # æœŸå¾…å€¤ & ã‚±ãƒªãƒ¼è¨ˆç®—
    picks = []
    p_win = (df_disp['å‹ç‡%_PL'].to_numpy()/100.0)
    p_plc = (df_disp['è¤‡å‹ç‡%_PL'].to_numpy()/100.0)
    names = df_disp['é¦¬å'].tolist()
    waku  = df_disp['æ '].tolist()
    umaban= df_disp['ç•ª'].tolist()

    for i, row in odds_edit.reset_index(drop=True).iterrows():
        name = names[i]; w=waku[i]; n=umaban[i]
        # å˜å‹
        o_t = row.get('å˜ã‚ªãƒƒã‚º', np.nan)
        if pd.notna(o_t) and alloc_mode in ("å˜å‹ã®ã¿","å˜å‹ï¼‹è¤‡å‹"):
            p = float(p_win[i]); b = float(o_t) - 1.0; q = 1.0 - p
            ev_roi = p * float(o_t) - 1.0                      # ROIï¼ˆ1ãªã‚‰ç­‰ä¾¡ã€>0ã§+EVï¼‰
            f = max(0.0, (b*p - q) / max(1e-9, b))             # Kelly fraction
            if ev_roi > 0 and f > 0:
                picks.append({'ç¨®åˆ¥':'å˜å‹','æ ':w,'ç•ª':n,'é¦¬å':name,'p':p,'odds':float(o_t),'EV_ROI':ev_roi,'kelly_f':f})
        # è¤‡å‹ï¼ˆç°¡æ˜“ï¼šTop3ç¢ºç‡Ã—è¤‡å‹ã‚ªãƒƒã‚ºï¼‰
        o_p = row.get('è¤‡ã‚ªãƒƒã‚º', np.nan)
        if pd.notna(o_p) and alloc_mode in ("è¤‡å‹ã®ã¿","å˜å‹ï¼‹è¤‡å‹"):
            p = float(p_plc[i]); b = float(o_p) - 1.0; q = 1.0 - p
            ev_roi = p * float(o_p) - 1.0
            f = max(0.0, (b*p - q) / max(1e-9, b))
            if ev_roi > 0 and f > 0:
                picks.append({'ç¨®åˆ¥':'è¤‡å‹','æ ':w,'ç•ª':n,'é¦¬å':name,'p':p,'odds':float(o_p),'EV_ROI':ev_roi,'kelly_f':f})

    if not picks:
        st.warning("ç¾çŠ¶ã€EV>0ï¼ˆ= pÃ—ã‚ªãƒƒã‚º > 1ï¼‰ã¨ãªã‚‹å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ªãƒƒã‚ºã‚’å…¥åŠ›/æ›´æ–°ã—ã¦å†è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚")
    else:
        df_picks = pd.DataFrame(picks)
        # ä¸Šé™ç‚¹æ•°ã§ã‚«ãƒƒãƒˆï¼ˆæœŸå¾…å€¤ã®é«˜ã„é †ï¼‰
        df_picks.sort_values(['EV_ROI','p','odds'], ascending=[False, False, True], inplace=True)
        df_picks = df_picks.head(int(max_lines)).copy()

        # åˆ†æ•°ã‚±ãƒªãƒ¼ã«åŸºã¥ãé‡ã¿ â†’ äºˆç®—ã§ã‚¹ã‚±ãƒ¼ãƒ«
        df_picks['w'] = df_picks['kelly_f'] * float(kelly_frac)
        sw = df_picks['w'].sum()
        if sw <= 0:
            st.warning("ã‚±ãƒªãƒ¼åŸºæº–ã®é‡ã¿ãŒã‚¼ãƒ­ã§ã™ã€‚ã‚±ãƒªãƒ¼ä¿‚æ•°ã‚„ã‚ªãƒƒã‚ºãƒ»ç¢ºç‡ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            budget = float(total_budget)
            raw_alloc = budget * (df_picks['w'] / sw)
            # æœ€å°å˜ä½ã§ä¸¸ã‚
            stake = (np.floor(raw_alloc / float(min_unit)) * float(min_unit)).astype(int)
            # 0è¡Œã‚’å‰Šé™¤ã—ã™ããªã„ã‚ˆã†ã€æœ€ä½1å˜ä½ã‚’é«˜EVã‹ã‚‰é †ã«é…å¸ƒ
            deficit = int(budget - stake.sum())
            if deficit >= min_unit:
                # ä½™ã‚Šã‚’é«˜EVé †ã«å‰²å½“
                order = df_picks['EV_ROI'].rank(method='first', ascending=False).astype(int)
                idx_order = np.argsort(order.values)
                for j in idx_order:
                    if deficit < min_unit: break
                    stake.iloc[j] += int(min_unit)
                    deficit -= int(min_unit)
            df_picks['æ¨å¥¨é‡‘é¡(å††)'] = stake
            df_picks['æƒ³å®šæœŸå¾…åç›Š(å††)'] = (df_picks['æ¨å¥¨é‡‘é¡(å††)'] * (df_picks['p'] * df_picks['odds'] - 1.0)).round(1)

            # å‡ºåŠ›
            show_bet_cols = ['ç¨®åˆ¥','æ ','ç•ª','é¦¬å','odds','p','EV_ROI','kelly_f','æ¨å¥¨é‡‘é¡(å††)','æƒ³å®šæœŸå¾…åç›Š(å††)']
            st.dataframe(
                df_picks[show_bet_cols].rename(columns={'odds':'ã‚ªãƒƒã‚º','p':'å‹ç‡(ã¾ãŸã¯Top3ç¢ºç‡)','EV_ROI':'æœŸå¾…ROI','kelly_f':'ã‚±ãƒªãƒ¼ç‡'})
                    .style.format({'ã‚ªãƒƒã‚º':'{:.2f}','å‹ç‡(ã¾ãŸã¯Top3ç¢ºç‡)':'{:.3f}','æœŸå¾…ROI':'{:.3f}','ã‚±ãƒªãƒ¼ç‡':'{:.3f}'}),
                use_container_width=True, height=H(df_picks, 480)
            )
            st.write(f"**åˆè¨ˆæŠ•å…¥é¡:** {int(df_picks['æ¨å¥¨é‡‘é¡(å††)'].sum())} å†† / äºˆç®— {int(budget)} å††")

            bet_csv = df_picks[show_bet_cols].to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ§¾ è²·ã„ç›®CSVã‚’ä¿å­˜", data=bet_csv, file_name="bets_kelly.csv", mime="text/csv")

# ============== ä»•ä¸Šã’ãƒ¡ãƒ¢ ==============
st.markdown("""
<small>
- å‹ç‡ã¯ãƒ¬ãƒ¼ã‚¹å†… **Plackettâ€“Luceï¼ˆsoftmaxï¼‰** ã®è§£æè§£ã§ã™ã€‚Top3ï¼ˆè¤‡å‹ç‡ï¼‰ã¯**è»½é‡Gumbelã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ã§è¿‘ä¼¼ã—ã¦ã„ã¾ã™ã€‚<br>
- ã€ŒğŸ“ ç¢ºç‡æ ¡æ­£ã€ã‚’ONã«ã™ã‚‹ã¨ã€sheet0ï¼ˆéå»èµ°ï¼‰ã‹ã‚‰**ç­‰æ¸©å›å¸°/Platt**ã§æ ¡æ­£ã‚«ãƒ¼ãƒ–ã‚’å­¦ç¿’ã—ã€ç¾è¡Œãƒ¬ãƒ¼ã‚¹å‹ç‡ã«åæ˜ ã—ã¾ã™ã€‚<br>
- ã€ŒğŸ’¸ è²·ã„ç›®ã€ã¯**EV>0**ã‹ã¤åˆ†æ•°ã‚±ãƒªãƒ¼ã§äºˆç®—é…åˆ†ã€‚ã‚ªãƒƒã‚ºã¯å°æ•°ï¼ˆæ‰•æˆ»ç·é¡å€ç‡ï¼‰ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
</small>
""", unsafe_allow_html=True)
