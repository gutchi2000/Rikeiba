# keiba_web_app_fix.py
# ã‚µã‚¤ãƒ‰ãƒãƒ¼äº’æ›ï¼ˆexpanderï¼‰ã€ç¸¦è»¸å …ç‰¢åŒ–ã€å¹´é½¢/æ é‡ã¿ãƒ»MCãƒ»è¡€çµ±HTML å®Œå‚™ã®å³å‹•ç‰ˆ
import streamlit as st
import pandas as pd
import numpy as np
import re, io, json
import matplotlib.pyplot as plt
from matplotlib import font_manager
from itertools import combinations

# ===== Optional: Altairï¼ˆæ•£å¸ƒå›³ï¼‰ =====
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

# ===== Optional: HTML åŸ‹ã‚è¾¼ã¿ï¼ˆè¡€çµ±è¡¨ç¤ºç”¨ï¼‰ =====
try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

# ---- åŸºæœ¬è¨­å®šã¨ãƒ•ã‚©ãƒ³ãƒˆ ----
@st.cache_resource
def get_jp_font():
    for path in ["ipaexg.ttf", "C:/Windows/Fonts/meiryo.ttc"]:
        try:
            return font_manager.FontProperties(fname=path)
        except Exception:
            pass
    return None

jp_font = get_jp_font()
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['IPAPGothic', 'Meiryo', 'MS Gothic']
st.set_page_config(page_title="ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰", layout="wide")

# ---- ä¾¿åˆ©CSSï¼ˆsidebar å¹…ã ã‘èª¿æ•´ï¼‰----
st.markdown("""
<style>
#MainMenu, footer {visibility:hidden;}
section[data-testid="stSidebar"] {width: 340px !important;}
div.block-container {padding-top: .6rem; padding-bottom: .8rem; max-width: 1400px;}
</style>
""", unsafe_allow_html=True)

STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fwid = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

# ======================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========================
def season_of(month: int) -> str:
    if 3 <= month <= 5: return 'æ˜¥'
    if 6 <= month <= 8: return 'å¤'
    if 9 <= month <= 11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    std = s.std(ddof=0)
    if std == 0 or pd.isna(std):
        return pd.Series([50]*len(s), index=s.index)
    return 50 + 10 * (s - s.mean()) / std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + float('0.'+m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + int(m.group(3))/10
    try:  return float(s)
    except: return np.nan

def normalize_grade_text(x: str | None) -> str | None:
    if x is None or (isinstance(x, float) and np.isnan(x)): 
        return None
    s = str(x).translate(_fwid)
    s = (s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
         .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s = re.sub(r'G\s*III', 'G3', s, flags=re.I)
    s = re.sub(r'G\s*II',  'G2', s, flags=re.I)
    s = re.sub(r'G\s*I',   'G1', s, flags=re.I)
    s = re.sub(r'ï¼ªï¼°ï¼®', 'Jpn', s, flags=re.I)
    s = re.sub(r'JPN',    'Jpn', s, flags=re.I)
    s = re.sub(r'Jpn\s*III', 'Jpn3', s, flags=re.I)
    s = re.sub(r'Jpn\s*II',  'Jpn2', s, flags=re.I)
    s = re.sub(r'Jpn\s*I',   'Jpn1', s, flags=re.I)
    m = re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def safe_take(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    return df[[c for c in cols if c in df.columns]].copy()

def _trim_name(x):
    try:
        return str(x).replace('\u3000',' ').strip()
    except Exception:
        return str(x)

# å®‰å®šã—ãŸé‡ã¿ä»˜ãæ¨™æº–åå·®ï¼ˆä¸åè£œæ­£ã¤ãï¼‰
def w_std_unbiased(x, w, ddof=1):
    x = np.asarray(x, dtype=float)
    w = np.asarray(w, dtype=float)
    sw = w.sum()
    if not np.isfinite(sw) or sw <= 0: return np.nan
    m = np.sum(w * x) / sw
    var = np.sum(w * (x - m)**2) / sw
    n_eff = (sw**2) / np.sum(w**2) if np.sum(w**2) > 0 else 0
    if ddof and n_eff > ddof:
        var *= n_eff / (n_eff - ddof)
    return float(np.sqrt(max(var, 0.0)))

@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    return s0, s1

def validate_inputs(df_score: pd.DataFrame, horses: pd.DataFrame):
    problems = []
    req = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
    for c in req:
        if c not in df_score.columns:
            problems.append(f"sheet0 å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {c}")
    if 'æ–¤é‡' in horses:
        bad = horses['æ–¤é‡'].dropna()
        if len(bad)>0 and ((bad<45)|(bad>65)).any():
            problems.append("sheet1 æ–¤é‡ãŒãƒ¬ãƒ³ã‚¸å¤–ï¼ˆ45â€“65ï¼‰")
    if {'é€šé4è§’','é ­æ•°'}.issubset(df_score.columns):
        tmp = df_score[['é€šé4è§’','é ­æ•°']].dropna()
        if len(tmp)>0 and ((tmp['é€šé4è§’']<1) | (tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
            problems.append("sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–")
    if problems: st.warning("âš  å…¥åŠ›ãƒã‚§ãƒƒã‚¯ï¼š\n- " + "\n- ".join(problems))

# ======================== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚¿ãƒ–ãªã—ãƒ»äº’æ›ç¬¬ä¸€ï¼‰ ========================
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®š")

with st.sidebar.expander("ğŸ”° ã‚ˆãä½¿ã†ï¼ˆåŸºæœ¬ï¼‰", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    besttime_w   = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿", 0.0, 2.0, 1.0)
    with st.expander("æˆ¦ç¸¾ç‡ã®é‡ã¿ï¼ˆå½“è©²é¦¬å ´ï¼‰", expanded=False):
        win_w  = st.slider("å‹ç‡ã®é‡ã¿",   0.0, 5.0, 1.0, 0.1, key="w_win")
        quin_w = st.slider("é€£å¯¾ç‡ã®é‡ã¿", 0.0, 5.0, 0.7, 0.1, key="w_quin")
        plc_w  = st.slider("è¤‡å‹ç‡ã®é‡ã¿", 0.0, 5.0, 0.5, 0.1, key="w_plc")
    with st.expander("å„ç¨®ãƒœãƒ¼ãƒŠã‚¹è¨­å®š", expanded=False):
        grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
        agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
        agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
        agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)
        bw_bonus     = st.slider("é¦¬ä½“é‡é©æ­£ãƒœãƒ¼ãƒŠã‚¹(Â±10kg)", 0, 10, 2)
    with st.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆBTé‡ã¿ç”¨ï¼‰", expanded=True):
        TARGET_GRADE    = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", ["G1", "G2", "G3", "L", "OP"], index=4)
        TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ", "ãƒ€"], index=0)
        TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", min_value=1000, max_value=3600, value=1800, step=100)

with st.sidebar.expander("ğŸ›  è©³ç´°ï¼ˆè£œæ­£/è„šè³ª/ãƒšãƒ¼ã‚¹ï¼‰", expanded=False):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)

    with st.expander("æ–¤é‡ãƒ™ãƒ¼ã‚¹ï¼ˆWFAç°¡ç•¥ï¼‰", expanded=False):
        race_date = pd.to_datetime(st.date_input("é–‹å‚¬æ—¥", value=pd.Timestamp.today().date()))
        use_wfa_base = st.checkbox("WFAåŸºæº–ã‚’ä½¿ã†ï¼ˆæ¨å¥¨ï¼‰", value=True)
        wfa_2_early_m = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 55.0, 0.5)
        wfa_2_early_f = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 54.0, 0.5)
        wfa_2_late_m  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 56.0, 0.5)
        wfa_2_late_f  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 55.0, 0.5)
        wfa_3p_m      = st.number_input("3æ­³ä»¥ä¸Š ç‰¡/ã›ã‚“ [kg]" , 50.0, 62.0, 57.0, 0.5)
        wfa_3p_f      = st.number_input("3æ­³ä»¥ä¸Š ç‰ [kg]"     , 48.0, 60.0, 55.0, 0.5)

    with st.expander("å±æ€§é‡ã¿ï¼ˆ1èµ°ã‚¹ã‚³ã‚¢ä¿‚æ•°ï¼‰", expanded=False):
        gender_w = {g: st.slider(f"{g}", 0.0, 2.0, 1.0) for g in ['ç‰¡','ç‰','ã‚»']}
        style_w  = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in STYLES}
        season_w = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in ['æ˜¥','å¤','ç§‹','å†¬']}
        age_w    = {str(age): st.slider(f"{age}æ­³", 0.0, 2.0, 1.0, 0.05) for age in range(3, 11)}
        frame_w  = {str(i): st.slider(f"{i}æ ", 0.0, 2.0, 1.0) for i in range(1,9)}

    with st.expander("è„šè³ªè‡ªå‹•æ¨å®š / ãƒšãƒ¼ã‚¹æ¨å®š", expanded=False):
        auto_style_on   = st.checkbox("è„šè³ªã®è‡ªå‹•æ¨å®šã‚’ä½¿ã†ï¼ˆç©ºæ¬„ã‚’åŸ‹ã‚ã‚‹ï¼‰", True)
        AUTO_OVERWRITE  = st.checkbox("æ‰‹å…¥åŠ›ã‚ˆã‚Šè‡ªå‹•ã‚’å„ªå…ˆã—ã¦ä¸Šæ›¸ã", False)
        NRECENT         = st.slider("ç›´è¿‘ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆè„šè³ªæ¨å®šï¼‰", 1, 10, 5)
        HL_DAYS_STYLE   = st.slider("åŠæ¸›æœŸï¼ˆæ—¥ãƒ»è„šè³ªç”¨ï¼‰", 30, 365, 180, 15)
        pace_mc_draws   = st.slider("ãƒšãƒ¼ã‚¹MCå›æ•°", 500, 30000, 5000, 500)
        pace_mode = st.radio("ãƒšãƒ¼ã‚¹ã®æ‰±ã„", ["è‡ªå‹•ï¼ˆMCï¼‰", "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰"], index=0)
        pace_fixed = st.selectbox("å›ºå®šãƒšãƒ¼ã‚¹", ["ãƒã‚¤ãƒšãƒ¼ã‚¹","ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹","ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹","ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"],
                                  index=1, disabled=(pace_mode=="è‡ªå‹•ï¼ˆMCï¼‰"))
        epi_alpha = st.slider("é€ƒã’ä¿‚æ•° Î±", 0.0, 2.0, 1.0, 0.05)
        epi_beta  = st.slider("å…ˆè¡Œä¿‚æ•° Î²", 0.0, 2.0, 0.60, 0.05)
        thr_hi    = st.slider("é–¾å€¤: ãƒã‚¤ãƒšãƒ¼ã‚¹ â‰¥", 0.30, 1.00, 0.52, 0.01)
        thr_mid   = st.slider("é–¾å€¤: ãƒŸãƒ‰ãƒ« â‰¥",    0.10, 0.99, 0.30, 0.01)
        thr_slow  = st.slider("é–¾å€¤: ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ â‰¥",0.00, 0.98, 0.18, 0.01)

with st.sidebar.expander("ğŸ§ª ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ / ä¿å­˜", expanded=False):
    mc_iters   = st.slider("å‹ç‡MC åå¾©å›æ•°", 1000, 100000, 20000, 1000)
    mc_beta    = st.slider("å¼·ã•â†’å‹ç‡ æ¸©åº¦Î²", 0.1, 5.0, 1.5, 0.1)
    mc_tau     = st.slider("å®‰å®šåº¦ãƒã‚¤ã‚ºä¿‚æ•° Ï„", 0.0, 2.0, 0.6, 0.05)
    mc_seed    = st.number_input("ä¹±æ•°Seed", 0, 999999, 42, 1)
    st.markdown("---")
    total_budget = st.slider("åˆè¨ˆäºˆç®—", 500, 50000, 10000, 100)
    min_unit     = st.selectbox("æœ€å°è³­ã‘å˜ä½", [100, 200, 300, 500], index=0)
    max_lines    = st.slider("æœ€å¤§ç‚¹æ•°(é€£ç³»)", 1, 60, 20, 1)
    scenario     = st.selectbox("ã‚·ãƒŠãƒªã‚ª", ['é€šå¸¸','ã¡ã‚‡ã„ä½™è£•','ä½™è£•'])
    st.markdown("---")

    # ---- JSONä¿å­˜ã‚’å®‰å…¨åŒ–ï¼ˆéã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå“ã‚’æ–‡å­—åˆ—åŒ–ï¼‰ ----
    def _jsonable(x):
        import numpy as _np
        if isinstance(x, (str, int, float, bool)) or x is None:
            return x
        if isinstance(x, (_np.integer,)):
            return int(x)
        if isinstance(x, (_np.floating,)):
            return float(x)
        if isinstance(x, (_np.bool_,)):
            return bool(x)
        if isinstance(x, (list, tuple, set)):
            return [_jsonable(i) for i in x]
        if isinstance(x, dict):
            return {str(k): _jsonable(v) for k, v in x.items()}
        return str(x)

    col_a, col_b = st.columns(2)
    if col_a.button("è¨­å®šã‚’ä¿å­˜"):
        try:
            cfg_dict = {}
            for k, v in st.session_state.items():
                if str(k).startswith('_'):
                    continue
                if k in ('excel_up','cfg_up','pedi_html_up'):
                    continue
                cfg_dict[k] = _jsonable(v)
            cfg = json.dumps(cfg_dict, ensure_ascii=False, indent=2)
            st.download_button("JSONã‚’DL", data=cfg, file_name="keiba_config.json", mime="application/json")
        except Exception as e:
            st.error(f"è¨­å®šä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    cfg_file = col_b.file_uploader("è¨­å®šèª­ã¿è¾¼ã¿", type=["json"], key="cfg_up")
    if cfg_file is not None:
        try:
            cfg = json.loads(cfg_file.read().decode("utf-8"))
            for k,v in cfg.items():
                st.session_state[k]=v
            st.success("è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆå¿…è¦ãªã‚‰å†å®Ÿè¡Œï¼‰ã€‚")
        except Exception as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ======================== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ========================
st.title("ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰")
st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
excel_file = st.file_uploader("Excelï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

sheet0, sheet1 = load_excel_bytes(excel_file.getvalue())

# === sheet0 / sheet1 ãƒãƒƒãƒ”ãƒ³ã‚° ===
def _norm_col(s: str) -> str:
    s = str(s).strip()
    s = re.sub(r'\s+', '', s)
    s = s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789'))
    s = s.replace('ï¼ˆ','(').replace('ï¼‰',')').replace('ï¼…','%')
    return s
def _auto_guess(col_map, pats):
    for orig, normed in col_map.items():
        for p in pats:
            if re.search(p, normed, flags=re.I):
                return orig
    return None
def _interactive_map(df, patterns, required_keys, title, state_key, show_ui=False):
    cols = list(df.columns)
    cmap = {c: _norm_col(c) for c in cols}
    auto = {k: st.session_state.get(f"{state_key}:{k}") or _auto_guess(cmap, pats)
            for k, pats in patterns.items()}
    if not show_ui:
        missing = [k for k in required_keys if not auto.get(k)]
        if not missing:
            for k, v in auto.items():
                if v: st.session_state[f"{state_key}:{k}"] = v
            return auto
        else:
            st.warning(f"{title} ã®å¿…é ˆåˆ—ãŒè‡ªå‹•èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ: " + ", ".join(missing))
            show_ui = True
    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=True):
        mapping = {}
        for key, pats in patterns.items():
            default = st.session_state.get(f"{state_key}:{key}") or auto.get(key)
            mapping[key] = st.selectbox(
                key, options=['<æœªé¸æŠ>'] + cols,
                index=(['<æœªé¸æŠ>']+cols).index(default) if default in cols else 0,
                key=f"map:{state_key}:{key}")
            if mapping[key] != '<æœªé¸æŠ>':
                st.session_state[f"{state_key}:{key}"] = mapping[key]
    missing = [k for k in required_keys if mapping.get(k) in (None, '<æœªé¸æŠ>')]
    if missing: st.stop()
    return {k: (None if v=='<æœªé¸æŠ>' else v) for k, v in mapping.items()}

PAT_S0 = {
    'é¦¬å'         : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥'     : [r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜S|æ—¥ä»˜|å¹´æœˆæ—¥'],
    'ç«¶èµ°å'       : [r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å'     : [r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°'         : [r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †'     : [r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ '           : [r'æ |æ ç•ª'],
    'ç•ª'           : [r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡'         : [r'æ–¤é‡'],
    'é¦¬ä½“é‡'       : [r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ': [r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3Fã‚¿ã‚¤ãƒ |ä¸Š3F'],
    'ä¸Š3Fé †ä½'     : [r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’'      : [r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼é †ä½|4è§’é †ä½'],
    'æ€§åˆ¥'         : [r'æ€§åˆ¥'],
    'å¹´é½¢'         : [r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' : [r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢'         : [r'è·é›¢'],
    'é¦¬å ´'         : [r'é¦¬å ´|é¦¬å ´çŠ¶æ…‹'],
    'å¤©å€™'         : [r'å¤©å€™'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
MAP_S0 = _interactive_map(sheet0, PAT_S0, REQ_S0, "sheet0ï¼ˆéå»èµ°ï¼‰", "s0", show_ui=False)

df_score = pd.DataFrame()
for k, col in MAP_S0.items():
    if col is None: continue
    df_score[k] = sheet0[col]

df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
for c in ['é ­æ•°','ç¢ºå®šç€é †','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½','é€šé4è§’','è·é›¢']:
    if c in df_score: df_score[c] = pd.to_numeric(df_score[c], errors='coerce')
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' in df_score: df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].apply(_parse_time_to_sec)
if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in df_score: df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# é€šé4è§’/é ­æ•°ã®æ•´å½¢
if 'é ­æ•°' in df_score.columns:
    df_score['é ­æ•°'] = (
        df_score['é ­æ•°'].astype(str).str.extract(r'(\d+)')[0]
        .apply(pd.to_numeric, errors='coerce')
    )
if 'é€šé4è§’' in df_score.columns:
    s = df_score['é€šé4è§’']
    if s.dtype.kind not in 'iu':
        last_num = s.astype(str).str.extract(r'(\d+)(?!.*\d)')[0]
        df_score['é€šé4è§’'] = pd.to_numeric(last_num, errors='coerce')
    ok = df_score['é ­æ•°'].notna() & df_score['é€šé4è§’'].notna()
    bad = ok & ((df_score['é€šé4è§’'] < 1) | (df_score['é€šé4è§’'] > df_score['é ­æ•°']))
    df_score.loc[df_score['é€šé4è§’'].eq(0), 'é€šé4è§’'] = np.nan
    df_score.loc[bad, 'é€šé4è§’'] = np.nan

# === sheet1ï¼ˆå½“æ—¥å‡ºèµ°è¡¨ï¼‰ ===
PAT_S1 = {
    'é¦¬å'   : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ '     : [r'æ |æ ç•ª'],
    'ç•ª'     : [r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥'   : [r'æ€§åˆ¥'],
    'å¹´é½¢'   : [r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡'   : [r'æ–¤é‡'],
    'é¦¬ä½“é‡' : [r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª'   : [r'è„šè³ª'],
    'å‹ç‡'   : [r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡' : [r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡' : [r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ': [r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1 = ['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢']
MAP_S1 = _interactive_map(sheet1, PAT_S1, REQ_S1, "sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰", "s1", show_ui=False)

attrs = pd.DataFrame()
for k, col in MAP_S1.items():
    if col is None: continue
    attrs[k] = sheet1[col]
for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in attrs: attrs[c] = pd.to_numeric(attrs[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in attrs: attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] = attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# å…¥åŠ›UIï¼ˆè„šè³ªãƒ»æ–¤é‡ãƒ»é¦¬ä½“é‡ç·¨é›†ï¼‰
if 'è„šè³ª' not in attrs.columns: attrs['è„šè³ª'] = ''
if 'æ–¤é‡' not in attrs.columns: attrs['æ–¤é‡'] = np.nan
if 'é¦¬ä½“é‡' not in attrs.columns: attrs['é¦¬ä½“é‡'] = np.nan

st.subheader("é¦¬ä¸€è¦§ãƒ»è„šè³ªãƒ»æ–¤é‡ãƒ»å½“æ—¥é¦¬ä½“é‡å…¥åŠ›")
edited = st.data_editor(
    attrs[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
    column_config={
        'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
        'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
        'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1)
    },
    use_container_width=True,
    num_rows='static'
)
horses = edited.copy()
validate_inputs(df_score, horses)

# --- è„šè³ª è‡ªå‹•æ¨å®š ---
df_style = pd.DataFrame({'é¦¬å': [], 'p_é€ƒã’': [], 'p_å…ˆè¡Œ': [], 'p_å·®ã—': [], 'p_è¿½è¾¼': [], 'æ¨å®šè„šè³ª': []})
need_cols = {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
if auto_style_on and need_cols.issubset(df_score.columns):
    tmp = (
        df_score[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’','ä¸Š3Fé †ä½']].copy()
        .dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'])
        .sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    )
    tmp['_rn'] = tmp.groupby('é¦¬å').cumcount()+1
    tmp = tmp[tmp['_rn'] <= int(NRECENT)].copy()
    today = pd.Timestamp.today()
    tmp['_days'] = (today - pd.to_datetime(tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    tmp['_w'] = 0.5 ** (tmp['_days'] / float(HL_DAYS_STYLE))

    denom = (pd.to_numeric(tmp['é ­æ•°'], errors='coerce') - 1).replace(0, np.nan)
    pos_ratio = (pd.to_numeric(tmp['é€šé4è§’'], errors='coerce') - 1) / denom
    pos_ratio = pos_ratio.clip(0, 1).fillna(0.5)

    if 'ä¸Š3Fé †ä½' in tmp.columns:
        ag = pd.to_numeric(tmp['ä¸Š3Fé †ä½'], errors='coerce')
        close_strength = ((3.5 - ag) / 3.5).clip(lower=0, upper=1).fillna(0.0)
    else:
        close_strength = pd.Series(0.0, index=tmp.index)

    b_nige, b_sengo, b_sashi, b_oikomi = -1.2, 0.6, 0.3, -0.7
    tmp['L_nige']   = b_nige  + 1.6*(1 - pos_ratio) - 1.2*close_strength
    tmp['L_sengo']  = b_sengo + 1.1*(1 - pos_ratio) - 0.1*close_strength
    tmp['L_sashi']  = b_sashi + 1.1*(pos_ratio)     + 0.9*close_strength
    tmp['L_oikomi'] = b_oikomi+ 1.6*(pos_ratio)     + 0.5*close_strength

    rows = []
    for name, g in tmp.groupby('é¦¬å'):
        w  = g['_w'].to_numpy(); sw = w.sum()
        if sw <= 0: continue
        def wavg(v): return float((v*w).sum()/sw)
        vec = np.array([wavg(g['L_nige']), wavg(g['L_sengo']), wavg(g['L_sashi']), wavg(g['L_oikomi'])], dtype=float)
        vec = vec - vec.max()
        p = np.exp(vec); p = p / p.sum()
        pred = STYLES[int(np.argmax(p))]

        pr = (pd.to_numeric(g['é€šé4è§’'], errors='coerce') - 1) / (pd.to_numeric(g['é ­æ•°'], errors='coerce') - 1)
        pr = pr.clip(0,1).fillna(0.5)
        wpr = float((pr*w).sum()/sw)
        if pred == 'é€ƒã’' and not (wpr <= 0.22 or ((pr <= 0.15)*w).sum()/sw >= 0.25): pred='å…ˆè¡Œ'
        if pred == 'è¿½è¾¼' and not (wpr >= 0.78 or ((pr >= 0.85)*w).sum()/sw >= 0.25): pred='å·®ã—'

        rows.append([name, *p.tolist(), pred])
    if rows:
        df_style = pd.DataFrame(rows, columns=['é¦¬å','p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼','æ¨å®šè„šè³ª'])
        manual_has_nige = ('è„šè³ª' in horses.columns) and horses['è„šè³ª'].eq('é€ƒã’').any()
        if (df_style['æ¨å®šè„šè³ª'].eq('é€ƒã’').sum() == 0) and (not manual_has_nige):
            early = tmp.assign(early=(1 - pos_ratio).clip(0, 1), w=tmp['_w'].values)\
                      .groupby('é¦¬å').apply(lambda g: float((g['early']*g['w']).sum()/g['w'].sum()))
            nige_cand = early.idxmax()
            df_style.loc[df_style['é¦¬å'] == nige_cand, 'æ¨å®šè„šè³ª'] = 'é€ƒã’'

# --- æˆ¦ç¸¾ç‡ãƒ»ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ  ---
rate_cols = [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡'] if c in attrs.columns]
if rate_cols:
    rate = attrs[['é¦¬å'] + rate_cols].copy()
    for c in rate_cols:
        rate[c] = rate[c].astype(str).str.replace('%','', regex=False).str.replace('ï¼…','', regex=False)
        rate[c] = pd.to_numeric(rate[c], errors='coerce')
    mx = pd.concat([rate[c] for c in rate_cols], axis=1).max().max()
    if pd.notna(mx) and mx <= 1.0:
        for c in rate_cols: rate[c] *= 100.0
    if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in attrs:
        rate = rate.merge(attrs[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
else:
    rate = pd.DataFrame({'é¦¬å':[]})

# === éå»èµ°ã‹ã‚‰ã€Œé©æ­£é¦¬ä½“é‡ã€æ¨å®š ===
best_bw_map = {}
if {'é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †'}.issubset(df_score.columns):
    _bw = df_score[['é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †']].dropna()
    _bw['ç¢ºå®šç€é †'] = pd.to_numeric(_bw['ç¢ºå®šç€é †'], errors='coerce')
    _bw = _bw[_bw['ç¢ºå®šç€é †'].notna()]
    try:
        best_idx = _bw.groupby('é¦¬å')['ç¢ºå®šç€é †'].idxmin()
        best_bw_map = _bw.loc[best_idx].set_index('é¦¬å')['é¦¬ä½“é‡'].astype(float).to_dict()
    except Exception:
        best_bw_map = {}

# é‡è¤‡ã‚¬ãƒ¼ãƒ‰
try:
    horses.drop_duplicates('é¦¬å', keep='first', inplace=True)
except Exception:
    pass
try:
    df_score = df_score.drop_duplicates(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å'], keep='first')
except Exception:
    pass

# ===== ãƒãƒ¼ã‚¸ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    df_score.drop(columns=[dup], errors='ignore', inplace=True)
df_score = df_score.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')
if len(rate) > 0:
    use_cols = ['é¦¬å'] + [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] if c in rate.columns]
    df_score = df_score.merge(rate[use_cols], on='é¦¬å', how='left')

# ===== ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ =====
bt_min = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_max = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_span = (bt_max - bt_min) if pd.notna(bt_min) and pd.notna(bt_max) and (bt_max > bt_min) else 1.0
CLASS_BASE_BT = {"OP": 1.50, "L": 1.38, "G3": 1.19, "G2": 1.00, "G1": 0.80}
def besttime_weight_final(grade: str, surface: str, distance_m: int, user_scale: float) -> float:
    base = CLASS_BASE_BT.get(str(grade), CLASS_BASE_BT["OP"])
    s = 1.10 if str(surface) == "ãƒ€" else 1.00
    try:
        d = int(distance_m)
        if d <= 1400:           dfac = 1.20
        elif d == 1600:         dfac = 1.10
        elif 1800 <= d <= 2200: dfac = 1.00
        elif d >= 2400:         dfac = 0.85
        else:                   dfac = 1.00
    except: dfac = 1.00
    w = base * s * dfac * float(user_scale)
    return float(np.clip(w, 0.0, 2.0))

CLASS_PTS = {'G1':10, 'G2':8, 'G3':6, 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5, 'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}
def class_points(row) -> int:
    g = normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row:
        g = normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name = str(row.get('ã‚¯ãƒ©ã‚¹å','')) + ' ' + str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

def wfa_base_for(sex: str, age: int | None, dt: pd.Timestamp) -> float:
    try:
        a = int(age) if age is not None and not pd.isna(age) else None
    except Exception:
        a = None
    m = int(dt.month) if isinstance(dt, pd.Timestamp) else 1
    if a == 2:
        male = wfa_2_early_m if m <= 9 else wfa_2_late_m
        filly = wfa_2_early_f if m <= 9 else wfa_2_late_f
    elif a is not None and a >= 3:
        male, filly = wfa_3p_m, wfa_3p_f
    else:
        male, filly = wfa_3p_m, wfa_3p_f
    return male if sex in ("ç‰¡", "ã‚»") else filly

# ===== è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰èª­ã‚€ =====
pedi_bonus_pts: float = float(st.session_state.get('pedi:points', 0.0))
_pedi_map_session = st.session_state.get('pedi:map', {})
pedi_bonus_map = { _trim_name(k): bool(v) for k, v in dict(_pedi_map_session).items() }

def _pedi_bonus_for(name: str) -> float:
    nm = _trim_name(name)
    return float(pedi_bonus_pts) if pedi_bonus_map.get(nm, False) else 0.0

def calc_score(r):
    g = class_points(r)
    raw = g * (r['é ­æ•°'] + 1 - r['ç¢ºå®šç€é †']) + lambda_part * g

    sw  = season_w[season_of(pd.to_datetime(r['ãƒ¬ãƒ¼ã‚¹æ—¥']).month)]
    gw  = gender_w.get(r.get('æ€§åˆ¥'), 1)
    stw = style_w.get(r.get('è„šè³ª'), 1)
    fw  = frame_w.get(str(r.get('æ ')), 1)
    aw  = age_w.get(str(r.get('å¹´é½¢')), 1.0)

    gnorm = normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))
    grade_point = grade_bonus if gnorm in ['G1','G2','G3'] else 0

    agari_bonus = 0
    try:
        ao = int(r.get('ä¸Š3Fé †ä½', np.nan))
        if   ao == 1: agari_bonus = agari1_bonus
        elif ao == 2: agari_bonus = agari2_bonus
        elif ao == 3: agari_bonus = agari3_bonus
    except: pass

    body_bonus = 0
    try:
        name = r['é¦¬å']
        now_bw = float(r.get('é¦¬ä½“é‡', np.nan))
        tekitai = float(best_bw_map.get(name, np.nan))
        if not np.isnan(now_bw) and not np.isnan(tekitai) and abs(now_bw - tekitai) <= 10:
            body_bonus = bw_bonus
    except Exception: pass

    rate_bonus = 0.0
    try:
        if 'å‹ç‡' in r and pd.notna(r.get('å‹ç‡', np.nan)):   rate_bonus += win_w  * (float(r['å‹ç‡'])  / 100.0)
        if 'é€£å¯¾ç‡' in r and pd.notna(r.get('é€£å¯¾ç‡', np.nan)): rate_bonus += quin_w * (float(r['é€£å¯¾ç‡']) / 100.0)
        if 'è¤‡å‹ç‡' in r and pd.notna(r.get('è¤‡å‹ç‡', np.nan)): rate_bonus += plc_w  * (float(r['è¤‡å‹ç‡'])  / 100.0)
    except: pass

    bt_bonus = 0.0
    try:
        if pd.notna(r.get('ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’', np.nan)):
            bt_norm = (bt_max - float(r['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])) / bt_span
            bt_norm = max(0.0, min(1.0, bt_norm))
            bt_w_final = besttime_weight_final(
                grade=TARGET_GRADE, surface=TARGET_SURFACE, distance_m=int(TARGET_DISTANCE), user_scale=besttime_w
            )
            bt_bonus = bt_w_final * bt_norm
    except Exception:
        pass

    kg_pen = 0.0
    try:
        kg = float(r.get('æ–¤é‡', np.nan))
        if not np.isnan(kg):
            base = wfa_base_for(r.get('æ€§åˆ¥',''), int(r.get('å¹´é½¢')) if pd.notna(r.get('å¹´é½¢',np.nan)) else None, race_date) if use_wfa_base else 56.0
            delta = kg - float(base)
            kg_pen = (-max(0.0,  delta) * float(weight_coeff)
                      + 0.5 * max(0.0, -delta) * float(weight_coeff))
    except Exception:
        pass

    # âœ… è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹
    pedi_bonus = _pedi_bonus_for(r['é¦¬å'])

    total_bonus = grade_point + agari_bonus + body_bonus + rate_bonus + bt_bonus + kg_pen + pedi_bonus
    return raw * sw * gw * stw * fw * aw + total_bonus

# 1èµ°â†’æ­£è¦åŒ–
if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df_score.columns:
    st.error("ãƒ¬ãƒ¼ã‚¹æ—¥ åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Excelã®1æšç›®ã«å«ã‚ã¦ãã ã•ã„ã€‚")
    st.stop()
df_score['score_raw']  = df_score.apply(calc_score, axis=1)
if df_score['score_raw'].max() == df_score['score_raw'].min():
    df_score['score_norm'] = 50.0
else:
    df_score['score_norm'] = (
        (df_score['score_raw'] - df_score['score_raw'].min()) /
        (df_score['score_raw'].max() - df_score['score_raw'].min()) * 100
    )

# ===== æ™‚ç³»åˆ—åŠ é‡ =====
now = pd.Timestamp.today()
df_score['_days_ago'] = (now - df_score['ãƒ¬ãƒ¼ã‚¹æ—¥']).dt.days
df_score['_w'] = 0.5 ** (df_score['_days_ago'] / (half_life_m * 30.4375)) if half_life_m > 0 else 1.0

def w_mean(x, w):
    x = np.asarray(x, dtype=float); w = np.asarray(w, dtype=float)
    s = w.sum()
    return float((x*w).sum()/s) if s>0 else np.nan

agg = []
for name, g in df_score.groupby('é¦¬å'):
    avg  = g['score_norm'].mean()
    std  = g['score_norm'].std(ddof=0)
    wavg = w_mean(g['score_norm'], g['_w'])
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g) >= 2 else np.nan
    agg.append({'é¦¬å':name,'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg = pd.DataFrame(agg)

# â€”â€” WStd ã®ç©´åŸ‹ã‚
wstd_nontrivial = df_agg.loc[df_agg['Nrun']>=2, 'WStd']
default_std = float(wstd_nontrivial.median()) if (wstd_nontrivial.notna().any()) else 6.0
df_agg['WStd'] = df_agg['WStd'].fillna(default_std)
min_floor = max(1.0, default_std*0.6)
df_agg.loc[df_agg['WStd'] < min_floor, 'WStd'] = min_floor

# ===== è„šè³ªçµ±åˆï¼ˆæ‰‹å…¥åŠ›å„ªå…ˆâ†’è‡ªå‹•ï¼‰ =====
for df in [horses, df_agg]:
    if 'é¦¬å' in df.columns: df['é¦¬å'] = df['é¦¬å'].map(_trim_name)

name_list = df_agg['é¦¬å'].tolist()
combined_style = pd.Series(index=name_list, dtype=object)
if 'è„šè³ª' in horses.columns:
    combined_style.update(horses.set_index('é¦¬å')['è„šè³ª'])
if not df_style.empty and auto_style_on:
    pred_series = df_style.set_index('é¦¬å')['æ¨å®šè„šè³ª']
    if AUTO_OVERWRITE:
        combined_style.update(pred_series)
    else:
        mask_blank = combined_style.isna() | combined_style.astype(str).str.strip().eq('')
        combined_style.loc[mask_blank] = pred_series.reindex(combined_style.index)[mask_blank]
combined_style = combined_style.fillna('')
df_agg['è„šè³ª'] = df_agg['é¦¬å'].map(combined_style)

# ===== Pè¡Œåˆ—ï¼ˆè„šè³ªç¢ºç‡ï¼‰â†’ ãƒšãƒ¼ã‚¹MC =====
H = len(name_list)
P = np.zeros((H, 4), dtype=float)
pmap = None
if not df_style.empty and 'é¦¬å' in df_style.columns:
    df_prob = df_style.rename(columns={
        'p_å·®è¾¼': 'p_å·®ã—','p_è¿½ã„è¾¼ã¿': 'p_è¿½è¾¼','p_è¿½è¾¼ã¿': 'p_è¿½è¾¼',
        'p_é€ƒã’ç‡': 'p_é€ƒã’','p_å…ˆè¡Œç‡': 'p_å…ˆè¡Œ','p_å·®ã—ç‡': 'p_å·®ã—','p_è¿½è¾¼ç‡': 'p_è¿½è¾¼',
    })
    need_cols = ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']
    if set(need_cols).issubset(df_prob.columns):
        for c in need_cols:
            df_prob[c] = pd.to_numeric(df_prob[c], errors='coerce').fillna(0.0)
        pmap = (df_prob[['é¦¬å'] + need_cols].set_index('é¦¬å')[need_cols])
        row_sums = pmap.sum(axis=1).replace(0, np.nan)
        pmap = pmap.div(row_sums, axis=0).fillna(0.0)

for i, nm in enumerate(name_list):
    stl = combined_style.get(nm, '')
    if not AUTO_OVERWRITE and (stl in STYLES):
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0; continue
    if pmap is not None and nm in pmap.index:
        P[i, :] = pmap.loc[nm, ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']].to_numpy(dtype=float)
        if P[i, :].sum() == 0:
            if stl in STYLES: P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
            else: P[i, :] = np.array([0.25,0.25,0.25,0.25])
    elif stl in STYLES:
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
    else:
        P[i, :] = np.array([0.25,0.25,0.25,0.25])

mark_rule = {
    'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
    'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
    'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
}
mark_to_pts = {'â—':2, 'ã€‡':1, 'â—‹':1, 'â–³':0, 'Ã—':-1}

rng_pace = np.random.default_rng(int(mc_seed) + 12345)
sum_pts = np.zeros(H, dtype=float)
pace_counter = {'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
for _ in range(int(pace_mc_draws)):
    sampled = [rng_pace.choice(4, p=P[i]) for i in range(H)]
    nige  = sum(1 for s in sampled if s==0)
    sengo = sum(1 for s in sampled if s==1)
    epi = (epi_alpha*nige + epi_beta*sengo) / max(1, H)
    if   epi >= thr_hi:   pace_t = "ãƒã‚¤ãƒšãƒ¼ã‚¹"
    elif epi >= thr_mid:  pace_t = "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
    elif epi >= thr_slow: pace_t = "ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    else:                 pace_t = "ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    pace_counter[pace_t] += 1
    mk = mark_rule[pace_t]
    for i, s in enumerate(sampled):
        sum_pts[i] += mark_to_pts[ mk[STYLES[s]] ]
df_agg['PacePts'] = sum_pts / max(1, int(pace_mc_draws))
pace_type = max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
if pace_mode == "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰":
    pace_type = pace_fixed
    v_pts = np.array([mark_to_pts[ mark_rule[pace_type][st] ] for st in STYLES], dtype=float)
    df_agg['PacePts'] = (P @ v_pts)

# ===== æœ€çµ‚ã‚¹ã‚³ã‚¢ & å‹ç‡MC =====
df_agg['RecencyZ'] = z_score(df_agg['WAvgZ'])
df_agg['StabZ']    = z_score(-df_agg['WStd'])
df_agg['FinalRaw'] = df_agg['RecencyZ'] + stab_weight * df_agg['StabZ'] + pace_gain * df_agg['PacePts']
df_agg['FinalZ']   = z_score(df_agg['FinalRaw'])

# å‹ç‡MC
S = df_agg['FinalRaw'].to_numpy(dtype=float)
S = (S - np.nanmean(S)) / (np.nanstd(S) + 1e-9)
W = df_agg['WStd'].to_numpy(dtype=float)
W = (W - W.min()) / (W.max() - W.min() + 1e-9)
n = len(S)
rng = np.random.default_rng(int(mc_seed))
gumbel = rng.gumbel(loc=0.0, scale=1.0, size=(mc_iters, n))
noise  = (mc_tau * W)[None, :] * rng.standard_normal((mc_iters, n))
U = mc_beta * S[None, :] + noise + gumbel
rank_idx = np.argsort(-U, axis=1)
win_counts  = np.bincount(rank_idx[:, 0], minlength=n).astype(float)
top3_counts = np.zeros(n, dtype=float)
for k in range(3):
    top3_counts += np.bincount(rank_idx[:, k], minlength=n).astype(float)
p_win  = win_counts  / mc_iters
p_top3 = top3_counts / mc_iters
df_agg['å‹ç‡%_MC']   = (p_win  * 100).round(2)
df_agg['è¤‡å‹ç‡%_MC'] = (p_top3 * 100).round(2)

# ä¸Šä½æŠ½å‡ºã¨å°
CUTOFF = 50.0
cand = df_agg[df_agg['FinalZ'] >= CUTOFF].sort_values('FinalZ', ascending=False).copy()
topN = cand.head(6).copy()
marks = ['â—','ã€‡','â–²','â˜†','â–³','â–³']
if len(topN)>0: topN['å°'] = marks[:len(topN)]

# ===== æ•£å¸ƒå›³ï¼ˆç¸¦è»¸å …ç‰¢åŒ–ï¼‰ =====
st.markdown("### æ•£å¸ƒå›³ï¼ˆæœ€çµ‚åå·®å€¤ Ã— å®‰å®šåº¦ï¼‰")
if ALT_AVAILABLE and len(df_agg)>0:
    w_all = df_agg['WStd'].replace([np.inf,-np.inf], np.nan).dropna()
    if len(w_all) >= 2:
        q10, q25, q75, q90 = np.percentile(w_all, [10,25,75,90])
        iqr = max(0.1, q75 - q25)
        y_lo = max(0.0, q10 - 0.3*iqr)
        y_hi = q90 + 0.3*iqr
        if (y_hi - y_lo) < 4.0:
            mid = (y_hi + y_lo)/2
            y_lo, y_hi = max(0.0, mid-2.5), mid+2.5
    else:
        y_lo, y_hi = (0.0, 5.0)

    x_mid = 50.0
    y_mid = float(df_agg['WStd'].mean())
    rect = alt.Chart(pd.DataFrame([
        {'x1': df_agg['FinalZ'].min(), 'x2': x_mid, 'y1': y_mid, 'y2': y_hi},
        {'x1': x_mid, 'x2': df_agg['FinalZ'].max(), 'y1': y_mid, 'y2': y_hi},
        {'x1': df_agg['FinalZ'].min(), 'x2': x_mid, 'y1': y_lo, 'y2': y_mid},
        {'x1': x_mid, 'x2': df_agg['FinalZ'].max(), 'y1': y_lo, 'y2': y_mid},
    ])).mark_rect(opacity=0.07).encode(x='x1:Q', x2='x2:Q', y='y1:Q', y2='y2:Q')
    points = alt.Chart(df_agg).mark_circle(size=100).encode(
        x=alt.X('FinalZ:Q', title='æœ€çµ‚åå·®å€¤'),
        y=alt.Y('WStd:Q',  title='åŠ é‡æ¨™æº–åå·®ï¼ˆå°ã•ã„ã»ã©å®‰å®šï¼‰', scale=alt.Scale(domain=(float(y_lo), float(y_hi)))),
        tooltip=['é¦¬å','WAvgZ','WStd','RecencyZ','StabZ','PacePts','FinalZ','å‹ç‡%_MC']
    )
    labels = alt.Chart(df_agg).mark_text(dx=6, dy=-6, fontSize=10, color='#ffffff').encode(
        x='FinalZ:Q', y='WStd:Q', text='é¦¬å:N'
    )
    vline = alt.Chart(pd.DataFrame({'x':[x_mid]})).mark_rule(color='gray').encode(x='x:Q')
    hline = alt.Chart(pd.DataFrame({'y':[y_mid]})).mark_rule(color='gray').encode(y='y:Q')
    chart = (rect + points + labels + vline + hline).properties(width=700, height=420).interactive()
    st.altair_chart(chart, use_container_width=True)
else:
    st.table(df_agg[['é¦¬å','FinalZ','WStd']].sort_values('FinalZ', ascending=False).head(20))

# ===== horses2ï¼ˆçŸ­è©•ï¼‰ =====
å°map = dict(zip(topN.get('é¦¬å', pd.Series(dtype=str)), topN.get('å°', pd.Series(dtype=str))))
merge_cols = [c for c in ['é¦¬å','WAvgZ','WStd','FinalZ','è„šè³ª','PacePts'] if c in df_agg.columns]
horses2 = horses.merge(df_agg[merge_cols], on='é¦¬å', how='left') if merge_cols else horses.copy()
for col, default in [('å°',''), ('è„šè³ª',''), ('çŸ­è©•',''), ('WAvgZ', np.nan), ('WStd', np.nan), ('FinalZ', np.nan), ('PacePts', np.nan)]:
    if col not in horses2.columns: horses2[col] = default
horses2['å°'] = horses2['é¦¬å'].map(å°map).fillna('')

def ai_comment(row):
    base = ""
    if row['å°'] == 'â—':
        base += "æœ¬å‘½è©•ä¾¡ã€‚" + ("é«˜ã„å®‰å®šæ„Ÿã§ä¿¡é ¼åº¦æŠœç¾¤ã€‚" if pd.notna(row['WStd']) and row['WStd'] <= 8 else "èƒ½åŠ›ä¸Šä½ã‚‚ãƒ ãƒ©ã‚ã‚Šã€‚")
    elif row['å°'] == 'ã€‡':
        base += "å¯¾æŠ—è©•ä¾¡ã€‚" + ("è¿‘èµ°å®‰å®šã—ã¦ãŠã‚Šè»¸å€™è£œã€‚" if pd.notna(row['WStd']) and row['WStd'] <= 10 else "å±•é–‹ã²ã¨ã¤ã§é€†è»¢ã‚‚ã€‚")
    elif row['å°'] in ['â–²','â˜†']:
        base += "ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸€è§’ã€‚" + ("ãƒ ãƒ©ãŒã‚ã‚Šä¸€ç™ºã‚¿ã‚¤ãƒ—ã€‚" if pd.notna(row['WStd']) and row['WStd'] > 15 else "å®‰å®šå‹ã§å …å®Ÿã€‚")
    elif row['å°'] == 'â–³':
        base += "æŠ¼ã•ãˆå€™è£œã€‚" + ("å …å®Ÿã ãŒå‹ã¡åˆ‡ã‚‹ã¾ã§ã¯ï¼Ÿ" if pd.notna(row['WStd']) and row['WStd'] < 12 else "å±•é–‹æ¬¡ç¬¬ã§æµ®ä¸Šã‚‚ã€‚")
    style = str(row.get('è„šè³ª','')).strip()
    base += {
        "é€ƒã’":"ãƒãƒŠã‚’å¥ªãˆã‚Œã°ç²˜ã‚Šè¾¼ã¿ååˆ†ã€‚",
        "å…ˆè¡Œ":"å…ˆè¡ŒåŠ›ã‚’æ´»ã‹ã—ã¦ä¸Šä½äº‰ã„ã€‚",
        "å·®ã—":"å±•é–‹ãŒå‘ã‘ã°æœ«è„šå¼·çƒˆã€‚",
        "è¿½è¾¼":"ç›´ç·šå‹è² ã®ä¸€æ’ƒã«æœŸå¾…ã€‚"
    }.get(style, "")
    return base
try:
    horses2['çŸ­è©•'] = horses2.apply(ai_comment, axis=1)
except Exception:
    if 'çŸ­è©•' not in horses2: horses2['çŸ­è©•'] = ""

# ======================== çµæœã‚¿ãƒ– ========================
tab_dash, tab_prob, tab_pace, tab_bets, tab_all, tab_pedi = st.tabs(
    ["ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ“ˆ å‹ç‡", "ğŸ§­ å±•é–‹", "ğŸ« è²·ã„ç›®", "ğŸ“ å…¨é ­ã‚³ãƒ¡ãƒ³ãƒˆ", "ğŸ§¬ è¡€çµ±HTML"]
)

with tab_dash:
    st.subheader("ã‚µãƒãƒªãƒ¼")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æƒ³å®šãƒšãƒ¼ã‚¹", locals().get("pace_type","â€”"))
    c2.metric("å‡ºèµ°é ­æ•°", len(horses))
    if len(topN) > 0:
        c3.metric("â— FinalZ", f"{topN.iloc[0]['FinalZ']:.1f}")
        try:
            win_pct = float(df_agg.loc[df_agg['é¦¬å']==topN.iloc[0]['é¦¬å'],'å‹ç‡%_MC'].iloc[0])
            c4.metric("â— æ¨å®šå‹ç‡", f"{win_pct:.1f}%")
        except Exception:
            c4.metric("â— æ¨å®šå‹ç‡", "â€”")
    st.markdown("#### ä¸Šä½é¦¬ï¼ˆFinalZâ‰§50ãƒ»æœ€å¤§6é ­ï¼‰")
    _top = topN.merge(df_agg[['é¦¬å','å‹ç‡%_MC']], on='é¦¬å', how='left') if 'å‹ç‡%_MC' not in topN else topN
    show_cols = [c for c in ['é¦¬å','å°','FinalZ','WAvgZ','WStd','PacePts','å‹ç‡%_MC'] if c in _top.columns]
    st.dataframe(_top[show_cols], use_container_width=True, height=220)

with tab_prob:
    st.subheader("æ¨å®šå‹ç‡ãƒ»è¤‡å‹ç‡ï¼ˆãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ï¼‰")
    prob_view = (
        df_agg[['é¦¬å','FinalZ','WAvgZ','WStd','PacePts','å‹ç‡%_MC','è¤‡å‹ç‡%_MC']]
        .sort_values('å‹ç‡%_MC', ascending=False).reset_index(drop=True)
    )
    _pv = prob_view.copy()
    for c in ['å‹ç‡%_MC','è¤‡å‹ç‡%_MC']:
        if c in _pv: _pv[c] = _pv[c].map(lambda x: f"{x:.2f}%")
    st.dataframe(_pv, use_container_width=True, height=380)

with tab_pace:
    st.subheader("å±•é–‹ãƒ»è„šè³ªã‚µãƒãƒªãƒ¼")
    st.caption(f"æƒ³å®šãƒšãƒ¼ã‚¹: {locals().get('pace_type','â€”')}ï¼ˆ{'å›ºå®š' if pace_mode=='å›ºå®šï¼ˆæ‰‹å‹•ï¼‰' else 'è‡ªå‹•MC'}ï¼‰")

    # --- å …ç‰¢ç‰ˆï¼šæ‰‹å…¥åŠ›ã¨è‡ªå‹•æ¨å®šã‚’ãƒãƒ¼ã‚¸ï¼ˆæ‰‹å…¥åŠ›å„ªå…ˆï¼‰ ---
    df_map = horses.copy()
    if 'è„šè³ª' not in df_map.columns:
        df_map['è„šè³ª'] = ''

    auto_st = df_map['é¦¬å'].map(combined_style)

    cond_filled = df_map['è„šè³ª'].astype(str).str.strip().ne('')
    df_map.loc[~cond_filled, 'è„šè³ª'] = auto_st.loc[~cond_filled]

    df_map['è„šè³ª'] = df_map['è„šè³ª'].fillna('')
    df_map['è„šè³ª'] = df_map['è„šè³ª'].where(df_map['è„šè³ª'].isin(STYLES), other='')

    style_counts = df_map['è„šè³ª'].value_counts().reindex(STYLES).fillna(0).astype(int)
    total_heads = int(style_counts.sum()) if style_counts.sum() > 0 else 1
    style_pct = (style_counts / total_heads * 100).round(1)

    pace_summary = pd.DataFrame([{
        'æƒ³å®šãƒšãƒ¼ã‚¹': locals().get('pace_type','â€”'),
        'é€ƒã’':  f"{style_counts['é€ƒã’']}é ­ï¼ˆ{style_pct['é€ƒã’']}%ï¼‰",
        'å…ˆè¡Œ':  f"{style_counts['å…ˆè¡Œ']}é ­ï¼ˆ{style_pct['å…ˆè¡Œ']}%ï¼‰",
        'å·®ã—':  f"{style_counts['å·®ã—']}é ­ï¼ˆ{style_pct['å·®ã—']}%ï¼‰",
        'è¿½è¾¼':  f"{style_counts['è¿½è¾¼']}é ­ï¼ˆ{style_pct['è¿½è¾¼']}%ï¼‰",
    }])
    st.table(pace_summary)

    def _normalize_ban(x):
        return pd.to_numeric(str(x).translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789')), errors='coerce')

    if 'ç•ª' in df_map.columns:
        df_map['_ban'] = _normalize_ban(df_map['ç•ª'])
        loc_df = df_map.dropna(subset=['_ban']).copy()
        loc_df = loc_df[loc_df['è„šè³ª'].isin(STYLES)]
        if not loc_df.empty:
            loc_df['_ban'] = loc_df['_ban'].astype(int)
            loc_df = loc_df.sort_values('_ban')

            fig, ax = plt.subplots(figsize=(10, 3))
            colors = {'é€ƒã’':'red', 'å…ˆè¡Œ':'orange', 'å·®ã—':'green', 'è¿½è¾¼':'blue'}
            for _, row in loc_df.iterrows():
                x = int(row['_ban']); y = STYLES.index(row['è„šè³ª'])
                ax.scatter(x, y, color=colors[row['è„šè³ª']], s=200)
                ax.text(
                    x, y, str(row['é¦¬å']),
                    ha='center', va='center', color='white', fontsize=9, weight='bold',
                    bbox=dict(facecolor=colors[row['è„šè³ª']], alpha=0.7, boxstyle='round'),
                    fontproperties=jp_font
                )
            ax.set_yticks([0,1,2,3]); ax.set_yticklabels(STYLES, fontproperties=jp_font)
            xs = sorted(loc_df['_ban'].unique())
            ax.set_xticks(xs); ax.set_xticklabels([f"{i}ç•ª" for i in xs], fontproperties=jp_font)
            ax.set_xlabel("é¦¬ç•ª", fontproperties=jp_font); ax.set_ylabel("è„šè³ª", fontproperties=jp_font)
            ax.set_title(f"å±•é–‹ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ{locals().get('pace_type','â€”')}æƒ³å®šï¼‰", fontproperties=jp_font)
            st.pyplot(fig)
        else:
            st.info("é¦¬ç•ªã¾ãŸã¯è„šè³ªãŒæœªå…¥åŠ›ã®ãŸã‚ã€é…ç½®å›³ã¯çœç•¥ã—ã¾ã—ãŸã€‚ä¸Šã®è¡¨ã¯æœ‰åŠ¹ã§ã™ã€‚")
    else:
        st.info("å‡ºèµ°è¡¨ã«ã€ç•ªã€åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€é…ç½®å›³ã¯çœç•¥ã—ã¾ã—ãŸã€‚åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

with tab_bets:
    h1 = topN.iloc[0]['é¦¬å'] if len(topN) >= 1 else None
    h2 = topN.iloc[1]['é¦¬å'] if len(topN) >= 2 else None
    symbols = topN.get('å°', pd.Series([], dtype=str)).tolist()
    names   = topN['é¦¬å'].tolist()
    others_names   = names[1:] if len(names) > 1 else []
    others_symbols = symbols[1:] if len(symbols) > 1 else []

    three = ['é¦¬é€£','ãƒ¯ã‚¤ãƒ‰','é¦¬å˜']
    def round_to_unit(x, unit): return int(np.floor(x / unit) * unit)

    main_share = 0.5
    pur1 = round_to_unit(total_budget * main_share * (1/4), int(min_unit))  # å˜å‹
    pur2 = round_to_unit(total_budget * main_share * (3/4), int(min_unit))  # è¤‡å‹
    rem  = total_budget - (pur1 + pur2)

    win_each   = round_to_unit(pur1 / 2, int(min_unit))
    place_each = round_to_unit(pur2 / 2, int(min_unit))

    st.subheader("â–  è³‡é‡‘é…åˆ† (å³å¯†åˆè¨ˆ)")
    st.write(f"åˆè¨ˆäºˆç®—ï¼š{total_budget:,}å††  å˜å‹ï¼š{pur1:,}å††  è¤‡å‹ï¼š{pur2:,}å††  æ®‹ï¼š{rem:,}å††  [å˜ä½:{min_unit}å††]")

    bets = []
    if h1 is not None:
        bets += [{'åˆ¸ç¨®':'å˜å‹','å°':'â—','é¦¬':h1,'ç›¸æ‰‹':'','é‡‘é¡':win_each},
                 {'åˆ¸ç¨®':'è¤‡å‹','å°':'â—','é¦¬':h1,'ç›¸æ‰‹':'','é‡‘é¡':place_each}]
    if h2 is not None:
        bets += [{'åˆ¸ç¨®':'å˜å‹','å°':'ã€‡','é¦¬':h2,'ç›¸æ‰‹':'','é‡‘é¡':win_each},
                 {'åˆ¸ç¨®':'è¤‡å‹','å°':'ã€‡','é¦¬':h2,'ç›¸æ‰‹':'','é‡‘é¡':place_each}]

    finalZ_map = df_agg.set_index('é¦¬å')['FinalZ'].to_dict()
    pair_candidates, tri_candidates, tri1_candidates = [], [], []
    if h1 is not None and len(others_names) > 0:
        for nm, mk in zip(others_names, others_symbols):
            score = finalZ_map.get(nm, 0)
            pair_candidates.append(('ãƒ¯ã‚¤ãƒ‰', f'â—â€“{mk}', h1, nm, score))
            pair_candidates.append(('é¦¬é€£', f'â—â€“{mk}', h1, nm, score))
            pair_candidates.append(('é¦¬å˜', f'â—â†’{mk}', h1, nm, score))
        for a, b in combinations(others_names, 2):
            score = finalZ_map.get(a,0) + finalZ_map.get(b,0)
            tri_candidates.append(('ä¸‰é€£è¤‡','â—-ã€‡â–²â˜†â–³â–³', h1, f"{a}ï¼{b}", score))
        second_opts = others_names[:2]
        for s in second_opts:
            for t in others_names:
                if t == s: continue
                score = finalZ_map.get(s,0) + 0.7*finalZ_map.get(t,0)
                tri1_candidates.append(('ä¸‰é€£å˜ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³','â—-ã€‡â–²-ã€‡â–²â˜†â–³â–³', h1, f"{s}ï¼{t}", score))

    if scenario == 'é€šå¸¸':
        with st.expander("é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ãƒ»é¦¬å˜ ã‹ã‚‰ï¼‘åˆ¸ç¨®ã‚’é¸æŠ", expanded=True):
            choice = st.radio("è³¼å…¥åˆ¸ç¨®", options=three, index=1)
            st.write(f"â–¶ {choice} ã«æ®‹ã‚Š {rem:,}å†† ã‚’å……å½“")
        cand = [c for c in pair_candidates if c[0]==choice]
        cand = sorted(cand, key=lambda x: x[-1], reverse=True)[:int(max_lines)]
        K = len(cand)
        if K > 0 and rem >= int(min_unit):
            base = round_to_unit(rem / K, int(min_unit))
            amounts = [base]*K
            leftover = rem - base*K
            i = 0
            while leftover >= int(min_unit) and i < K:
                amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
            for (typ, mks, base_h, pair_h, _), amt in zip(cand, amounts):
                bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
        else:
            st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

    elif scenario == 'ã¡ã‚‡ã„ä½™è£•':
        st.write("â–¶ æ®‹ã‚Šäºˆç®—ã‚’ ãƒ¯ã‚¤ãƒ‰ ï¼‹ ä¸‰é€£è¤‡ ã§é…åˆ†")
        cand_wide = sorted([c for c in pair_candidates if c[0]=='ãƒ¯ã‚¤ãƒ‰'], key=lambda x: x[-1], reverse=True)
        cand_tri  = sorted(tri_candidates, key=lambda x: x[-1], reverse=True)
        cut_w = min(len(cand_wide), int(max_lines)//2 if int(max_lines)>1 else 1)
        cut_t = min(len(cand_tri),  int(max_lines) - cut_w)
        cand_wide, cand_tri = cand_wide[:cut_w], cand_tri[:cut_t]
        K = len(cand_wide) + len(cand_tri)
        if K>0 and rem >= int(min_unit):
            base = round_to_unit(rem / K, int(min_unit))
            amounts = [base]*K
            leftover = rem - base*K
            i = 0
            while leftover >= int(min_unit) and i < K:
                amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
            all_cand = cand_wide + cand_tri
            for (typ, mks, base_h, pair_h, _), amt in zip(all_cand, amounts):
                bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
        else:
            st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

    elif scenario == 'ä½™è£•':
        st.write("â–¶ æ®‹ã‚Šäºˆç®—ã‚’ ãƒ¯ã‚¤ãƒ‰ ï¼‹ ä¸‰é€£è¤‡ ï¼‹ ä¸‰é€£å˜ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ ã§é…åˆ†")
        cand_wide = sorted([c for c in pair_candidates if c[0]=='ãƒ¯ã‚¤ãƒ‰'], key=lambda x: x[-1], reverse=True)
        cand_tri  = sorted(tri_candidates, key=lambda x: x[-1], reverse=True)
        cand_tri1 = sorted(tri1_candidates, key=lambda x: x[-1], reverse=True)
        r_w, r_t, r_t1 = 2, 2, 1
        denom = r_w + r_t + r_t1
        q_w = max(1, (int(max_lines) * r_w)//denom)
        q_t = max(1, (int(max_lines) * r_t)//denom)
        q_t1= max(1, int(max_lines) - q_w - q_t)
        cand_wide, cand_tri, cand_tri1 = cand_wide[:q_w], cand_tri[:q_t], cand_tri1[:q_t1]
        K = len(cand_wide) + len(cand_tri) + len(cand_tri1)
        if K>0 and rem >= int(min_unit):
            base = round_to_unit(rem / K, int(min_unit))
            amounts = [base]*K
            leftover = rem - base*K
            i = 0
            while leftover >= int(min_unit) and i < K:
                amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
            all_cand = cand_wide + cand_tri + cand_tri1
            for (typ, mks, base_h, pair_h, _), amt in zip(all_cand, amounts):
                bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
        else:
            st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

    _df = pd.DataFrame(bets)
    spent = int(_df['é‡‘é¡'].fillna(0).replace('',0).sum()) if len(_df)>0 else 0
    diff = total_budget - spent
    if diff != 0 and len(_df) > 0:
        for idx in _df.index:
            cur = int(_df.at[idx,'é‡‘é¡'])
            new = cur + diff
            if new >= 0 and new % int(min_unit) == 0:
                _df.at[idx,'é‡‘é¡'] = new
                break

    _df_disp = _df.copy()
    if 'é‡‘é¡' in _df_disp.columns and len(_df_disp) > 0:
        def fmt_money(x):
            try:
                xv = float(x)
                if np.isnan(xv) or int(xv) <= 0: return ""
                return f"{int(xv):,}å††"
            except Exception:
                return ""
        _df_disp['é‡‘é¡'] = _df_disp['é‡‘é¡'].map(fmt_money)
    st.subheader("â–  æœ€çµ‚è²·ã„ç›®ä¸€è¦§ï¼ˆå…¨åˆ¸ç¨®ã¾ã¨ã‚ï¼‰")
    if len(_df_disp) == 0:
        st.info("ç¾åœ¨ã€è²·ã„ç›®ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.table(safe_take(_df_disp, ['åˆ¸ç¨®','å°','é¦¬','ç›¸æ‰‹','é‡‘é¡']))

with tab_all:
    st.subheader("å…¨é ­AIè¨ºæ–­ã‚³ãƒ¡ãƒ³ãƒˆ")
    q = st.text_input("é¦¬åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")
    show_cols = [c for c in ['é¦¬å','å°','è„šè³ª','çŸ­è©•','WAvgZ','WStd'] if c in horses2.columns]
    _all = horses2[show_cols].copy()
    if q.strip():
        _all = _all[_all['é¦¬å'].astype(str).str.contains(q.strip(), case=False, na=False)]
    if _all.empty:
        st.info("ã‚³ãƒ¡ãƒ³ãƒˆè¡¨ç¤ºå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¸Šéƒ¨ã®å…¥åŠ›ã¨è¨ˆç®—çµæœã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    else:
        st.dataframe(_all, use_container_width=True, height=420)

# ======================== è¡€çµ±HTMLï¼ˆãƒ“ãƒ¥ãƒ¼ï¼‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´â†’ãƒœãƒ¼ãƒŠã‚¹ï¼‰ ========================
with tab_pedi:
    st.subheader("è¡€çµ±HTMLãƒ“ãƒ¥ãƒ¼ã‚¢ + ãƒœãƒ¼ãƒŠã‚¹ä»˜ä¸")
    st.caption("NetKeibaç­‰ã®è¡€çµ±ãƒšãƒ¼ã‚¸HTMLã‚’è¡¨ç¤ºã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã—ãŸé¦¬ã¸ãƒœãƒ¼ãƒŠã‚¹ã‚’ä»˜ä¸ã—ã¾ã™ã€‚")

    m = st.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True)

    # --- æ–‡å­—ã‚³ãƒ¼ãƒ‰æ¤œå‡º & ãƒ‡ã‚³ãƒ¼ãƒ‰ ------------------------------
    def _detect_charset_from_head(raw: bytes) -> str | None:
        if raw.startswith(b"\xef\xbb\xbf"): return "utf-8-sig"
        if raw.startswith(b"\xff\xfe"):     return "utf-16-le"
        if raw.startswith(b"\xfe\xff"):     return "utf-16-be"
        head_txt = raw[:4096].decode("ascii", "ignore")
        m1 = re.search(r"charset\s*=\s*['\"]?([\w\-]+)", head_txt, flags=re.I)
        return m1.group(1).lower() if m1 else None

    def _decode_html_bytes(raw: bytes, preferred: str | None = None) -> str:
        declared = _detect_charset_from_head(raw)
        cands = [c for c in [preferred, declared,
                             "cp932", "shift_jis",
                             "utf-8", "utf-8-sig",
                             "euc_jp", "iso2022_jp",
                             "utf-16", "utf-16-le", "utf-16-be"] if c]
        seen = set()
        for enc in [c for c in cands if not (c in seen or seen.add(c))]:
            try:
                txt = raw.decode(enc)
                if enc.startswith("utf-8") and txt.count("ï¿½") > 10:
                    continue
                return txt
            except Exception:
                continue
        return raw.decode("utf-8", errors="replace")

    html_text = ""
    if m == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
        html_txt = st.text_area("HTMLã‚’è²¼ã‚Šä»˜ã‘", height=220, placeholder="<html>...</html>")
        html_text = html_txt
    else:
        up = st.file_uploader("è¡€çµ±HTMLãƒ•ã‚¡ã‚¤ãƒ«", type=["html", "htm"], key="pedi_html_up")
        if up:
            raw  = up.read()
            html_text = _decode_html_bytes(raw)

    # è¡¨ç¤ºï¼ˆcomponents ãŒä½¿ãˆã‚‹å ´åˆï¼‰
    if html_text.strip() and COMPONENTS:
        components.html(html_text, height=700, scrolling=True)
    elif html_text.strip() and not COMPONENTS:
        st.code(html_text[:8000], language="html")

    # ========= åˆ—åæ­£è¦åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
    def _col_norm(s: str) -> str:
        s = str(s).replace('\u3000', ' ').replace('\xa0', ' ').strip()
        s = s.translate(_fwid)                 # å…¨è§’â†’åŠè§’ï¼ˆæ•°å­—/ï¼…ï¼‰
        return re.sub(r'\s+', '', s)           # ã™ã¹ã¦ã®ç©ºç™½ã‚’é™¤å»

    # ========= è¡¨æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼šMultiIndexâ†’å¹³å¦åŒ–ï¼ãƒ˜ãƒƒãƒ€æ˜‡æ ¼ï¼html5libï¼‰ =========
    def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
        if isinstance(df.columns, pd.MultiIndex):
            cols = []
            for i, tup in enumerate(df.columns):
                parts = [str(c) for c in tup if "Unnamed" not in str(c)]
                lab = _col_norm("".join(parts)) or f"col{i}"
                cols.append(lab)
            df.columns = cols
        else:
            df.columns = [_col_norm(c) for c in df.columns]
        return df

    def _promote_header_row_if_needed(df: pd.DataFrame) -> pd.DataFrame:
        # æ—¢ã«ã€Œé¦¬åã€ã‚’å«ã‚€ãªã‚‰OKï¼ˆæ­£è¦åŒ–å¾Œã§åˆ¤å®šï¼‰
        if any("é¦¬å" in _col_norm(c) or re.fullmatch(r"é¦¬", _col_norm(c)) for c in df.columns):
            return df
        # å…ˆé ­ã€œ8è¡Œã‚’ãƒ˜ãƒƒãƒ€å€™è£œã¨ã—ã¦ã‚¹ã‚­ãƒ£ãƒ³
        up_to = min(8, len(df))
        for i in range(up_to):
            row = df.iloc[i].astype(str).map(_col_norm)
            if row.str.contains("é¦¬å").any() or row.str.fullmatch("é¦¬").any():
                df = df.iloc[i+1:].copy()
                df.columns = row.tolist()
                return df
        return df

    def _read_pedigree_tables(html_text: str):
        tables = []
        for flv in ['lxml', 'bs4', 'html5lib']:  # â† ã“ã“ã§æœ€å¤§é™ç²˜ã‚‹
            try:
                tables = pd.read_html(html_text, flavor=flv)
                if tables:
                    break
            except Exception:
                continue
        fixed = []
        for t in tables:
            t = _flatten_columns(t)
            t = _promote_header_row_if_needed(t)
            # é¦¬ååˆ—ã‚’å«ã‚€ã‚‚ã®ã ã‘æ¡ç”¨
            if any(("é¦¬å" in _col_norm(c)) or re.fullmatch(r"é¦¬", _col_norm(c)) for c in t.columns):
                fixed.append(t.reset_index(drop=True))
        return fixed

    # ========= ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ é¦¬ã¸ã®ãƒœãƒ¼ãƒŠã‚¹ä»˜ä¸è¨­å®š =========
    st.markdown("### ğŸ§¬ è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹è¨­å®šï¼ˆä¸‹ã§ä¸€è‡´ã—ãŸé¦¬ã«åŠ ç‚¹ï¼‰")

    default_pts = int(st.session_state.get('pedi:points', 3))
    points = st.slider("ä¸€è‡´ã—ãŸé¦¬ã¸ã®ãƒœãƒ¼ãƒŠã‚¹ç‚¹", 0, 20, default_pts)

    dfs = _read_pedigree_tables(html_text) if html_text.strip() else []
    st.caption(f"æ¤œå‡ºãƒ†ãƒ¼ãƒ–ãƒ«: {len(dfs)}ä»¶" + (f" / ä¾‹: {', '.join(dfs[0].columns[:8])}" if dfs else ""))

    if dfs:
        idx = st.selectbox(
            "å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ",
            options=list(range(len(dfs))),
            format_func=lambda i: f"Table {i+1}ï¼ˆåˆ—: {', '.join(dfs[i].columns[:6])} â€¦ï¼‰"
        )
        dfp = dfs[idx]

        # é¦¬ååˆ—ã¨å€™è£œåˆ—
        name_candidates = [c for c in dfp.columns if ("é¦¬å" in _col_norm(c)) or re.fullmatch(r"é¦¬", _col_norm(c))]
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ï¼šå€™è£œãŒç„¡ã„å ´åˆã§ã‚‚é¸ã¹ã‚‹ã‚ˆã†ã«0ç•ªç›®ã«é€€é¿
        default_name_idx = dfp.columns.tolist().index(name_candidates[0]) if name_candidates else 0
        name_col = st.selectbox("é¦¬ååˆ—", options=dfp.columns.tolist(), index=default_name_idx)

        known = ["çˆ¶ã‚¿ã‚¤ãƒ—å","çˆ¶å","æ¯çˆ¶ã‚¿ã‚¤ãƒ—å","æ¯çˆ¶å","çˆ¶ç³»","æ¯çˆ¶ç³»","çˆ¶","æ¯çˆ¶"]
        candidate_cols = [c for c in known if c in dfp.columns] or [c for c in dfp.columns if c != name_col]

        st.caption("â€» ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã€é¸æŠã—ãŸåˆ—ã®ã©ã‚Œã‹ã«ä¸€è‡´ã—ãŸã€é¦¬åã€ã¸åŠ ç‚¹ã—ã¾ã™ã€‚è¤‡æ•°è¡ŒOKã€‚")
        keys_text = st.text_area("è¡€çµ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1è¡Œ1ãƒ¯ãƒ¼ãƒ‰ï¼‰",
                                 value=st.session_state.get('pedi:keys', ""), height=120)
        match_cols = st.multiselect("ç…§åˆå¯¾è±¡ã®åˆ—", candidate_cols,
                                    default=[c for c in known if c in candidate_cols] or candidate_cols)
        method = st.radio("ç…§åˆæ–¹æ³•", ["éƒ¨åˆ†ä¸€è‡´", "å®Œå…¨ä¸€è‡´"], index=0, horizontal=True)

        def _norm(s: str) -> str:
            s = str(s)
            s = s.translate(_fwid).replace('\u3000', ' ').replace('\xa0', ' ').strip()
            return re.sub(r'\s+', '', s)

        keys = [k for k in (keys_text.splitlines() if keys_text else []) if k.strip()]
        keys_norm = [_norm(k) for k in keys]

        matched_names = []
        if keys and match_cols:
            for _, row in dfp.iterrows():
                try:
                    nm = _trim_name(row[name_col])
                except Exception:
                    continue
                row_texts_norm = [_norm(row.get(c, "")) for c in match_cols]
                hit = False
                for k in keys_norm:
                    if method == "å®Œå…¨ä¸€è‡´":
                        if any(r == k for r in row_texts_norm):
                            hit = True; break
                    else:
                        if any(k in r for r in row_texts_norm):
                            hit = True; break
                if hit:
                    matched_names.append(nm)

        matched_map = { _trim_name(n): True for n in matched_names }
        st.session_state['pedi:map'] = matched_map
        st.session_state['pedi:points'] = int(points)
        st.session_state['pedi:keys'] = keys_text

        colL, colR = st.columns([2,3])
        with colL:
            st.write("ä¸€è‡´ã—ãŸé¦¬ï¼ˆåŠ ç‚¹å¯¾è±¡ï¼‰")
            if matched_names:
                st.table(pd.DataFrame({'é¦¬å': matched_names}))
            else:
                st.info("ç¾åœ¨ã€ä¸€è‡´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰/ç…§åˆåˆ—/ç…§åˆæ–¹æ³•ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        with colR:
            st.info(f"è¨­å®šï¼šãƒœãƒ¼ãƒŠã‚¹ {points} ç‚¹ / ç…§åˆåˆ— {', '.join(match_cols) if match_cols else 'ï¼ˆæœªé¸æŠï¼‰'} / {method}")

    else:
        st.info("é¦¬ååˆ—ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€HTMLãŒæœªå…¥åŠ›ã§ã™ã€‚å…ˆã«HTMLã‚’è²¼ã‚Šä»˜ã‘ã‚‹ã‹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

