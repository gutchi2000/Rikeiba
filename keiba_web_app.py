# -*- coding: utf-8 -*-
# ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆAUTOçµ±åˆç‰ˆ / 2025-09-22ï¼‰
# - â€œå¢—ã‚„ã™â€ã‚ˆã‚Šâ€œç•³ã‚€/è‡ªå‹•åŒ–â€ã€‚ä¸€æ‹¬åæ˜ ãƒ‘ãƒƒãƒã€‚
# - ä¸»è¦å¤‰æ›´ç‚¹:
#   A) ãƒ¬ãƒ¼ã‚¹å†…ãƒ‡ãƒ•ãƒ¬ãƒ¼ãƒˆ(score_adj)ã‚’æ¨™æº–åŒ–
#   B) å‹ç‡PLæ¸©åº¦Î²ã‚’è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ‰‹å‹•å¯ï¼‰
#   C) å³/å·¦å›ã‚ŠåŠ ç‚¹ã¯é€£ç¶šå€¤åŒ– (gapÃ—æœ‰åŠ¹æœ¬æ•°ã‚²ãƒ¼ãƒˆ)
#   D) ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ã¯å±¥æ­´ã‹ã‚‰è‡ªå·±å­¦ç¿’(ç›¸é–¢)
#   E) è·é›¢Ã—å›ã‚Šã®å¸¯åŸŸ(h)ã¯è‡ªå‹•ï¼ˆSilvermané¢¨ï¼‰
#   F) ç‡ãƒœãƒ¼ãƒŠã‚¹ã¯ãƒ™ãƒ¼ã‚¿ç¸®ç´„ã®ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼‰
#   G) è¤‡å‹(Top3)ã¯åå¯¾ç§°Gumbelã§åˆ†æ•£ä½æ¸›
#   H) AR100ã¯åˆ†ä½å†™åƒï¼ˆå˜èª¿å¤‰æ›ï¼é †ä½ä¸å¤‰ï¼‰
#   I) Isotonicæ ¡æ­£ã®ã¿ï¼ˆPlatté™¤å¤–ï¼‰
#   J) è¡¨ç¤ºã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆåŒ– / è¨ºæ–­ã¯åˆ¥ã‚¿ãƒ–

import os, io, re, json
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from itertools import combinations

# ---- optional ----
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

try:
    from sklearn.isotonic import IsotonicRegression
    SK_ISO = True
except Exception:
    SK_ISO = False

# ===== æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ =====
from matplotlib import font_manager
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'IPAexGothic','IPAGothic','Noto Sans CJK JP','Yu Gothic UI','Meiryo','Hiragino Sans','MS Gothic'
]

st.set_page_config(page_title="ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆAUTOç‰ˆï¼‰", layout="wide")

# ===== å°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fw = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

STYLE_ALIASES = {
    'è¿½ã„è¾¼ã¿':'è¿½è¾¼','è¿½è¾¼ã¿':'è¿½è¾¼','ãŠã„ã“ã¿':'è¿½è¾¼','ãŠã„è¾¼ã¿':'è¿½è¾¼',
    'ã•ã—':'å·®ã—','å·®è¾¼':'å·®ã—','å·®è¾¼ã¿':'å·®ã—',
    'ã›ã‚“ã“ã†':'å…ˆè¡Œ','å…ˆè¡Œ ':'å…ˆè¡Œ','å…ˆè¡Œã€€':'å…ˆè¡Œ',
    'ã«ã’':'é€ƒã’','é€ƒã’ ':'é€ƒã’','é€ƒã’ã€€':'é€ƒã’'
}

def normalize_style(s: str) -> str:
    s = str(s).replace('ã€€','').strip().translate(_fw)
    s = STYLE_ALIASES.get(s, s)
    return s if s in STYLES else ''

@st.cache_resource
def get_jp_font():
    for p in [
        'ipaexg.ttf',
        '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf',
        '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc',
        '/System/Library/Fonts/Hiragino Sans W3.ttc',
        'C:/Windows/Fonts/meiryo.ttc',
    ]:
        if os.path.exists(p):
            try:
                font_manager.fontManager.addfont(p)
            except Exception:
                pass
            return font_manager.FontProperties(fname=p)
    return None

jp_font = get_jp_font()
if jp_font is not None:
    try:
        plt.rcParams['font.family'] = jp_font.get_name()
    except Exception:
        pass

# ===== ã‚¹ã‚¿ã‚¤ãƒ«é–¢ä¿‚ =====
WAKU_COL = {1:"#ffffff",2:"#000000",3:"#e6002b",4:"#1560bd",5:"#ffd700",6:"#00a04b",7:"#ff7f27",8:"#f19ec2"}

def _style_waku(s: pd.Series):
    out=[]
    for v in s:
        if pd.isna(v):
            out.append("")
        else:
            v=int(v); bg=WAKU_COL.get(v,"#fff"); fg="#000" if v==1 else "#fff"
            out.append(f"background-color:{bg}; color:{fg}; font-weight:700; text-align:center;")
    return out

# ===== é–¢æ•°ç¾¤ =====

def season_of(m: int) -> str:
    if 3<=m<=5: return 'æ˜¥'
    if 6<=m<=8: return 'å¤'
    if 9<=m<=11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    s = pd.to_numeric(s, errors='coerce')
    std = s.std(ddof=0)
    if not np.isfinite(std) or std==0: return pd.Series([50]*len(s), index=s.index)
    return 50 + 10*(s - s.mean())/std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x,float) and np.isnan(x)): return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + float('0.'+m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + int(m.group(3))/10
    try: return float(s)
    except: return np.nan

def _trim_name(x):
    try: return str(x).replace('\u3000',' ').strip()
    except: return str(x)

# å®‰å®šã—ãŸé‡ã¿ä»˜ãæ¨™æº–åå·®

def w_std_unbiased(x, w, ddof=1):
    x=np.asarray(x,float); w=np.asarray(w,float)
    sw=w.sum()
    if not np.isfinite(sw) or sw<=0: return np.nan
    m=np.sum(w*x)/sw
    var=np.sum(w*(x-m)**2)/sw
    n_eff=(sw**2)/np.sum(w**2) if np.sum(w**2)>0 else 0
    if ddof and n_eff>ddof: var*= n_eff/(n_eff-ddof)
    return float(np.sqrt(max(var,0.0)))

# NDCG@kï¼ˆå®‰å…¨ï¼‰


def ndcg_by_race(frame: pd.DataFrame, scores, k: int=3) -> float:
    f = frame[['race_id','y']].copy().reset_index(drop=True)
    s = np.asarray(scores,float)
    if len(s)!=len(f): s=s[:len(f)]
    vals=[]
    for _, idx in f.groupby('race_id').groups.items():
        idx=np.asarray(list(idx),int)
        y_true=np.nan_to_num(f.loc[idx,'y'].astype(float).to_numpy(), nan=0.0)
        y_pred=np.nan_to_num(s[idx].astype(float), nan=0.0)
        m=len(idx)
        if m==0: continue
        if m==1:
            vals.append(1.0 if y_true[0]>0 else 0.0); continue
        kk=int(min(max(1,k),m))
        order=np.argsort(-y_pred)
        gains=(2.0**y_true[order]-1.0)
        discounts=1.0/np.log2(np.arange(2,m+2))
        dcg=float(np.sum(gains[:kk]*discounts[:kk]))
        order_best=np.argsort(-y_true)
        gains_best=(2.0**y_true[order_best]-1.0)
        idcg=float(np.sum(gains_best[:kk]*discounts[:kk]))
        vals.append(dcg/idcg if idcg>0 else 0.0)
    return float(np.mean(vals)) if vals else float('nan')

# === å®‰å…¨ãªç­‰æ¸©å›å¸°é©ç”¨ ===
def safe_iso_predict(ir, p_vec: np.ndarray) -> np.ndarray:
    x = np.asarray(p_vec, float)
    x = np.nan_to_num(x, nan=1.0 / max(len(x), 1), posinf=1 - 1e-6, neginf=1e-6)
    x = np.clip(x, 1e-6, 1 - 1e-6)
    try:
        y = ir.predict(x)
        y = np.nan_to_num(y, nan=x.mean(), posinf=1 - 1e-6, neginf=1e-6)
        y = np.clip(y, 1e-6, 1 - 1e-6)
        s = y.sum()
        return (y / s) if s > 0 else x
    except Exception:
        return x

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ =====
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®šï¼ˆAUTOçµ±åˆï¼‰")
MODE = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ["AUTOï¼ˆæ¨å¥¨ï¼‰","æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰"], index=0, horizontal=True)

with st.sidebar.expander("ğŸ”° åŸºæœ¬", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
    agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
    agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
    agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)

with st.sidebar.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶", expanded=True):
    TARGET_GRADE    = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", ["G1","G2","G3","L","OP"], index=4)
    TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ","ãƒ€"], index=0)
    TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", 1000, 3600, 1800, 100)
    TARGET_TURN     = st.radio("å›ã‚Š", ["å³","å·¦"], index=0, horizontal=True)

with st.sidebar.expander("ğŸ›  å®‰å®šåŒ–/è£œæ­£", expanded=True):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)

with st.sidebar.expander("ğŸ“ ç¢ºç‡æ ¡æ­£", expanded=False):
    do_calib = st.checkbox("ç­‰æ¸©å›å¸°ã§å‹ç‡ã‚’æ ¡æ­£", value=False)

with st.sidebar.expander("ğŸ› æ‰‹å‹•ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰", expanded=(MODE=="æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰")):
    # æ‰‹å‹•æ™‚ã®ã¿ä½¿ã†ã€‚AUTOã§ã¯å†…éƒ¨ã§æ±ºå®šã€‚
    besttime_w_manual = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿(æ‰‹å‹•)", 0.0, 2.0, 1.0)
    dist_bw_m_manual  = st.slider("è·é›¢å¸¯ã®å¹…[æ‰‹å‹•]", 50, 600, 200, 25)
    mc_beta_manual    = st.slider("PLæ¸©åº¦Î²(æ‰‹å‹•)", 0.3, 5.0, 1.4, 0.1)

with st.sidebar.expander("ğŸ–¥ è¡¨ç¤º", expanded=False):
    FULL_TABLE_VIEW = st.checkbox("å…¨é ­è¡¨ç¤ºï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç„¡ã—ï¼‰", True)
    MAX_TABLE_HEIGHT = st.slider("æœ€å¤§é«˜ã•(px)", 800, 10000, 5000, 200)
    SHOW_CORNER = st.checkbox("4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã‚’è¡¨ç¤º", False)

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
st.title("ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆAUTOç‰ˆï¼‰")
st.subheader("Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰")
excel_file = st.file_uploader("Excelï¼ˆ.xlsxï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    return s0, s1

sheet0, sheet1 = load_excel_bytes(excel_file.getvalue())

# ===== åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè»½é‡ï¼‰ =====

def _norm_col(s: str) -> str:
    s=str(s).strip(); s=re.sub(r'\s+','',s)
    return s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')).replace('ï¼ˆ','(').replace('ï¼‰',')')

def _auto_pick(df, pats):
    cmap={c:_norm_col(c) for c in df.columns}
    for c, n in cmap.items():
        for p in pats:
            if re.search(p, n, flags=re.I):
                return c
    return None

def _map_ui(df, patterns, required, title, key_prefix):
    cols = list(df.columns)
    auto = {k: _auto_pick(df, pats) for k, pats in patterns.items()}
    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=False):
        mapping = {}
        for k, _ in patterns.items():
            options = ['<æœªé¸æŠ>'] + cols
            default = auto.get(k) if auto.get(k) in cols else '<æœªé¸æŠ>'
            mapping[k] = st.selectbox(k, options, index=options.index(default), key=f"map:{key_prefix}:{k}")
    for k in required:
        if mapping.get(k, '<æœªé¸æŠ>') == '<æœªé¸æŠ>':
            st.error(f"{title} å¿…é ˆåˆ—ãŒæœªé¸æŠ: {k}")
            st.stop()
    return {k: (v if v != '<æœªé¸æŠ>' else None) for k, v in mapping.items()}


PAT_S0 = {
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥':[r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜(?!S)|å¹´æœˆæ—¥|æ–½è¡Œæ—¥|é–‹å‚¬æ—¥'],
    'ç«¶èµ°å':[r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å':[r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°':[r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †':[r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ':[r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3F'],
    'ä¸Š3Fé †ä½':[r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’':[r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼|4è§’é€šéé †'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’':[r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢':[r'è·é›¢'],
    'èŠãƒ»ãƒ€':[r'èŠ.?ãƒ».?ãƒ€|èŠãƒ€|ã‚³ãƒ¼ã‚¹|é¦¬å ´ç¨®åˆ¥|Surface'],
    'é¦¬å ´':[r'é¦¬å ´(?!.*æŒ‡æ•°)|é¦¬å ´çŠ¶æ…‹'],
    'å ´å':[r'å ´å|å ´æ‰€|ç«¶é¦¬å ´|é–‹å‚¬(åœ°|å ´|å ´æ‰€)'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']

# è‡ªå‹•ã§æ‹¾ã„ã€è¶³ã‚Šãªã„æ™‚ã ã‘UI
MAP_S0 = {k: _auto_pick(sheet0, v) for k,v in PAT_S0.items()}
missing = [k for k in REQ_S0 if MAP_S0.get(k) is None]
if missing:
    MAP_S0 = _map_ui(sheet0, PAT_S0, REQ_S0, 'sheet0ï¼ˆéå»èµ°ï¼‰', 's0')

# å–ã‚Šå‡ºã— & å‰å‡¦ç†
s0 = pd.DataFrame()
for k, col in MAP_S0.items():
    if col and col in sheet0.columns:
        s0[k]=sheet0[col]

s0['ãƒ¬ãƒ¼ã‚¹æ—¥']=pd.to_datetime(s0['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
for c in ['é ­æ•°','ç¢ºå®šç€é †','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½','é€šé4è§’','è·é›¢']:
    if c in s0: s0[c]=pd.to_numeric(s0[c], errors='coerce')
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' in s0: s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’']=s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].apply(_parse_time_to_sec)
if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in s0: s0['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']=s0['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)
# === ã‚¿ã‚¤ãƒ /æŒ‡æ¨™ã®å³å¯†æ•°å€¤åŒ– ===
for col in ['PCI', 'PCI3', 'Ave-3F']:
    if col in s0.columns:
        s0[col] = pd.to_numeric(s0[col], errors='coerce')

if 'è·é›¢' in s0.columns:
    s0['è·é›¢'] = pd.to_numeric(s0['è·é›¢'], errors='coerce')

# ã‚·ãƒ¼ãƒˆ1
PAT_S1={
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª':[r'è„šè³ª'],
    'å‹ç‡':[r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡':[r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡':[r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ':[r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1=['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢']
MAP_S1 = {k: _auto_pick(sheet1, v) for k,v in PAT_S1.items()}
miss1=[k for k in REQ_S1 if MAP_S1.get(k) is None]
if miss1:
    MAP_S1=_map_ui(sheet1, PAT_S1, REQ_S1, 'sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰', 's1')

s1=pd.DataFrame()
for k, col in MAP_S1.items():
    if col and col in sheet1.columns:
        s1[k]=sheet1[col]

for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in s1: s1[c]=pd.to_numeric(s1[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in s1: s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']=s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# å…ˆé ­ç©ºè¡Œ/ç©ºé¦¬åé™¤å»
s1 = s1.replace(r'^\s*$', np.nan, regex=True).dropna(how='all')
if 'é¦¬å' in s1:
    s1['é¦¬å']=s1['é¦¬å'].astype(str).str.replace('\u3000',' ').str.strip()
    s1=s1[s1['é¦¬å'].ne('')]
s1=s1.reset_index(drop=True)

# è„šè³ªã‚¨ãƒ‡ã‚£ã‚¿ï¼ˆAUTOã§å…¨è£œå®Œâ†’è¶³ã‚Šãªã„ã¨ã“æ‰‹ç›´ã—å¯ï¼‰
if 'è„šè³ª' not in s1.columns: s1['è„šè³ª']=''
if 'æ–¤é‡' not in s1.columns: s1['æ–¤é‡']=np.nan
if 'é¦¬ä½“é‡' not in s1.columns: s1['é¦¬ä½“é‡']=np.nan

st.subheader("é¦¬ä¸€è¦§ï¼ˆå¿…è¦ãªã‚‰è„šè³ª/æ–¤é‡/ä½“é‡ã‚’èª¿æ•´ï¼‰")

# --- è„šè³ªã®è‡ªå‹•æ¨å®šï¼ˆè»½é‡/æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç°¡ç•¥ç‰ˆï¼‰ ---

def auto_style_from_history(df: pd.DataFrame, n_recent=5, hl_days=180):
    need={'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
    if not need.issubset(df.columns):
        return pd.DataFrame({'é¦¬å':[],'æ¨å®šè„šè³ª':[]})
    t=df[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’','ä¸Š3Fé †ä½']].dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’']).copy()
    t=t.sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    t['_rn']=t.groupby('é¦¬å').cumcount()+1
    t=t[t['_rn']<=n_recent].copy()
    today=pd.Timestamp.today()
    t['_days']=(today-pd.to_datetime(t['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    t['_w']=0.5 ** (t['_days']/float(hl_days))
    denom=(pd.to_numeric(t['é ­æ•°'], errors='coerce')-1).replace(0,np.nan)
    pos_ratio=(pd.to_numeric(t['é€šé4è§’'], errors='coerce')-1)/denom
    pos_ratio=pos_ratio.clip(0,1).fillna(0.5)
    if 'ä¸Š3Fé †ä½' in t.columns:
        ag=pd.to_numeric(t['ä¸Š3Fé †ä½'], errors='coerce')
        close=((3.5-ag)/3.5).clip(0,1).fillna(0.0)
    else:
        close=pd.Series(0.0, index=t.index)
    b={'é€ƒã’':-1.2,'å…ˆè¡Œ':0.6,'å·®ã—':0.3,'è¿½è¾¼':-0.7}
    t['L_é€ƒã’']= b['é€ƒã’'] + 1.6*(1-pos_ratio) - 1.2*close
    t['L_å…ˆè¡Œ']= b['å…ˆè¡Œ']+ 1.1*(1-pos_ratio) - 0.1*close
    t['L_å·®ã—']= b['å·®ã—']+ 1.1*(pos_ratio)   + 0.9*close
    t['L_è¿½è¾¼']= b['è¿½è¾¼']+ 1.6*(pos_ratio)   + 0.5*close
    rows=[]
    for name,g in t.groupby('é¦¬å'):
        w=g['_w'].to_numpy(); sw=w.sum()
        if sw<=0: continue
        vec=np.array([
            float((g['L_é€ƒã’']*w).sum()/sw),
            float((g['L_å…ˆè¡Œ']*w).sum()/sw),
            float((g['L_å·®ã—']*w).sum()/sw),
            float((g['L_è¿½è¾¼']*w).sum()/sw)
        ])
        vec=vec-vec.max(); p=np.exp(vec); p/=p.sum()
        rows.append([name, STYLES[int(np.argmax(p))]])
    return pd.DataFrame(rows, columns=['é¦¬å','æ¨å®šè„šè³ª'])

pred_style = auto_style_from_history(s0.copy())

# ãƒãƒ¼ã‚¸ï¼ˆæ‰‹å…¥åŠ›å„ªå…ˆã€‚ç©ºæ¬„ã¯è‡ªå‹•ã§åŸ‹ã‚ã‚‹ï¼‰
s1['è„šè³ª']=s1['è„šè³ª'].map(normalize_style)
if not pred_style.empty:
    s1=s1.merge(pred_style, on='é¦¬å', how='left')
    s1['è„šè³ª']=s1['è„šè³ª'].where(s1['è„šè³ª'].astype(str).str.strip().ne(''), s1['æ¨å®šè„šè³ª'])
    s1.drop(columns=['æ¨å®šè„šè³ª'], inplace=True)

# ç·¨é›†UIï¼ˆãŸã ã—å¿…é ˆå…¥åŠ›ã¯æ’¤å»ƒï¼‰
H = (lambda n: int(min(MAX_TABLE_HEIGHT, 38 + 35*max(1,int(n)) + 28)) if FULL_TABLE_VIEW else 460)
edit = st.data_editor(
    s1[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
    column_config={
        'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
        'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
        'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1),
    },
    use_container_width=True,
    num_rows='static',
    height=H(len(s1)),
    hide_index=True,
)
horses = edit.copy()

# ===== æ¤œè¨¼è»½ãƒã‚§ãƒƒã‚¯ =====
problems=[]
for c in ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']:
    if c not in s0.columns: problems.append(f"sheet0 å¿…é ˆåˆ—ãŒä¸è¶³: {c}")
if 'é€šé4è§’' in s0.columns and 'é ­æ•°' in s0.columns:
    tmp=s0[['é€šé4è§’','é ­æ•°']].dropna()
    if len(tmp)>0 and ((tmp['é€šé4è§’']<1)|(tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
        problems.append('sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–')
if problems:
    st.warning("å…¥åŠ›ãƒã‚§ãƒƒã‚¯:\n- "+"\n- ".join(problems))

# ===== ãƒãƒ¼ã‚¸ï¼ˆéå»èµ°Ã—å½“æ—¥æƒ…å ±ï¼‰ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    s0.drop(columns=[dup], errors='ignore', inplace=True)

df = s0.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')

# ===== 1èµ°ã‚¹ã‚³ã‚¢ï¼ˆå¾“æ¥ï¼‹è»½é‡ï¼‰ =====

CLASS_PTS={'G1':10,'G2':8,'G3':6,'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5,'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}

def normalize_grade_text(x: str|None) -> str|None:
    if x is None or (isinstance(x,float) and np.isnan(x)): return None
    s=str(x).translate(_fw)
    s=(s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
        .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s=re.sub(r'G\s*III','G3',s,flags=re.I)
    s=re.sub(r'G\s*II','G2',s,flags=re.I)
    s=re.sub(r'G\s*I','G1',s,flags=re.I)
    s=re.sub(r'ï¼ªï¼°ï¼®','Jpn',s,flags=re.I)
    s=re.sub(r'JPN','Jpn',s,flags=re.I)
    s=re.sub(r'Jpn\s*III','Jpn3',s,flags=re.I)
    s=re.sub(r'Jpn\s*II','Jpn2',s,flags=re.I)
    s=re.sub(r'Jpn\s*I','Jpn1',s,flags=re.I)
    m=re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def class_points(row) -> int:
    g=normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row: g=normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name=str(row.get('ã‚¯ãƒ©ã‚¹å',''))+' '+str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

race_date = pd.Timestamp.today()

# é©æ­£é¦¬ä½“é‡ï¼ˆæœ€è‰¯ç€é †æ™‚ï¼‰
best_bw_map={}
if {'é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †'}.issubset(df.columns):
    _bw=df[['é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †']].dropna()
    _bw['ç¢ºå®šç€é †']=pd.to_numeric(_bw['ç¢ºå®šç€é †'], errors='coerce')
    _bw=_bw[_bw['ç¢ºå®šç€é †'].notna()]
    try:
        best_idx=_bw.groupby('é¦¬å')['ç¢ºå®šç€é †'].idxmin()
        best_bw_map=_bw.loc[best_idx].set_index('é¦¬å')['é¦¬ä½“é‡'].astype(float).to_dict()
    except Exception:
        best_bw_map={}

# 1èµ°ã‚¹ã‚³ã‚¢

def calc_score(r):
    g=class_points(r)
    raw = g*(r['é ­æ•°'] + 1 - r['ç¢ºå®šç€é †']) + lambda_part*g
    # è»½ãƒœãƒ¼ãƒŠã‚¹
    grade_point = grade_bonus if (normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))) in ['G1','G2','G3'] else 0
    agari_bonus = 0
    try:
        ao=int(r.get('ä¸Š3Fé †ä½', np.nan))
        if ao==1: agari_bonus=agari1_bonus
        elif ao==2: agari_bonus=agari2_bonus
        elif ao==3: agari_bonus=agari3_bonus
    except: pass
    # é¦¬ä½“é‡Â±10kg
    body_bonus=0
    try:
        name=r['é¦¬å']; now_bw=float(r.get('é¦¬ä½“é‡', np.nan))
        tekitai=float(best_bw_map.get(name,np.nan))
        if np.isfinite(now_bw) and np.isfinite(tekitai) and abs(now_bw-tekitai)<=10:
            body_bonus=2
    except: pass
    return raw + grade_point + agari_bonus + body_bonus

if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df.columns:
    st.error('ãƒ¬ãƒ¼ã‚¹æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'); st.stop()

# ä¸€æ—¦æ­£è¦åŒ–
_df = df.copy()
_df['score_raw'] = _df.apply(calc_score, axis=1)
if _df['score_raw'].max()==_df['score_raw'].min():
    _df['score_norm']=50.0
else:
    _df['score_norm'] = (_df['score_raw'] - _df['score_raw'].min()) / (_df['score_raw'].max()-_df['score_raw'].min())*100

# æ™‚ç³»åˆ—é‡ã¿
now = pd.Timestamp.today()
_df['_days_ago']=(now - _df['ãƒ¬ãƒ¼ã‚¹æ—¥']).dt.days
_df['_w'] = 0.5 ** (_df['_days_ago'] / (half_life_m*30.4375)) if half_life_m>0 else 1.0

# ===== A) ãƒ¬ãƒ¼ã‚¹å†…ãƒ‡ãƒ•ãƒ¬ãƒ¼ãƒˆï¼ˆæ¨™æº–ï¼‰ =====

def _make_race_id_for_hist(dfh: pd.DataFrame) -> pd.Series:
    return pd.to_datetime(dfh['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d').fillna('00000000') + '_' + dfh['ç«¶èµ°å'].astype(str).fillna('')

_df['rid_hist'] = _make_race_id_for_hist(_df)
med = _df.groupby('rid_hist')['score_norm'].transform('median')
_df['score_adj'] = _df['score_norm'] - med

# ===== å³/å·¦å›ã‚Šï¼ˆå ´åç°¡æ˜“ï¼‰ =====
DEFAULT_VENUE_TURN = {'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦','ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'}

def infer_turn_row(row):
    name=str(row.get('ç«¶èµ°å',''))
    for v,t in DEFAULT_VENUE_TURN.items():
        if v in name: return t
    return np.nan

if 'å›ã‚Š' not in _df.columns:
    _df['å›ã‚Š'] = _df.apply(infer_turn_row, axis=1)

# ===== B) Î²è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° =====

def tune_beta(df_hist: pd.DataFrame, betas=np.linspace(0.6, 2.4, 19)) -> float:
    dfh = df_hist.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid']=_make_race_id_for_hist(dfh)
    def logloss(beta: float):
        tot=0.0; n=0
        for _, g in dfh.groupby('rid'):
            x=g['score_adj'].astype(float).to_numpy()
            y=(pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int).to_numpy()
            if len(x)<2: continue
            p=np.exp(beta*(x-x.max())); s=p.sum();
            if s<=0 or not np.isfinite(s): continue
            p/=s; p=np.clip(p,1e-9,1-1e-9)
            tot+=-np.mean(y*np.log(p) + (1-y)*np.log(1-p)); n+=1
        return tot/max(n,1)
    return float(min(betas, key=logloss))

# === ã‚¿ã‚¤ãƒ åˆ†ä½å›å¸°ï¼ˆGBRï¼‰ + åŠ é‡CV ===
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GroupKFold

def _recent_weight_from_dates(dates: pd.Series, half_life_days: float) -> np.ndarray:
    days = (pd.Timestamp.today() - pd.to_datetime(dates, errors='coerce')).dt.days
    days = days.clip(lower=0).fillna(9999)
    H = max(1.0, float(half_life_days))
    return (0.5 ** (days / H)).to_numpy(float)

def _weighted_mu_sigma(X: np.ndarray, w: np.ndarray):
    w = w / max(w.sum(), 1e-12)
    mu = (w[:, None] * X).sum(axis=0)
    sd = np.sqrt(np.maximum((w[:, None] * (X - mu) ** 2).sum(axis=0), 1e-12))
    return mu, sd

def _standardize_with_mu_sigma(X: np.ndarray, mu: np.ndarray, sd: np.ndarray):
    return (X - mu) / sd

def _build_time_features(df_hist: pd.DataFrame, dist_turn_df: pd.DataFrame | None = None):
    need = {'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’', 'è·é›¢', 'æ–¤é‡', 'èŠãƒ»ãƒ€', 'ãƒ¬ãƒ¼ã‚¹æ—¥', 'é¦¬å'}
    if not need.issubset(df_hist.columns):
        return None

    d = df_hist.copy()
    d = d.dropna(subset=list(need))
    if d.empty:
        return None

    feats = ['è·é›¢', 'æ–¤é‡', 'is_dirt']
    d['is_dirt'] = d['èŠãƒ»ãƒ€'].astype(str).str.contains('ãƒ€').astype(int)

    for c in ['PCI', 'PCI3', 'Ave-3F', 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']:
        if c in d.columns:
            feats.append(c)

    if 'PCI' in d.columns:
        d['è·é›¢xPCI'] = d['è·é›¢'] * d['PCI']; feats.append('è·é›¢xPCI')
    if 'PCI3' in d.columns and 'PCI' in d.columns:
        d['PCIgap'] = d['PCI3'] - d['PCI']; feats.append('PCIgap')

    if dist_turn_df is not None and 'DistTurnZ' in dist_turn_df.columns:
        dt = dist_turn_df[['é¦¬å', 'DistTurnZ']].drop_duplicates()
        d = d.merge(dt, on='é¦¬å', how='left')
        feats.append('DistTurnZ')

    X = d[feats].astype(float).to_numpy()
    y = pd.to_numeric(d['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'], errors='coerce').to_numpy(float)
    groups = d['é¦¬å'].astype(str).to_numpy()
    dates = pd.to_datetime(d['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')

    col_means = np.nanmean(X, axis=0)
    X = np.where(np.isfinite(X), X, col_means)

    return {'frame': d, 'X': X, 'y': y, 'feats': feats, 'groups': groups, 'dates': dates, 'col_means': col_means}

def fit_time_quantile_model(df_hist: pd.DataFrame, dist_turn_today_df: pd.DataFrame,
                            half_life_days: float, q_list=(0.2, 0.5, 0.8),
                            param_grid=(80, 120, 160), random_state=42):
    built = _build_time_features(df_hist, dist_turn_today_df)
    if built is None:
        return None

    X = built['X']; y = built['y']; groups = built['groups']; dates = built['dates']
    feats = built['feats']; col_means = built['col_means']

    # æ™‚ç³»åˆ—é‡ã¿ & æ¨™æº–åŒ–
    w = _recent_weight_from_dates(dates, half_life_days=max(1.0, float(half_life_days)))
    mu, sd = _weighted_mu_sigma(X, w)
    Xs = _standardize_with_mu_sigma(X, mu, sd)

    # ===== ã“ã“ã‹ã‚‰ãŒå®‰å…¨CVãƒ–ãƒ­ãƒƒã‚¯ï¼ˆã™ã¹ã¦é–¢æ•°å†…ã«ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼‰ =====
    n_groups = int(len(np.unique(groups)))
    n_splits = max(2, min(5, n_groups))
    best_param, best_rmse = None, 1e18

    if n_groups >= 2 and Xs.shape[0] >= 20:
        gkf = GroupKFold(n_splits=n_splits)
        for n_est in param_grid:
            rmse_fold = []
            for tr, va in gkf.split(Xs, y, groups=groups):
                Xt, Xv = Xs[tr], Xs[va]
                yt, yv = y[tr], y[va]
                wt, wv = w[tr], w[va]
                med = GradientBoostingRegressor(loss='quantile', alpha=0.5,
                                                n_estimators=n_est, max_depth=3,
                                                random_state=random_state)
                med.fit(Xt, yt, sample_weight=wt)
                pred = med.predict(Xv)
                rmse = np.sqrt(np.average((yv - pred) ** 2, weights=wv))
                rmse_fold.append(rmse)
            score = float(np.mean(rmse_fold)) if rmse_fold else 1e18
            if score < best_rmse:
                best_rmse, best_param = score, n_est
    else:
        # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„/ã‚°ãƒ«ãƒ¼ãƒ—ãŒ1ç¨®ã®ã¨ãã¯æ—¢å®šæœ¬æ•°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        best_param = sorted(param_grid)[len(param_grid)//2]

    # å¿µã®ãŸã‚ã®äºŒé‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if best_param is None:
        best_param = sorted(param_grid)[len(param_grid)//2]

    # å­¦ç¿’ï¼ˆåˆ†ä½ q ã”ã¨ï¼‰
    models = {}
    for q in q_list:
        m = GradientBoostingRegressor(loss='quantile', alpha=q,
                                      n_estimators=best_param, max_depth=3,
                                      random_state=random_state)
        m.fit(Xs, y, sample_weight=w)
        models[q] = m

    # æ®‹å·®åˆ†æ•£ã®æ¨å®šï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³äºˆæ¸¬åŸºæº–ï¼‰
    resid = y - models[0.5].predict(Xs)
    sigma_hat = float(np.sqrt(np.maximum(np.average(resid ** 2, weights=w), 1e-6)))

    return {
        'feats': feats, 'mu': mu, 'sd': sd, 'col_means': col_means,
        'models': models, 'n_estimators': int(best_param), 'sigma_hat': sigma_hat
    }


def _field_pci_from_pace(pace_type: str) -> float:
    return {'ãƒã‚¤ãƒšãƒ¼ã‚¹': 46.0, 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹': 50.0, 'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 53.0, 'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 56.0}.get(str(pace_type), 50.0)

def build_today_design(horses_today: pd.DataFrame, s0_hist: pd.DataFrame,
                       target_distance: int, target_surface: str,
                       dist_turn_today_df: pd.DataFrame, feats: list[str]):
    rec_w = None
    if not s0_hist.empty and 'ãƒ¬ãƒ¼ã‚¹æ—¥' in s0_hist:
        rec_w = 0.5 ** ((pd.Timestamp.today() - pd.to_datetime(s0_hist['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0) / 180.0)

    def _wmean(s, w):
        s = pd.to_numeric(s, errors='coerce')
        w = pd.to_numeric(w, errors='coerce')
        m = np.nansum(s * w); sw = np.nansum(w)
        return float(m / sw) if sw > 0 else np.nan

    pci_wmean, pci3_wmean, ave3_wmean, a3_wmedian = {}, {}, {}, {}
    if 'PCI' in s0_hist.columns:
        pci_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI'], g['_w'])).to_dict()
    if 'PCI3' in s0_hist.columns:
        pci3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI3'], g['_w'])).to_dict()
    if 'Ave-3F' in s0_hist.columns:
        ave3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['Ave-3F'], g['_w'])).to_dict()
    if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in s0_hist.columns:
        a3_wmedian = s0_hist.groupby('é¦¬å')['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].median().to_dict()

    dtt = dist_turn_today_df[['é¦¬å', 'DistTurnZ']].drop_duplicates() if ('DistTurnZ' in dist_turn_today_df.columns) \
        else pd.DataFrame({'é¦¬å': horses_today['é¦¬å'], 'DistTurnZ': np.nan})
    H = horses_today.merge(dtt, on='é¦¬å', how='left')

    rows = []
    for _, r in H.iterrows():
        name = str(r['é¦¬å'])
        x = {}
        x['è·é›¢'] = float(target_distance)
        x['æ–¤é‡'] = float(r.get('æ–¤é‡', np.nan))
        x['is_dirt'] = 1.0 if str(target_surface).startswith('ãƒ€') else 0.0

        pci_field = _field_pci_from_pace(globals().get('pace_type', 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'))
        if 'PCI' in feats:
            x['PCI'] = float(pci_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI']):
                x['PCI'] = pci_field
        if 'PCI3' in feats:
            x['PCI3'] = float(pci3_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI3']):
                x['PCI3'] = (x.get('PCI', pci_field) + 1.0)
        if 'Ave-3F' in feats:
            x['Ave-3F'] = float(ave3_wmean.get(name, np.nan))
        if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in feats:
            x['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = float(a3_wmedian.get(name, np.nan))
        if 'è·é›¢xPCI' in feats:
            x['è·é›¢xPCI'] = x['è·é›¢'] * x.get('PCI', pci_field)
        if 'PCIgap' in feats:
            x['PCIgap'] = x.get('PCI3', x.get('PCI', pci_field) + 1.0) - x.get('PCI', pci_field)
        if 'DistTurnZ' in feats:
            x['DistTurnZ'] = float(r.get('DistTurnZ', np.nan))

        rows.append({'é¦¬å': name, **x})
    return pd.DataFrame(rows)

# ===== E) è·é›¢ãƒãƒ³ãƒ‰å¹… è‡ªå‹• =====

def auto_h_m(x_all: np.ndarray) -> float:
    x = pd.to_numeric(pd.Series(x_all), errors='coerce').dropna().to_numpy(float)
    if x.size<3: return 300.0
    q75,q25=np.percentile(x,75),np.percentile(x,25)
    iqr=q75-q25
    sigma=np.std(x)
    bw=0.9*min(sigma, iqr/1.34)*(x.size**(-1/5))
    return float(np.clip(bw, 120.0, 600.0))

# ===== è·é›¢Ã—å›ã‚Š è¿‘å‚åŒ–ï¼ˆNWã€score_adjä½¿ç”¨ï¼‰ =====

def kish_neff(w: np.ndarray) -> float:
    w=np.asarray(w,float); sw=w.sum(); s2=np.sum(w**2)
    return float((sw*sw)/s2) if s2>0 else 0.0

def nw_mean(x, y, w, h):
    x=np.asarray(x,float); y=np.asarray(y,float); w=np.asarray(w,float)
    if len(x)==0: return np.nan
    K=np.exp(-0.5*(x/max(1e-9,h))**2) * w
    sK=K.sum()
    return float((K*y).sum()/sK) if sK>0 else np.nan

# æº–å‚™
hist_for_turn = _df[['é¦¬å','è·é›¢','å›ã‚Š','score_adj','_w']].dropna(subset=['é¦¬å','è·é›¢','score_adj','_w']).copy()

# hé¸å®š
h_auto = auto_h_m(hist_for_turn['è·é›¢'].to_numpy())

# æŒ‡å®šè·é›¢ãƒ»å›ã‚Šã§ã®æ¨å®š

def dist_turn_profile(name: str, df_hist: pd.DataFrame, target_d: int, target_turn: str, h_m: float, opp_turn_w: float=0.5):
    g=df_hist[df_hist['é¦¬å'].astype(str).str.strip()==str(name)].copy()
    if g.empty: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    w0 = g['_w'].to_numpy(float) * np.where(g['å›ã‚Š'].astype(str)==str(target_turn), 1.0, float(opp_turn_w))
    x  = g['è·é›¢'].to_numpy(float)
    y  = g['score_adj'].to_numpy(float)
    msk=np.isfinite(x)&np.isfinite(y)&np.isfinite(w0)
    x,y,w0=x[msk],y[msk],w0[msk]
    if x.size==0: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    z_hat = nw_mean(x-float(target_d), y, w0, h_m)
    w_eff = kish_neff(np.exp(-0.5*((x-float(target_d))/max(1e-9,h_m))**2)*w0)
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    ds=np.arange(int(target_d-600), int(target_d+600)+1, 100)
    best_d,best_val=np.nan,-1e18
    for d0 in ds:
        z=nw_mean(x-float(d0), y, w0, h_m)
        if np.isfinite(z) and z>best_val:
            best_val=float(z); best_d=int(d0)
    return {
        'DistTurnZ': float(z_hat) if np.isfinite(z_hat) else np.nan,
        'n_eff_turn': float(w_eff),
        'BestDist_turn': float(best_d) if np.isfinite(best_d) else np.nan,
        'DistTurnZ_best': float(best_val) if np.isfinite(best_val) else np.nan
    }

# ===== é¦¬ã”ã¨é›†è¨ˆ =====
agg=[]
for name, g in _df.groupby('é¦¬å'):
    avg=g['score_norm'].mean()
    std=g['score_norm'].std(ddof=0)
    wavg = np.average(g['score_norm'], weights=g['_w']) if g['_w'].sum()>0 else np.nan
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g)>=2 else np.nan
    agg.append({'é¦¬å':_trim_name(name),'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg=pd.DataFrame(agg)
if df_agg.empty:
    st.error('éå»èµ°ã®é›†è¨ˆãŒç©ºã§ã™ã€‚'); st.stop()

# WStdã®åºŠ
wstd_nontrivial=df_agg.loc[df_agg['Nrun']>=2,'WStd']
def_std=float(wstd_nontrivial.median()) if wstd_nontrivial.notna().any() else 6.0
min_floor=max(1.0, def_std*0.6)
df_agg['WStd']=df_agg['WStd'].fillna(def_std)
df_agg.loc[df_agg['WStd']<min_floor,'WStd']=min_floor

# ä»Šæ—¥æƒ…å ±
for df_ in [df_agg, horses]:
    if 'é¦¬å' in df_.columns: df_['é¦¬å']=df_['é¦¬å'].map(_trim_name)

if 'è„šè³ª' in horses.columns: horses['è„šè³ª']=horses['è„šè³ª'].map(normalize_style)

df_agg = df_agg.merge(horses[['é¦¬å','æ ','ç•ª','è„šè³ª']], on='é¦¬å', how='left')

# å³/å·¦é›†è¨ˆï¼ˆscore_adjã®é‡ã¿å¹³å‡ï¼‰
turn_base = _df[['é¦¬å','å›ã‚Š','score_adj','_w']].dropna()
if not turn_base.empty:
    right = (turn_base[turn_base['å›ã‚Š'].astype(str)=='å³']
             .groupby('é¦¬å').apply(lambda s: np.average(s['score_adj'], weights=s['_w']) if s['_w'].sum()>0 else np.nan)
             .rename('RightZ'))
    left  = (turn_base[turn_base['å›ã‚Š'].astype(str)=='å·¦']
             .groupby('é¦¬å').apply(lambda s: np.average(s['score_adj'], weights=s['_w']) if s['_w'].sum()>0 else np.nan)
             .rename('LeftZ'))
    counts = (turn_base.pivot_table(index='é¦¬å', columns='å›ã‚Š', values='score_adj', aggfunc='count')
              .rename(columns={'å³':'nR','å·¦':'nL'}))
    turn_pref = pd.concat([right,left,counts], axis=1).reset_index()
else:
    turn_pref = pd.DataFrame({'é¦¬å':df_agg['é¦¬å']})

for c in ['RightZ','LeftZ','nR','nL']:
    if c not in turn_pref.columns: turn_pref[c]=np.nan

# C) é€£ç¶šTurnPrefPtsï¼šgapÃ—æœ‰åŠ¹æœ¬æ•°ã‚²ãƒ¼ãƒˆ
turn_pref['TurnGap'] = (turn_pref['RightZ'].fillna(0) - turn_pref['LeftZ'].fillna(0))
# æœ‰åŠ¹æœ¬æ•°ã®è¿‘ä¼¼ï¼šnR/nLã®åˆç®—ã‚’Kishé¢¨ã«ï¼ˆå˜ç´”ï¼‰
turn_pref['n_eff_turn'] = (turn_pref['nR'].fillna(0) + turn_pref['nL'].fillna(0)).clip(lower=0)
conf = np.clip(turn_pref['n_eff_turn'] / 3.0, 0.0, 1.0)
turn_pref['TurnPrefPts'] = np.clip(turn_pref['TurnGap']/1.5, -1.0, 1.0) * conf

df_agg = df_agg.merge(turn_pref[['é¦¬å','RightZ','LeftZ','TurnGap','n_eff_turn','TurnPrefPts']], on='é¦¬å', how='left')

# è·é›¢Ã—å›ã‚Šï¼ˆè‡ªå‹•hï¼‰
rows=[]
for nm in df_agg['é¦¬å'].astype(str):
    prof=dist_turn_profile(nm, hist_for_turn, int(TARGET_DISTANCE), str(TARGET_TURN), h_auto, opp_turn_w=0.5)
    rows.append({'é¦¬å':nm, **prof})
_dfturn = pd.DataFrame(rows)
df_agg = df_agg.merge(_dfturn, on='é¦¬å', how='left')

# RecencyZ / StabZ
base_for_recency = df_agg.get('WAvgZ', pd.Series(np.nan, index=df_agg.index)).fillna(df_agg.get('AvgZ', pd.Series(0.0, index=df_agg.index)))
df_agg['RecencyZ']=z_score(pd.to_numeric(base_for_recency, errors='coerce').fillna(0.0))
wstd_fill=pd.to_numeric(df_agg['WStd'], errors='coerce')
if not np.isfinite(wstd_fill).any(): wstd_fill=pd.Series(6.0, index=df_agg.index)
df_agg['StabZ']=z_score(-(wstd_fill.fillna(wstd_fill.median())))

# D) ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ è‡ªå·±å­¦ç¿’
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns and s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].notna().any():
    bt = _df.merge(s1[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
    bt_min=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True); bt_max=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True)
    span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
    bt['BT_norm']=((bt_max - bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])/span).clip(0,1)
    corr = np.corrcoef(bt['BT_norm'].fillna(0.0), bt['score_adj'].fillna(0.0))[0,1]
    if not np.isfinite(corr): corr=0.0
    w_bt = float(np.clip(corr, 0.0, 1.2)) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)
else:
    w_bt = 0.0 if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)

# æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆæœªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™ï¼‰
turn_gain = 1.0
pace_gain = float(pace_gain)
stab_weight = float(stab_weight)
dist_gain = 1.0


# === æ¬ æã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆFinalRawã®ç›´å‰ã§ä¸€åº¦ã ã‘ï¼‰ ===
df_agg['RecencyZ']    = pd.to_numeric(df_agg['RecencyZ'], errors='coerce').fillna(df_agg['RecencyZ'].median())
df_agg['StabZ']       = pd.to_numeric(df_agg['StabZ'],    errors='coerce').fillna(df_agg['StabZ'].median())
df_agg['TurnPrefPts'] = pd.to_numeric(df_agg['TurnPrefPts'], errors='coerce').fillna(0.0)
df_agg['DistTurnZ']   = pd.to_numeric(df_agg['DistTurnZ'],   errors='coerce').fillna(0.0)

df_agg['PacePts']=0.0  # å¾Œã§MCã‹ã‚‰
# FinalRawï¼ˆåŸºç¤ï¼šRecency/Stab/Turn/Dist ã‚’åˆç®—ã€‚BTãƒ»Paceã¯å¾Œã§åŠ ç‚¹ï¼‰
df_agg['FinalRaw'] = (
    df_agg['RecencyZ']
    + float(stab_weight) * df_agg['StabZ']
    + 1.0 * df_agg['TurnPrefPts']
    + 1.0 * df_agg['DistTurnZ'].fillna(0.0)
)


# BTã‚’åŠ ç‚¹ï¼ˆè‡ªå·±å­¦ç¿’ä¿‚æ•°ï¼‰
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns:
    btmap = s1.set_index('é¦¬å')['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].to_dict()
    btvals = df_agg['é¦¬å'].map(btmap)
    if pd.Series(btvals).notna().any():
        bts = pd.Series(btvals)
        bt_min=bts.min(skipna=True); bt_max=bts.max(skipna=True)
        span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
        BT_norm = ((bt_max - bts)/span).clip(0,1).fillna(0.0)
        df_agg['FinalRaw'] += w_bt * BT_norm

# ===== ãƒšãƒ¼ã‚¹MCï¼ˆåå¯¾ç§°Gumbelã§åˆ†æ•£ä½æ¸›ï¼‰ =====
# ç°¡æ˜“: è„šè³ªã‹ã‚‰ãƒšãƒ¼ã‚¹æŒ‡æ•°ã‚’ä½œã‚Šã€å°ã®æœŸå¾…ç‚¹ã‚’åŠ ç®—
# ã“ã“ã§ã¯æ—¢å­˜ã®â€œæœŸå¾…ç‚¹ãƒãƒƒãƒ—â€ã‚’ç”¨ã„ãŸå¹³å‡åŒ–
mark_rule={
    'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
    'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
    'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
}
mark_to_pts={'â—':2,'ã€‡':1,'â—‹':1,'â–³':0,'Ã—':-1}

# è„šè³ªç¢ºç‡ï¼ˆä»Šå›ã¯ç¢ºå®šå€¤â†’one-hotï¼‰
name_list=df_agg['é¦¬å'].tolist()
P=np.zeros((len(name_list),4),float)
for i, nm in enumerate(name_list):
    stl = df_agg.loc[df_agg['é¦¬å']==nm, 'è„šè³ª'].values
    stl = stl[0] if len(stl)>0 else ''
    if stl in STYLES:
        P[i, STYLES.index(stl)] = 1.0
    else:
        P[i,:]=0.25

epi_alpha, epi_beta = 1.0, 0.6
thr_hi, thr_mid, thr_slow = 0.52, 0.30, 0.18

# Î²ï¼šAUTO or æ‰‹å‹•
beta_pl = tune_beta(_df.copy()) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(mc_beta_manual)

# ãƒšãƒ¼ã‚¹MCï¼ˆåå¯¾ç§°ï¼‰
rng = np.random.default_rng(24601)
draws = 4000  # å®Ÿç”¨ååˆ†ï¼ˆå¿…è¦ãªã‚‰UIåŒ–ï¼‰
Hn=len(name_list)
sum_pts=np.zeros(Hn,float); pace_counter={'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
# é€ƒã’/å…ˆè¡Œã‚«ã‚¦ãƒ³ãƒˆ â†’ ãƒšãƒ¼ã‚¹åˆ†é¡
for _ in range(draws//2):
    sampled = [np.argmax(P[i]) for i in range(Hn)]
    nige  = sum(1 for s in sampled if s==0)
    sengo = sum(1 for s in sampled if s==1)
    epi=(epi_alpha*nige + epi_beta*sengo)/max(1,Hn)
    if   epi>=thr_hi:   pace_t='ãƒã‚¤ãƒšãƒ¼ã‚¹'
    elif epi>=thr_mid:  pace_t='ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'
    elif epi>=thr_slow: pace_t='ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
    else:               pace_t='ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
    pace_counter[pace_t]+=2  # å¯¾ç§°ãƒšã‚¢åˆ†ã¾ã¨ã‚ã¦
    mk=mark_rule[pace_t]
    for i,s in enumerate(sampled):
        sum_pts[i]+=2*mark_to_pts[ mk[STYLES[s]] ]

df_agg['PacePts']=sum_pts/max(1,draws)
pace_type=max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'

# === ã‚¿ã‚¤ãƒ åˆ†å¸ƒ â†’ ç€é †MC ===
half_life_days = int(half_life_m * 30.4375) if half_life_m > 0 else 99999

time_model_pkg = fit_time_quantile_model(
    df_hist=_df.copy(),
    dist_turn_today_df=_dfturn.copy(),
    half_life_days=half_life_days,
    q_list=(0.2, 0.5, 0.8),
    param_grid=(80, 120, 160),
    random_state=24601
)

if time_model_pkg is not None:
    feats = time_model_pkg['feats']
    mu, sd = time_model_pkg['mu'], time_model_pkg['sd']
    models = time_model_pkg['models']

    todayX = build_today_design(
        horses_today=horses,
        s0_hist=s0,
        target_distance=int(TARGET_DISTANCE),
        target_surface=str(TARGET_SURFACE),
        dist_turn_today_df=_dfturn,
        feats=feats
    )

    v = todayX[feats].astype(float).to_numpy()
    v = np.where(np.isfinite(v), v, time_model_pkg['col_means'])
    vs = (v - mu) / sd

    Q20 = models[0.2].predict(vs)
    Q50 = models[0.5].predict(vs)
    Q80 = models[0.8].predict(vs)

    z08 = 0.8416212335729143
    sigma_from_span = (Q80 - Q20) / (2.0 * z08)
    sigma_raw = np.maximum(sigma_from_span, 0.25)

    sigma_hat = time_model_pkg['sigma_hat']
    sigma = np.sqrt(0.5 * sigma_raw ** 2 + 0.5 * sigma_hat ** 2)

    pred_time = pd.DataFrame({
        'é¦¬å': todayX['é¦¬å'],
        'PredTime_s': Q50,
        'PredTime_p20': Q20,
        'PredTime_p80': Q80,
        'PredSigma_s': sigma
    })

    draws_mc = 12000
    rng_t = np.random.default_rng(13579)
    names = pred_time['é¦¬å'].tolist()
    mu_vec = pred_time['PredTime_s'].to_numpy(float)
    sig_vec = pred_time['PredSigma_s'].to_numpy(float)
    n = len(mu_vec)

    tau = 0.30 * float(np.nanmedian(sig_vec))
    E_id = rng_t.normal(size=(draws_mc, n)) * sig_vec[None, :]
    E_cm = rng_t.normal(size=(draws_mc, 1)) * tau
    T = mu_vec[None, :] + E_id + E_cm

    rk = np.argsort(T, axis=1)
    win = np.bincount(rk[:, 0], minlength=n)
    top3 = np.zeros(n, int)
    for k in range(3):
        top3 += np.bincount(rk[:, k], minlength=n)
    exp_rank = (rk.argsort(axis=1) + 1).mean(axis=0)

    pred_time['å‹ç‡%_TIME'] = (100.0 * win / draws_mc).round(2)
    pred_time['è¤‡å‹ç‡%_TIME'] = (100.0 * top3 / draws_mc).round(2)
    pred_time['æœŸå¾…ç€é †_TIME'] = np.round(exp_rank, 3)

    df_agg = df_agg.merge(pred_time, on='é¦¬å', how='left')
else:
    df_agg['PredTime_s'] = np.nan
    df_agg['PredTime_p20'] = np.nan
    df_agg['PredTime_p80'] = np.nan
    df_agg['PredSigma_s'] = np.nan
    df_agg['å‹ç‡%_TIME'] = np.nan
    df_agg['è¤‡å‹ç‡%_TIME'] = np.nan
    df_agg['æœŸå¾…ç€é †_TIME'] = np.nan

# PacePtsåæ˜ 
# Pace ã‚’å¾Œä¹—ã›ï¼ˆåŸºç¤ï¼‹BTã‚’ä¿æŒã—ãŸã¾ã¾ï¼‰
df_agg['PacePts'] = pd.to_numeric(df_agg['PacePts'], errors='coerce').fillna(0.0)

df_agg['FinalRaw'] += float(pace_gain) * df_agg['PacePts']

# ===== å‹ç‡ï¼ˆPLè§£æè§£ï¼‰ï¼† Top3ï¼ˆGumbelåå¯¾ç§°ï¼‰ =====
# -- æ ¡æ­£å™¨ã®å­¦ç¿’ï¼ˆå±¥æ­´ã‹ã‚‰ï¼‰ --
calibrator = None
if do_calib and SK_ISO:
    dfh = _df.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid'] = _make_race_id_for_hist(dfh)
    X, Y = [], []
    for _, g in dfh.groupby('rid'):
        xs = g['score_adj'].astype(float).to_numpy()
        y  = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
        if len(xs) >= 2 and np.unique(y).size >= 2:
            p = np.exp(beta_pl * (xs - xs.max()))
            s = p.sum()
            if np.isfinite(s) and s > 0:
                p = np.clip(p / s, 1e-6, 1-1e-6)
                X.append(p); Y.append(y)
    if X:
        calibrator = IsotonicRegression(out_of_bounds='clip').fit(np.concatenate(X), np.concatenate(Y))

# -- PLå‹ç‡ï¼ˆNaNã‚»ãƒ¼ãƒ•ï¼‰ --
S = pd.to_numeric(df_agg['FinalRaw'], errors='coerce')
finite = np.isfinite(S)

p_win = np.zeros(len(S), float)
if finite.any():
    A = S[finite].to_numpy(float)
    m = A.mean(); s = A.std() + 1e-9
    z = (A - m) / s
    x = beta_pl * (z - z.max())
    ex = np.exp(x)
    p = ex / ex.sum()
    p_win[finite] = p
    if (~finite).any():
        p_win[~finite] = 1e-6
        p_win = p_win / p_win.sum()
else:
    p_win[:] = 1.0 / max(len(S), 1)

if calibrator is not None:
    p_win = safe_iso_predict(calibrator, p_win)

df_agg['å‹ç‡%_PL'] = (100 * p_win).round(2)

# -- Top3è¿‘ä¼¼ï¼ˆGumbelåå¯¾ç§°ã€NaNã‚»ãƒ¼ãƒ•ï¼‰ --
#   abilities ã¯ PLã¨åŒã˜æ¨™æº–åŒ–ã‚¹ã‚³ã‚¢ã‚’å…¨é ­ã«ä½œã‚‹ï¼ˆæ¬ æã¯0ï¼‰
if finite.any():
    A = S.copy()
    A[~finite] = A[finite].mean()
    m = A.mean(); s = A.std() + 1e-9
    abilities = ((A - m) / s).to_numpy(float)
else:
    abilities = np.zeros(len(S), float)

draws_top3 = 8000
rng3 = np.random.default_rng(13579)
G = rng3.gumbel(size=(draws_top3//2, len(abilities)))
U = np.vstack([beta_pl*abilities[None,:] + G, beta_pl*abilities[None,:] - G])
rank_idx = np.argsort(-U, axis=1)
counts = np.zeros(len(abilities), float)
for k in range(3):
    counts += np.bincount(rank_idx[:,k], minlength=len(abilities)).astype(float)
df_agg['è¤‡å‹ç‡%_PL'] = (100*(counts / draws_top3)).round(2)

# ===== H) AR100: åˆ†ä½å†™åƒï¼ˆNaNâ†’ä¸­å¤®å€¤æ‰±ã„ï¼‰ =====
ranks = S.rank(method='average', pct=True).fillna(0.5)
qx = np.array([0.00,0.10,0.25,0.50,0.75,0.90,0.97,1.00])
qy = np.array([40 , 45 , 55 , 65 , 72 , 80 , 90 , 98 ])
df_agg['AR100'] = np.interp(ranks.to_numpy(float), qx, qy)

def to_band(v):
    if not np.isfinite(v): return 'E'
    if v>=90: return 'SS'
    if v>=80: return 'S'
    if v>=70: return 'A'
    if v>=60: return 'B'
    if v>=50: return 'C'
    return 'E'

df_agg['Band'] = df_agg['AR100'].map(to_band)

# ===== ãƒ†ãƒ¼ãƒ–ãƒ«æ•´å½¢ï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ä»˜ãï¼‰ =====
_dfdisp = df_agg.copy().sort_values(['AR100','å‹ç‡%_PL'], ascending=[False, False]).reset_index(drop=True)
_dfdisp['é †ä½'] = np.arange(1, len(_dfdisp)+1)

def _fmt_int(x):
    try:
        return '' if pd.isna(x) else f"{int(x)}"
    except:
        return ''

show_cols = [
    'é †ä½','æ ','ç•ª','é¦¬å','è„šè³ª',
    'AR100','Band',
    'å‹ç‡%_PL','è¤‡å‹ç‡%_PL',
    'å‹ç‡%_TIME','è¤‡å‹ç‡%_TIME','æœŸå¾…ç€é †_TIME',
    'PredTime_s','PredTime_p20','PredTime_p80','PredSigma_s',
    'RecencyZ','StabZ','PacePts','TurnPrefPts','DistTurnZ'
]

JP = {
    'é †ä½':'é †ä½','æ ':'æ ','ç•ª':'é¦¬ç•ª','é¦¬å':'é¦¬å','è„šè³ª':'è„šè³ª',
    'AR100':'AR100','Band':'è©•ä¾¡å¸¯',
    'å‹ç‡%_PL':'å‹ç‡%ï¼ˆPLï¼‰','è¤‡å‹ç‡%_PL':'è¤‡å‹ç‡%ï¼ˆPLï¼‰',
    'å‹ç‡%_TIME':'å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','è¤‡å‹ç‡%_TIME':'è¤‡å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','æœŸå¾…ç€é †_TIME':'æœŸå¾…ç€é †ï¼ˆã‚¿ã‚¤ãƒ ï¼‰',
    'PredTime_s':'äºˆæ¸¬ã‚¿ã‚¤ãƒ ä¸­å¤®å€¤[s]','PredTime_p20':'20%é€Ÿã„å´[s]','PredTime_p80':'80%é…ã„å´[s]','PredSigma_s':'ã‚¿ã‚¤ãƒ åˆ†æ•£Ïƒ[s]',
    'RecencyZ':'è¿‘èµ°Z','StabZ':'å®‰å®šæ€§Z','PacePts':'ãƒšãƒ¼ã‚¹Pts','TurnPrefPts':'å›ã‚ŠåŠ ç‚¹','DistTurnZ':'è·é›¢Ã—å›ã‚ŠZ'
}

_dfdisp_view = _dfdisp[show_cols].rename(columns=JP)

# JPã¯ã€Œè‹±åâ†’æ—¥æœ¬èªåã€ã®è¾æ›¸ã€‚å‚ç…§ã¯è‹±åã‚­ãƒ¼ã§çµ±ä¸€ã—ã¦å€¤ï¼ˆæ—¥æœ¬èªåï¼‰ã‚’ä½¿ã†
fmt = {
    JP['AR100']:'{:.1f}',
    JP['å‹ç‡%_PL']:'{:.2f}',
    JP['è¤‡å‹ç‡%_PL']:'{:.2f}',
    JP['RecencyZ']:'{:.2f}',
    JP['StabZ']:'{:.2f}',
    JP['PacePts']:'{:.2f}',
    JP['TurnPrefPts']:'{:.2f}',
    JP['DistTurnZ']:'{:.2f}',
    JP['PredTime_s']:'{:.3f}',
    JP['PredTime_p20']:'{:.3f}',
    JP['PredTime_p80']:'{:.3f}',
    JP['PredSigma_s']:'{:.3f}',
}

# æ ãƒ»é¦¬ç•ªã¯æ•´æ•°è¡¨ç¤ºã«
num_fmt = {
    JP['æ ']: _fmt_int,
    JP['ç•ª']: _fmt_int,   # â† ãƒªãƒãƒ¼ãƒ å¾Œã¯ã€Œé¦¬ç•ªã€
}
num_fmt.update(fmt)

styled = (
    _dfdisp_view
      .style
      .apply(_style_waku, subset=[JP['æ ']])
      .format(num_fmt, na_rep="")
)


st.markdown("### æœ¬å‘½ãƒªã‚¹ãƒˆï¼ˆAUTOçµ±åˆï¼‰")
st.dataframe(styled, use_container_width=True, height=H(len(_dfdisp_view)))

# ä¸Šä½æŠœç²‹ï¼ˆ6é ­ï¼‰
# _dfdisp ãŒæœªå®šç¾©ã§ã‚‚å‹•ãã‚ˆã†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
if '_dfdisp' in globals():
    base = _dfdisp.rename(columns=JP)   # è‹±åâ†’æ—¥æœ¬èªã«å¤‰æ›
else:
    base = _dfdisp_view                 # æ—¢ã«æ—¥æœ¬èªåˆ—

# head_cols ã¯è‹±åã®ä¸¦ã³ãªã®ã§ã€æ—¥æœ¬èªåã«ãƒãƒƒãƒ—ã—ã¦æŠ½å‡º
cols_jp = [JP[c] if c in JP else c for c in head_cols]
head_view = base[cols_jp].head(6).copy()

st.markdown("#### ä¸Šä½æŠœç²‹")
st.dataframe(
    head_view.style.format({
        JP['æ ']: _fmt_int,
        JP['ç•ª']: _fmt_int,
        JP['AR100']:'{:.1f}',
        JP['å‹ç‡%_PL']:'{:.2f}',
        JP['å‹ç‡%_TIME']:'{:.2f}',
        JP['PacePts']:'{:.2f}',
        JP['PredTime_s']:'{:.3f}',
        JP['PredSigma_s']:'{:.3f}',
    }),
    use_container_width=True, height=H(len(head_view))
)


# è¦‹é€ã‚Šç›®å®‰
if not (_dfdisp['AR100']>=70).any():
    st.warning('ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã¯ã€Œè¦‹é€ã‚Šã€ï¼šAä»¥ä¸Šï¼ˆAR100â‰¥70ï¼‰ãŒä¸åœ¨ã€‚')
else:
    lead = _dfdisp.iloc[0]
    st.info(f"æœ¬å‘½å€™è£œï¼š**{int(lead['æ '])}-{int(lead['ç•ª'])} {lead['é¦¬å']}** / å‹ç‡{lead['å‹ç‡%_PL']:.2f}% / AR100 {lead['AR100']:.1f}")


# 4è§’å›³ï¼ˆä»»æ„ï¼‰
if SHOW_CORNER:
    try:
        # æ—¢å­˜ã®ãƒ¬ãƒ³ãƒ€ãƒ©ãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ã‚‹å‰æã€‚ç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã€‚
        from matplotlib.patches import Wedge, Rectangle, Circle
        # ãƒ€ãƒŸãƒ¼ã®ç°¡æ˜“å¯è¦–åŒ–ï¼ˆæ Ã—è„šè³ªã®æ•£å¸ƒï¼‰
        fig, ax = plt.subplots(figsize=(6,4))
        xs={'é€ƒã’':0.1,'å…ˆè¡Œ':0.3,'å·®ã—':0.7,'è¿½è¾¼':0.9}
        for _,r in _dfdisp.iterrows():
            x=xs.get(r.get('è„šè³ª',''),0.5); y=float(r.get('AR100',50))/100
            ax.scatter(x,y)
            ax.annotate(str(r.get('ç•ª','')), (x,y), xytext=(3,3), textcoords='offset points', fontsize=8)
        ax.set_xlim(0,1); ax.set_ylim(0,1); ax.set_xlabel('è„šè³ªå´'); ax.set_ylabel('AR100æ­£è¦åŒ–')
        ax.grid(alpha=.3)
        st.pyplot(fig)
    except Exception as e:
        st.caption(f"4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼š{e}")

# å¯è¦–åŒ–ã‚¿ãƒ–
with st.expander('ğŸ“Š æ•£å¸ƒå›³ï¼ˆAR100 Ã— PacePtsï¼‰', expanded=False):
    df_plot=_dfdisp[['é¦¬å','AR100','PacePts','å‹ç‡%_PL','è„šè³ª','æ ','ç•ª']].dropna(subset=['AR100','PacePts']).copy()
    if ALT_AVAILABLE and not df_plot.empty:
        ch=(alt.Chart(df_plot).mark_circle().encode(
            x=alt.X('PacePts:Q', title='PacePts'),
            y=alt.Y('AR100:Q', title='AR100'),
            size=alt.Size('å‹ç‡%_PL:Q', title='å‹ç‡%ï¼ˆPLï¼‰', scale=alt.Scale(range=[40, 600])),
            tooltip=['æ ','ç•ª','é¦¬å','è„šè³ª','AR100','å‹ç‡%_PL','PacePts']
        ).properties(height=420))
        st.altair_chart(ch, use_container_width=True)
    elif not df_plot.empty:
        fig=plt.figure(figsize=(8,5)); ax=fig.add_subplot(111)
        s=40 + (df_plot['å‹ç‡%_PL'].fillna(0).to_numpy())*6
        ax.scatter(df_plot['PacePts'], df_plot['AR100'], s=s, alpha=0.6)
        for _, r in df_plot.iterrows():
            ax.annotate(str(int(r['ç•ª'])), (r['PacePts'], r['AR100']), xytext=(2,2), textcoords='offset points', fontsize=8)
        ax.set_xlabel('PacePts'); ax.set_ylabel('AR100'); ax.grid(True, ls='--', alpha=.3)
        st.pyplot(fig)
    else:
        st.info('å¯è¦–åŒ–ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚')

# è¨ºæ–­ã‚¿ãƒ–ï¼ˆæ ¡æ­£/NDCGãªã©ï¼‰
with st.expander('ğŸ“ˆ è¨ºæ–­ï¼ˆæ ¡æ­£ãƒ»NDCGï¼‰', expanded=False):
    # NDCG@3ï¼ˆå‚è€ƒï¼‰
    try:
        df_tmp=_df[['ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_adj','ç¢ºå®šç€é †']].dropna().copy()
        df_tmp['race_id']=pd.to_datetime(df_tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d') + '_' + df_tmp['ç«¶èµ°å'].astype(str)
        df_tmp['y']=(pd.to_numeric(df_tmp['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int)
        # proxy: åŒä¸€åˆ†å¸ƒä¸Šã§ã®softmaxç¢ºç‡
        pr=[]
        for rid, g in df_tmp.groupby('race_id'):
            s=g['score_adj'].astype(float).to_numpy(); p=np.exp(beta_pl*(s-s.max())); p/=p.sum(); pr.append(p)
        p_raw=np.concatenate(pr) if pr else np.array([])
        ndcg=ndcg_by_race(df_tmp[['race_id','y']], p_raw, k=3)
        st.caption(f"NDCG@3ï¼ˆæœªæ ¡æ­£softmaxã®æ“¬ä¼¼ï¼‰: {ndcg:.4f}")
    except Exception:
        pass
    if calibrator is None and do_calib:
        st.warning('æ ¡æ­£å™¨ã®å­¦ç¿’ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚')
    elif calibrator is not None:
        st.success('ç­‰æ¸©å›å¸°ã§å‹ç‡ã‚’æ ¡æ­£ä¸­ã€‚')

st.markdown("""
<small>
- æœ¬ç‰ˆã¯ **AUTOãƒ¢ãƒ¼ãƒ‰** ãŒæ¨™æº–ã§ã™ã€‚æ‰‹å‹•ã¯ã€ŒğŸ› æ‰‹å‹•ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰ã€ã‚’å±•é–‹ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚<br>
- **score_adj**ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ãƒ‡ãƒ•ãƒ¬ãƒ¼ãƒˆï¼‰ã‚’åŸºæº–ã¨ã—ã¦ã€è·é›¢Ã—å›ã‚Š/å³å·¦/å‹ç‡åŒ–ã‚’çµ±ä¸€ã—ã¾ã—ãŸã€‚<br>
- å‹ç‡ã¯ **Plackettâ€“Luceï¼ˆsoftmaxï¼‰**ã€Top3ã¯ **Gumbelåå¯¾ç§°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** ã§è¿‘ä¼¼ã—ã¦ã„ã¾ã™ã€‚<br>
- AR100ã¯ **åˆ†ä½å†™åƒ** ã§ãƒãƒ³ãƒ‰ã®æ„å‘³ã‚’å›ºå®šï¼ˆé †ä½ä¸å¤‰ï¼‰ã€‚<br>
</small>
""", unsafe_allow_html=True)
