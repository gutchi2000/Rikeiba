# keiba_web_app_final.py
# ã‚µã‚¤ãƒ‰ãƒãƒ¼äº’æ›ï¼ˆexpanderï¼‰ã€ç¸¦è»¸å …ç‰¢åŒ–ã€å¹´é½¢/æ é‡ã¿ãƒ»MCãƒ»è¡€çµ±HTML å®Œå‚™
# + å³/å·¦å›ã‚Š è‡ªå‹•åˆ¤å®š&åŠ ç‚¹ + AR100/Bandã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆBä¸­å¿ƒåŒ–ï¼‰+ è¦‹é€ã‚Šãƒ­ã‚¸ãƒƒã‚¯
# + â˜… å…ˆé ­ã®ç©ºè¡Œã‚’ç¢ºå®Ÿã«é™¤å» / 4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã®å®‰å…¨åŒ–
# + â˜… è„šè³ªã‚¨ãƒ‡ã‚£ã‚¿ã®å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ãƒ»å¾©å…ƒï¼ˆãƒªãƒ©ãƒ³ã—ã¦ã‚‚æ¶ˆãˆãªã„ï¼‰
# + â˜… è„šè³ªã®è¡¨è¨˜ã‚†ã‚Œã‚’å¸åï¼ˆã€Œè¿½ã„è¾¼ã¿ã€ã€Œå·®è¾¼ã€ãªã©â†’æ­£è¦åŒ–ï¼‰

import streamlit as st
import pandas as pd
import numpy as np
import re, io, json
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib.patches import Wedge, Rectangle, Circle
from itertools import combinations

# ===== Optional: Altairï¼ˆæ•£å¸ƒå›³ï¼‰ =====
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

# ===== Optional: HTML åŸ‹ã‚è¾¼ã¿ï¼ˆè¡€çµ±è¡¨ç¤ºç”¨ï¼‰ =====
try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

# ---- åŸºæœ¬è¨­å®šã¨ãƒ•ã‚©ãƒ³ãƒˆ ----
@st.cache_resource
def get_jp_font():
    for path in ["ipaexg.ttf", "C:/Windows/Fonts/meiryo.ttc"]:
        try:
            return font_manager.FontProperties(fname=path)
        except Exception:
            pass
    return None

jp_font = get_jp_font()
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['IPAPGothic', 'Meiryo', 'MS Gothic']
st.set_page_config(page_title="ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰", layout="wide")

# ---- ä¾¿åˆ©CSSï¼ˆsidebar å¹…ã ã‘èª¿æ•´ï¼‰----
st.markdown("""
<style>
#MainMenu, footer {visibility:hidden;}
section[data-testid="stSidebar"] {width: 340px !important;}
div.block-container {padding-top: .6rem; padding-bottom: .8rem; max-width: 1400px;}
</style>
""", unsafe_allow_html=True)

STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fwid = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

# === NEW: è„šè³ªã®è¡¨è¨˜ã‚†ã‚Œå¸åï¼ˆæ­£è¦åŒ–é–¢æ•°ï¼‰ ===
STYLE_ALIASES = {
    'è¿½ã„è¾¼ã¿':'è¿½è¾¼','è¿½è¾¼ã¿':'è¿½è¾¼','ãŠã„ã“ã¿':'è¿½è¾¼','ãŠã„è¾¼ã¿':'è¿½è¾¼',
    'ã•ã—':'å·®ã—','å·®è¾¼':'å·®ã—','å·®è¾¼ã¿':'å·®ã—',
    'ã›ã‚“ã“ã†':'å…ˆè¡Œ','å…ˆè¡Œ ':'å…ˆè¡Œ','å…ˆè¡Œã€€':'å…ˆè¡Œ',
    'ã«ã’':'é€ƒã’','é€ƒã’ ':'é€ƒã’','é€ƒã’ã€€':'é€ƒã’'
}
def normalize_style(s: str) -> str:
    s = str(s).replace('ã€€','').strip().translate(_fwid)
    s = STYLE_ALIASES.get(s, s)
    return s if s in STYLES else ''

# ======================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========================
def season_of(month: int) -> str:
    if 3 <= month <= 5: return 'æ˜¥'
    if 6 <= month <= 8: return 'å¤'
    if 9 <= month <= 11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    std = s.std(ddof=0)
    if std == 0 or pd.isna(std):
        return pd.Series([50]*len(s), index=s.index)
    return 50 + 10 * (s - s.mean()) / std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + float('0.'+m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + int(m.group(3))/10
    try:  return float(s)
    except: return np.nan

def normalize_grade_text(x: str | None) -> str | None:
    if x is None or (isinstance(x, float) and np.isnan(x)): 
        return None
    s = str(x).translate(_fwid)
    s = (s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
         .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s = re.sub(r'G\s*III', 'G3', s, flags=re.I)
    s = re.sub(r'G\s*II',  'G2', s, flags=re.I)
    s = re.sub(r'G\s*I',   'G1', s, flags=re.I)
    s = re.sub(r'ï¼ªï¼°ï¼®', 'Jpn', s, flags=re.I)
    s = re.sub(r'JPN',    'Jpn', s, flags=re.I)
    s = re.sub(r'Jpn\s*III', 'Jpn3', s, flags=re.I)
    s = re.sub(r'Jpn\s*II',  'Jpn2', s, flags=re.I)
    s = re.sub(r'Jpn\s*I',   'Jpn1', s, flags=re.I)
    m = re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def safe_take(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    return df[[c for c in cols if c in df.columns]].copy()

def _trim_name(x):
    try:
        return str(x).replace('\u3000',' ').strip()
    except Exception:
        return str(x)

# å®‰å®šã—ãŸé‡ã¿ä»˜ãæ¨™æº–åå·®ï¼ˆä¸åè£œæ­£ã¤ãï¼‰
def w_std_unbiased(x, w, ddof=1):
    x = np.asarray(x, dtype=float)
    w = np.asarray(w, dtype=float)
    sw = w.sum()
    if not np.isfinite(sw) or sw <= 0: return np.nan
    m = np.sum(w * x) / sw
    var = np.sum(w * (x - m)**2) / sw
    n_eff = (sw**2) / np.sum(w**2) if np.sum(w**2) > 0 else 0
    if ddof and n_eff > ddof:
        var *= n_eff / (n_eff - ddof)
    return float(np.sqrt(max(var, 0.0)))

@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    return s0, s1

def validate_inputs(df_score: pd.DataFrame, horses: pd.DataFrame):
    problems = []
    req = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
    for c in req:
        if c not in df_score.columns:
            problems.append(f"sheet0 å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {c}")
    if 'æ–¤é‡' in horses:
        bad = horses['æ–¤é‡'].dropna()
        if len(bad)>0 and ((bad<45)|(bad>65)).any():
            problems.append("sheet1 æ–¤é‡ãŒãƒ¬ãƒ³ã‚¸å¤–ï¼ˆ45â€“65ï¼‰")
    if {'é€šé4è§’','é ­æ•°'}.issubset(df_score.columns):
        tmp = df_score[['é€šé4è§’','é ­æ•°']].dropna()
        if len(tmp)>0 and ((tmp['é€šé4è§’']<1) | (tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
            problems.append("sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–")
    if problems: st.warning("âš  å…¥åŠ›ãƒã‚§ãƒƒã‚¯ï¼š\n- " + "\n- ".join(problems))

# ===== å³/å·¦å›ã‚Šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
DEFAULT_VENUE_TURN = {
    'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦',
    'ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'
}
VENUE_SYNONYM_TO_VENUE = {'åºœä¸­':'æ±äº¬','æ·€':'äº¬éƒ½'}

def _normalize_turn_table(df: pd.DataFrame) -> pd.DataFrame:
    cols = { 'å ´å':None, 'ç«¶èµ°å':None, 'å›ã‚Š':None, 'æ­£è¦è¡¨ç¾':None }
    for c in df.columns:
        s = str(c).strip()
        if s in ['å ´å','ç«¶é¦¬å ´','ã‚³ãƒ¼ã‚¹','é–‹å‚¬','é–‹å‚¬å ´']: cols['å ´å']=c
        if s in ['ç«¶èµ°å','ãƒ¬ãƒ¼ã‚¹å','åç§°']:              cols['ç«¶èµ°å']=c
        if s in ['å›ã‚Š','å³å·¦','å‘ã']:                    cols['å›ã‚Š']=c
        if s.lower() in ['regex','æ­£è¦è¡¨ç¾','æ­£è¦è¡¨ç¾?']:   cols['æ­£è¦è¡¨ç¾']=c
    out = pd.DataFrame()
    if cols['å ´å'] is not None:    out['å ´å'] = df[cols['å ´å']].astype(str).str.strip()
    if cols['ç«¶èµ°å'] is not None:  out['ç«¶èµ°å'] = df[cols['ç«¶èµ°å']].astype(str).str.strip()
    if cols['å›ã‚Š'] is not None:
        out['å›ã‚Š'] = (df[cols['å›ã‚Š']].astype(str)
                       .str.replace('å›ã‚Š','').str.replace('å‘¨ã‚Š','').str.strip().str[:1])
    else:
        out['å›ã‚Š'] = None
    out['æ­£è¦è¡¨ç¾'] = (df[cols['æ­£è¦è¡¨ç¾']] if cols['æ­£è¦è¡¨ç¾'] is not None else False).astype(bool)
    out = out[(out['å›ã‚Š'].isin(['å³','å·¦']))]
    return out

def _infer_venue_from_racename(name: str) -> str | None:
    s = str(name)
    for v in DEFAULT_VENUE_TURN.keys():
        if v in s: return v
    for syn, venue in VENUE_SYNONYM_TO_VENUE.items():
        if syn in s: return venue
    return None

def _attach_turn_to_scores(df_score: pd.DataFrame,
                           turn_df: pd.DataFrame | None,
                           use_default: bool=True) -> pd.DataFrame:
    df = df_score.copy()
    if 'å ´å' in df.columns:
        df['_å ´åæ¨å®š'] = df['å ´å'].astype(str).apply(_infer_venue_from_racename)
    else:
        df['_å ´åæ¨å®š'] = df['ç«¶èµ°å'].astype(str).apply(_infer_venue_from_racename)
    venue_map = {}
    if use_default:
        venue_map.update(DEFAULT_VENUE_TURN)
    if turn_df is not None and 'å ´å' in turn_df.columns:
        for v, t in turn_df[['å ´å','å›ã‚Š']].dropna().values:
            if str(v).strip():
                venue_map[str(v).strip()] = t
    df['å›ã‚Š'] = df['_å ´åæ¨å®š'].map(venue_map)
    if turn_df is not None and 'ç«¶èµ°å' in turn_df.columns:
        patt = turn_df.dropna(subset=['ç«¶èµ°å'])
        for _, row in patt.iterrows():
            pat = str(row['ç«¶èµ°å']).strip()
            trn = row['å›ã‚Š']
            is_re = bool(row.get('æ­£è¦è¡¨ç¾', False))
            mask = (df['ç«¶èµ°å'].astype(str).str.contains(pat, regex=is_re, na=False)
                    if is_re else
                    df['ç«¶èµ°å'].astype(str).str.contains(re.escape(pat), regex=True, na=False))
            df.loc[mask, 'å›ã‚Š'] = trn
    return df

# ======================== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ========================
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®š")

with st.sidebar.expander("ğŸ”° ã‚ˆãä½¿ã†ï¼ˆåŸºæœ¬ï¼‰", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    besttime_w   = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿", 0.0, 2.0, 1.0)
    with st.expander("æˆ¦ç¸¾ç‡ã®é‡ã¿ï¼ˆå½“è©²é¦¬å ´ï¼‰", expanded=False):
        win_w  = st.slider("å‹ç‡ã®é‡ã¿",   0.0, 5.0, 1.0, 0.1, key="w_win")
        quin_w = st.slider("é€£å¯¾ç‡ã®é‡ã¿", 0.0, 5.0, 0.7, 0.1, key="w_quin")
        plc_w  = st.slider("è¤‡å‹ç‡ã®é‡ã¿", 0.0, 5.0, 0.5, 0.1, key="w_plc")
    with st.expander("å„ç¨®ãƒœãƒ¼ãƒŠã‚¹è¨­å®š", expanded=False):
        grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
        agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
        agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
        agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)
        bw_bonus     = st.slider("é¦¬ä½“é‡é©æ­£ãƒœãƒ¼ãƒŠã‚¹(Â±10kg)", 0, 10, 2)
    with st.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆBTé‡ã¿ç”¨ï¼‰", expanded=True):
        TARGET_GRADE    = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", ["G1", "G2", "G3", "L", "OP"], index=4)
        TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ", "ãƒ€"], index=0)
        TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", min_value=1000, max_value=3600, value=1800, step=100)

with st.sidebar.expander("ğŸ›  è©³ç´°ï¼ˆè£œæ­£/è„šè³ª/ãƒšãƒ¼ã‚¹ï¼‰", expanded=False):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)

    with st.expander("æ–¤é‡ãƒ™ãƒ¼ã‚¹ï¼ˆWFAç°¡ç•¥ï¼‰", expanded=False):
        race_date = pd.to_datetime(st.date_input("é–‹å‚¬æ—¥", value=pd.Timestamp.today().date()))
        use_wfa_base = st.checkbox("WFAåŸºæº–ã‚’ä½¿ã†ï¼ˆæ¨å¥¨ï¼‰", value=True)
        wfa_2_early_m = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 55.0, 0.5)
        wfa_2_early_f = st.number_input("2æ­³ï¼ˆã€œ9æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 54.0, 0.5)
        wfa_2_late_m  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰¡/ã›ã‚“ [kg]", 50.0, 60.0, 56.0, 0.5)
        wfa_2_late_f  = st.number_input("2æ­³ï¼ˆ10-12æœˆï¼‰ ç‰ [kg]"    , 48.0, 60.0, 55.0, 0.5)
        wfa_3p_m      = st.number_input("3æ­³ä»¥ä¸Š ç‰¡/ã›ã‚“ [kg]" , 50.0, 62.0, 57.0, 0.5)
        wfa_3p_f      = st.number_input("3æ­³ä»¥ä¸Š ç‰ [kg]"     , 48.0, 60.0, 55.0, 0.5)

    with st.expander("å±æ€§é‡ã¿ï¼ˆ1èµ°ã‚¹ã‚³ã‚¢ä¿‚æ•°ï¼‰", expanded=False):
        gender_w = {g: st.slider(f"{g}", 0.0, 2.0, 1.0) for g in ['ç‰¡','ç‰','ã‚»']}
        style_w  = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in STYLES}
        season_w = {s: st.slider(f"{s}", 0.0, 2.0, 1.0) for s in ['æ˜¥','å¤','ç§‹','å†¬']}
        age_w    = {str(age): st.slider(f"{age}æ­³", 0.0, 2.0, 1.0, 0.05) for age in range(3, 11)}
        frame_w  = {str(i): st.slider(f"{i}æ ", 0.0, 2.0, 1.0) for i in range(1,8+1)}

    with st.expander("è„šè³ªè‡ªå‹•æ¨å®š / ãƒšãƒ¼ã‚¹æ¨å®š", expanded=False):
        auto_style_on   = st.checkbox("è„šè³ªã®è‡ªå‹•æ¨å®šã‚’ä½¿ã†ï¼ˆç©ºæ¬„ã‚’åŸ‹ã‚ã‚‹ï¼‰", True)
        AUTO_OVERWRITE  = st.checkbox("æ‰‹å…¥åŠ›ã‚ˆã‚Šè‡ªå‹•ã‚’å„ªå…ˆã—ã¦ä¸Šæ›¸ã", False)
        NRECENT         = st.slider("ç›´è¿‘ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆè„šè³ªæ¨å®šï¼‰", 1, 10, 5)
        HL_DAYS_STYLE   = st.slider("åŠæ¸›æœŸï¼ˆæ—¥ãƒ»è„šè³ªç”¨ï¼‰", 30, 365, 180, 15)
        pace_mc_draws   = st.slider("ãƒšãƒ¼ã‚¹MCå›æ•°", 500, 30000, 5000, 500)
        pace_mode = st.radio("ãƒšãƒ¼ã‚¹ã®æ‰±ã„", ["è‡ªå‹•ï¼ˆMCï¼‰", "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰"], index=0)
        pace_fixed = st.selectbox("å›ºå®šãƒšãƒ¼ã‚¹", ["ãƒã‚¤ãƒšãƒ¼ã‚¹","ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹","ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹","ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"],
                                  index=1, disabled=(pace_mode=="è‡ªå‹•ï¼ˆMCï¼‰"))
        epi_alpha = st.slider("é€ƒã’ä¿‚æ•° Î±", 0.0, 2.0, 1.0, 0.05)
        epi_beta  = st.slider("å…ˆè¡Œä¿‚æ•° Î²", 0.0, 2.0, 0.60, 0.05)
        thr_hi    = st.slider("é–¾å€¤: ãƒã‚¤ãƒšãƒ¼ã‚¹ â‰¥", 0.30, 1.00, 0.52, 0.01)
        thr_mid   = st.slider("é–¾å€¤: ãƒŸãƒ‰ãƒ« â‰¥",    0.10, 0.99, 0.30, 0.01)
        thr_slow  = st.slider("é–¾å€¤: ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ â‰¥",0.00, 0.98, 0.18, 0.01)

# === NEW: å›ã‚Šï¼ˆå³/å·¦ï¼‰è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ ===
with st.sidebar.expander("ğŸ”„ å›ã‚Šï¼ˆå³/å·¦ï¼‰", expanded=False):
    TARGET_TURN = st.radio("æœ¬ãƒ¬ãƒ¼ã‚¹ã®å›ã‚Š", ["å³","å·¦"], index=0, horizontal=True)
    turn_gain   = st.slider("å›ã‚Šé©æ€§ ä¿‚æ•°ï¼ˆFinalRawã¸åŠ ç‚¹ï¼‰", 0.0, 3.0, 1.0, 0.1)
    turn_gap_thr= st.slider("å¾—æ„åˆ¤å®šã®é–¾å€¤ï¼ˆRightZâˆ’LeftZ ã®æœ€å°å·®ï¼‰", 0.0, 10.0, 1.0, 0.1)
    use_default_venue_map = st.checkbox("JRAæ¨™æº–ã®ã€å ´åâ†’å›ã‚Šã€ã§è£œå®Œã™ã‚‹", True)
    st.caption("â€» å ´åæ—¢å®šè¡¨ï¼‹ç«¶èµ°åã‹ã‚‰è‡ªå‹•æ¨å®šã€‚")

# === NEW: ãƒãƒ³ãƒ‰æ ¡æ­£ï¼ˆBä¸­å¿ƒåŒ–ï¼‰ ===
with st.sidebar.expander("ğŸ· ãƒãƒ³ãƒ‰æ ¡æ­£ï¼ˆBä¸­å¿ƒåŒ–ï¼‰", expanded=True):
    band_mid_target = st.slider("ä¸­å¤®å€¤â†’ä½•ç‚¹ã«åˆã‚ã›ã‚‹ï¼Ÿ", 40, 80, 65, 1,
                                help="AR100ã§ãƒ¬ãƒ¼ã‚¹ã®çœŸã‚“ä¸­ã‚’ä½•ç‚¹ã«ç½®ãã‹ï¼ˆBã®çœŸã‚“ä¸­=65æ¨å¥¨ï¼‰")
    band_A_share = st.slider("Aä»¥ä¸Šã®ç›®æ¨™å‰²åˆ(%)", 1, 60, 25, 1,
                             help="AR100ãŒ70ä»¥ä¸Šï¼ˆA, S, SSï¼‰ã«ãªã‚‹é ­æ•°ã®å‰²åˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€‚Bã‚’åšãã—ãŸã„ãªã‚‰å°ã•ã‚ã«ã€‚")
    band_clip_lo = st.slider("ä¸‹é™ã‚¯ãƒªãƒƒãƒ—", 0, 60, 40, 1)
    band_clip_hi = st.slider("ä¸Šé™ã‚¯ãƒªãƒƒãƒ—", 80, 100, 100, 1)

# === è¡¨ç¤ºè¨­å®š ===
with st.sidebar.expander("ğŸ–¥ è¡¨ç¤ºè¨­å®š", expanded=True):
    FULL_TABLE_VIEW = st.checkbox("è¡¨ã¯å…¨é ­è¡¨ç¤ºï¼ˆå†…éƒ¨ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ç„¡ãã™ï¼‰", value=True)
    MAX_TABLE_HEIGHT = st.slider("å…¨é ­è¡¨ç¤ºã®æœ€å¤§é«˜ã•(px)", 800, 10000, 5000, 200)

def auto_table_height(n_rows: int, row_px: int = 35, header_px: int = 38, pad_px: int = 28) -> int:
    h = int(header_px + row_px * max(1, int(n_rows)) + pad_px)
    return min(h, int(MAX_TABLE_HEIGHT))

def H(df, default_px: int) -> int:
    try:
        return auto_table_height(len(df)) if FULL_TABLE_VIEW else int(default_px)
    except Exception:
        return int(default_px)

# === ä¾¿åˆ©ãƒªã‚»ãƒƒãƒˆ ===
with st.sidebar.expander("ğŸ§¹ ãƒˆãƒ©ãƒ–ãƒ«æ™‚ã®ãƒªã‚»ãƒƒãƒˆ", expanded=False):
    if st.button("åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        for key in list(st.session_state.keys()):
            if key.startswith("s0:") or key.startswith("map:s0:") or key.startswith("s1:") or key.startswith("map:s1:"):
                del st.session_state[key]
        st.success("åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚Excelã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")

with st.sidebar.expander("ğŸ§ª ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ / ä¿å­˜", expanded=False):
    mc_iters   = st.slider("å‹ç‡MC åå¾©å›æ•°", 1000, 100000, 20000, 1000)
    mc_beta    = st.slider("å¼·ã•â†’å‹ç‡ æ¸©åº¦Î²", 0.1, 5.0, 1.5, 0.1)
    mc_tau     = st.slider("å®‰å®šåº¦ãƒã‚¤ã‚ºä¿‚æ•° Ï„", 0.0, 2.0, 0.6, 0.05)
    mc_seed    = st.number_input("ä¹±æ•°Seed", 0, 999999, 42, 1)
    st.markdown("---")
    total_budget = st.slider("åˆè¨ˆäºˆç®—", 500, 50000, 10000, 100)
    min_unit     = st.selectbox("æœ€å°è³­ã‘å˜ä½", [100, 200, 300, 500], index=0)
    max_lines    = st.slider("æœ€å¤§ç‚¹æ•°(é€£ç³»)", 1, 60, 20, 1)
    scenario     = st.selectbox("ã‚·ãƒŠãƒªã‚ª", ['é€šå¸¸','ã¡ã‚‡ã„ä½™è£•','ä½™è£•'])
    st.markdown("---")

    # ---- JSONä¿å­˜ã‚’å®‰å…¨åŒ–ï¼ˆéã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå“ã‚’æ–‡å­—åˆ—åŒ–ï¼‰ ----
    def _jsonable(x):
        import numpy as _np
        if isinstance(x, (str, int, float, bool)) or x is None:
            return x
        if isinstance(x, (_np.integer,)):
            return int(x)
        if isinstance(x, (_np.floating,)):
            return float(x)
        if isinstance(x, (_np.bool_,)):
            return bool(x)
        if isinstance(x, (list, tuple, set)):
            return [_jsonable(i) for i in x]
        if isinstance(x, dict):
            return {str(k): _jsonable(v) for k, v in x.items()}
        return str(x)

    col_a, col_b = st.columns(2)
    if col_a.button("è¨­å®šã‚’ä¿å­˜"):
        try:
            cfg_dict = {}
            for k, v in st.session_state.items():
                if str(k).startswith('_'):
                    continue
                if k in ('excel_up','cfg_up','pedi_html_up'):
                    continue
                cfg_dict[k] = _jsonable(v)
            cfg = json.dumps(cfg_dict, ensure_ascii=False, indent=2)
            st.download_button("JSONã‚’DL", data=cfg, file_name="keiba_config.json", mime="application/json")
        except Exception as e:
            st.error(f"è¨­å®šä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    cfg_file = col_b.file_uploader("è¨­å®šèª­ã¿è¾¼ã¿", type=["json"], key="cfg_up")
    if cfg_file is not None:
        try:
            cfg = json.loads(cfg_file.read().decode("utf-8"))
            for k,v in cfg.items():
                st.session_state[k]=v
            st.success("è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆå¿…è¦ãªã‚‰å†å®Ÿè¡Œï¼‰ã€‚")
        except Exception as e:
            st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ======================== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ========================
st.title("ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆä¿®æ­£ç‰ˆï¼‰")
st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
excel_file = st.file_uploader("Excelï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

sheet0, sheet1 = load_excel_bytes(excel_file.getvalue())

# === sheet0 / sheet1 ãƒãƒƒãƒ”ãƒ³ã‚° ===
def _norm_col(s: str) -> str:
    s = str(s).strip()
    s = re.sub(r'\s+', '', s)
    s = s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789'))
    s = s.replace('ï¼ˆ','(').replace('ï¼‰',')').replace('ï¼…','%')
    return s
def _auto_guess(col_map, pats):
    for orig, normed in col_map.items():
        for p in pats:
            if re.search(p, normed, flags=re.I):
                return orig
    return None

def _interactive_map(df, patterns, required_keys, title, state_key, show_ui=False):
    cols = list(df.columns)
    cmap = {c: _norm_col(c) for c in cols}

    auto = {k: (st.session_state.get(f"{state_key}:{k}") or _auto_guess(cmap, pats))
            for k, pats in patterns.items()}

    for k, v in list(auto.items()):
        if v not in cols:
            auto[k] = None

    if not show_ui:
        missing = [k for k in required_keys if not auto.get(k)]
        if not missing:
            for k, v in auto.items():
                if v:
                    st.session_state[f"{state_key}:{k}"] = v
            return auto
        else:
            st.warning(f"{title} ã®å¿…é ˆåˆ—ãŒè‡ªå‹•èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ: " + ", ".join(missing))
            show_ui = True

    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=True):
        mapping = {}
        for key, pats in patterns.items():
            default = st.session_state.get(f"{state_key}:{key}") or auto.get(key)
            options = ['<æœªé¸æŠ>'] + cols
            idx = options.index(default) if (default in cols) else 0
            mapping[key] = st.selectbox(key, options=options, index=idx, key=f"map:{state_key}:{key}")
            if mapping[key] != '<æœªé¸æŠ>':
                st.session_state[f"{state_key}:{key}"] = mapping[key]

    missing = [k for k in required_keys if mapping.get(k) in (None, '<æœªé¸æŠ>')]
    if missing:
        st.stop()

    return {k: (None if v=='<æœªé¸æŠ>' else v) for k, v in mapping.items()}

# ãƒ‘ã‚¿ãƒ¼ãƒ³
PAT_S0 = {
    'é¦¬å'         : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥'     : [r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜(?!S)|å¹´æœˆæ—¥|æ–½è¡Œæ—¥|é–‹å‚¬æ—¥'],
    'ç«¶èµ°å'       : [r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å'     : [r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°'         : [r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †'     : [r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ '           : [r'æ \b|æ ç•ª'],
    'ç•ª'           : [r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡'         : [r'æ–¤é‡'],
    'é¦¬ä½“é‡'       : [r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ': [r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3Fã‚¿ã‚¤ãƒ |ä¸Š3F|Ave-?3F'],
    'ä¸Š3Fé †ä½'     : [r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’'      : [r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼.*|4è§’é †ä½|4è§’é€šéé †'],
    'æ€§åˆ¥'         : [r'æ€§åˆ¥'],
    'å¹´é½¢'         : [r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' : [r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢'         : [r'è·é›¢'],
    'é¦¬å ´'         : [r'é¦¬å ´(?!.*æŒ‡æ•°)|é¦¬å ´çŠ¶æ…‹'],
    'å ´å'         : [r'å ´å|å ´æ‰€|ç«¶é¦¬å ´|é–‹å‚¬(åœ°|å ´|å ´æ‰€)'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']
MAP_S0 = _interactive_map(sheet0, PAT_S0, REQ_S0, "sheet0ï¼ˆéå»èµ°ï¼‰", "s0", show_ui=False)

df_score = pd.DataFrame()
for k, col in MAP_S0.items():
    if (col is None) or (col not in sheet0.columns):
        continue
    df_score[k] = sheet0[col]

df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(df_score['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
for c in ['é ­æ•°','ç¢ºå®šç€é †','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½','é€šé4è§’','è·é›¢']:
    if c in df_score: df_score[c] = pd.to_numeric(df_score[c], errors='coerce')
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' in df_score: df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = df_score['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].apply(_parse_time_to_sec)
if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in df_score: df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = df_score['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# é€šé4è§’/é ­æ•°ã®æ•´å½¢
if 'é ­æ•°' in df_score.columns:
    df_score['é ­æ•°'] = (
        df_score['é ­æ•°'].astype(str).str.extract(r'(\d+)')[0]
        .apply(pd.to_numeric, errors='coerce')
    )
if 'é€šé4è§’' in df_score.columns:
    s = df_score['é€šé4è§’']
    if s.dtype.kind not in 'iu':
        last_num = s.astype(str).str.extract(r'(\d+)(?!.*\d)')[0]
        df_score['é€šé4è§’'] = pd.to_numeric(last_num, errors='coerce')
    ok = df_score['é ­æ•°'].notna() & df_score['é€šé4è§’'].notna()
    bad = ok & ((df_score['é€šé4è§’'] < 1) | (df_score['é€šé4è§’'] > df_score['é ­æ•°']))
    df_score.loc[df_score['é€šé4è§’'].eq(0), 'é€šé4è§’'] = np.nan
    df_score.loc[bad, 'é€šé4è§’'] = np.nan

# === sheet1ï¼ˆå½“æ—¥å‡ºèµ°è¡¨ï¼‰ ===
PAT_S1 = {
    'é¦¬å'   : [r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ '     : [r'æ \b|æ ç•ª'],
    'ç•ª'     : [r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥'   : [r'æ€§åˆ¥'],
    'å¹´é½¢'   : [r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡'   : [r'æ–¤é‡'],
    'é¦¬ä½“é‡' : [r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª'   : [r'è„šè³ª'],
    'å‹ç‡'   : [r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡' : [r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡' : [r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ': [r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1 = ['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢']
MAP_S1 = _interactive_map(sheet1, PAT_S1, REQ_S1, "sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰", "s1", show_ui=False)

attrs = pd.DataFrame()
for k, col in MAP_S1.items():
    if (col is None) or (col not in sheet1.columns):
        continue
    attrs[k] = sheet1[col]
for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in attrs: attrs[c] = pd.to_numeric(attrs[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in attrs: attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] = attrs['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# â˜…â˜…â˜… å…ˆé ­ã®ç©ºè¡Œï¼ˆå…¨åˆ—NaNï¼‰ã‚„é¦¬åãŒç©ºã®è¡Œã‚’é™¤å»ã—ã¦ã‹ã‚‰ã‚¨ãƒ‡ã‚£ã‚¿ã¸ â˜…â˜…â˜…
attrs = attrs.replace(r'^\s*$', np.nan, regex=True)   # ç©ºæ–‡å­—â†’NaN
attrs = attrs.dropna(how='all')                       # å…¨åˆ—NaNè¡Œã‚’å‰Šé™¤
if 'é¦¬å' in attrs.columns:
    attrs['é¦¬å'] = attrs['é¦¬å'].astype(str).str.replace('\u3000',' ').str.strip()
    attrs = attrs[attrs['é¦¬å'].ne('')]
attrs = attrs.reset_index(drop=True)

# å…¥åŠ›UIï¼ˆè„šè³ªãƒ»æ–¤é‡ãƒ»é¦¬ä½“é‡ç·¨é›†ï¼‰
if 'è„šè³ª' not in attrs.columns: attrs['è„šè³ª'] = ''
if 'æ–¤é‡' not in attrs.columns: attrs['æ–¤é‡'] = np.nan
if 'é¦¬ä½“é‡' not in attrs.columns: attrs['é¦¬ä½“é‡'] = np.nan

# === NEW: ã‚¨ãƒ‡ã‚£ã‚¿ã®å†…å®¹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å¾©å…ƒï¼ˆãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰ ===
if 'horses_df' in st.session_state and isinstance(st.session_state['horses_df'], pd.DataFrame) and not st.session_state['horses_df'].empty:
    prev = st.session_state['horses_df'][['é¦¬å','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy()
    attrs = attrs.merge(prev, on='é¦¬å', how='left', suffixes=('','_prev'))
    for c in ['è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']:
        attrs[c] = attrs[c].where(attrs[c].notna() & (attrs[c] != ''), attrs.get(f'{c}_prev'))
    drop_cols = [f'{c}_prev' for c in ['è„šè³ª','æ–¤é‡','é¦¬ä½“é‡'] if f'{c}_prev' in attrs.columns]
    if drop_cols:
        attrs.drop(columns=drop_cols, inplace=True)

st.subheader("é¦¬ä¸€è¦§ãƒ»è„šè³ªãƒ»æ–¤é‡ãƒ»å½“æ—¥é¦¬ä½“é‡å…¥åŠ›")
edited = st.data_editor(
    attrs[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
    column_config={
        'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
        'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
        'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1)
    },
    use_container_width=True,
    num_rows='static',
    height=H(attrs, 420),
    hide_index=True,  # â˜… è¡Œç•ªå·ã‚‚éè¡¨ç¤ºã«ï¼ˆä»»æ„ï¼‰
    key="horses_editor",  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿æŒã®ãŸã‚ key ã‚’ä»˜ä¸
)
horses = edited.copy()

# â˜… å…¥åŠ›ç›´å¾Œã«è„šè³ªã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ã—ã¦ãŠã
if 'è„šè³ª' in horses.columns:
    horses['è„šè³ª'] = horses['è„šè³ª'].map(normalize_style)

# â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¸ä¿å­˜ï¼ˆã‚¿ãƒ–ç§»å‹•ã‚„ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼æ“ä½œã§æ¶ˆãˆãªã„ï¼‰
st.session_state['horses_df'] = horses.copy()

validate_inputs(df_score, horses)

# --- è„šè³ª è‡ªå‹•æ¨å®š ---
df_style = pd.DataFrame({'é¦¬å': [], 'p_é€ƒã’': [], 'p_å…ˆè¡Œ': [], 'p_å·®ã—': [], 'p_è¿½è¾¼': [], 'æ¨å®šè„šè³ª': []})
need_cols = {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
if auto_style_on and need_cols.issubset(df_score.columns):
    tmp = (
        df_score[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’','ä¸Š3Fé †ä½']].copy()
        .dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'])
        .sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    )
    tmp['_rn'] = tmp.groupby('é¦¬å').cumcount()+1
    tmp = tmp[tmp['_rn'] <= int(NRECENT)].copy()
    today = pd.Timestamp.today()
    tmp['_days'] = (today - pd.to_datetime(tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    tmp['_w'] = 0.5 ** (tmp['_days'] / float(HL_DAYS_STYLE))

    denom = (pd.to_numeric(tmp['é ­æ•°'], errors='coerce') - 1).replace(0, np.nan)
    pos_ratio = (pd.to_numeric(tmp['é€šé4è§’'], errors='coerce') - 1) / denom
    pos_ratio = pos_ratio.clip(0, 1).fillna(0.5)

    if 'ä¸Š3Fé †ä½' in tmp.columns:
        ag = pd.to_numeric(tmp['ä¸Š3Fé †ä½'], errors='coerce')
        close_strength = ((3.5 - ag) / 3.5).clip(lower=0, upper=1).fillna(0.0)
    else:
        close_strength = pd.Series(0.0, index=tmp.index)

    b_nige, b_sengo, b_sashi, b_oikomi = -1.2, 0.6, 0.3, -0.7
    tmp['L_nige']   = b_nige  + 1.6*(1 - pos_ratio) - 1.2*close_strength
    tmp['L_sengo']  = b_sengo + 1.1*(1 - pos_ratio) - 0.1*close_strength
    tmp['L_sashi']  = b_sashi + 1.1*(pos_ratio)     + 0.9*close_strength
    tmp['L_oikomi'] = b_oikomi+ 1.6*(pos_ratio)     + 0.5*close_strength

    rows = []
    for name, g in tmp.groupby('é¦¬å'):
        w  = g['_w'].to_numpy(); sw = w.sum()
        if sw <= 0: continue
        def wavg(v): return float((v*w).sum()/sw)
        vec = np.array([wavg(g['L_nige']), wavg(g['L_sengo']), wavg(g['L_sashi']), wavg(g['L_oikomi'])], dtype=float)
        vec = vec - vec.max()
        p = np.exp(vec); p = p / p.sum()
        pred = STYLES[int(np.argmax(p))]

        pr = (pd.to_numeric(g['é€šé4è§’'], errors='coerce') - 1) / (pd.to_numeric(g['é ­æ•°'], errors='coerce') - 1)
        pr = pr.clip(0,1).fillna(0.5)
        wpr = float((pr*w).sum()/sw)
        if pred == 'é€ƒã’' and not (wpr <= 0.22 or ((pr <= 0.15)*w).sum()/sw >= 0.25): pred='å…ˆè¡Œ'
        if pred == 'è¿½è¾¼' and not (wpr >= 0.78 or ((pr >= 0.85)*w).sum()/sw >= 0.25): pred='å·®ã—'

        rows.append([name, *p.tolist(), pred])
    if rows:
        df_style = pd.DataFrame(rows, columns=['é¦¬å','p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼','æ¨å®šè„šè³ª'])
        manual_has_nige = ('è„šè³ª' in horses.columns) and horses['è„šè³ª'].eq('é€ƒã’').any()
        if (df_style['æ¨å®šè„šè³ª'].eq('é€ƒã’').sum() == 0) and (not manual_has_nige):
            early = tmp.assign(early=(1 - pos_ratio).clip(0, 1), w=tmp['_w'].values)\
                      .groupby('é¦¬å').apply(lambda g: float((g['early']*g['w']).sum()/g['w'].sum()))
            nige_cand = early.idxmax()
            df_style.loc[df_style['é¦¬å'] == nige_cand, 'æ¨å®šè„šè³ª'] = 'é€ƒã’'

# --- æˆ¦ç¸¾ç‡ãƒ»ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ  ---
rate_cols = [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡'] if c in attrs.columns]
if rate_cols:
    rate = attrs[['é¦¬å'] + rate_cols].copy()
    for c in rate_cols:
        rate[c] = rate[c].astype(str).str.replace('%','', regex=False).str.replace('ï¼…','', regex=False)
        rate[c] = pd.to_numeric(rate[c], errors='coerce')
    mx = pd.concat([rate[c] for c in rate_cols], axis=1).max().max()
    if pd.notna(mx) and mx <= 1.0:
        for c in rate_cols: rate[c] *= 100.0
    if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in attrs:
        rate = rate.merge(attrs[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
else:
    rate = pd.DataFrame({'é¦¬å':[]})

# === éå»èµ°ã‹ã‚‰ã€Œé©æ­£é¦¬ä½“é‡ã€æ¨å®š ===
best_bw_map = {}
if {'é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †'}.issubset(df_score.columns):
    _bw = df_score[['é¦¬å','é¦¬ä½“é‡','ç¢ºå®šç€é †']].dropna()
    _bw['ç¢ºå®šç€é †'] = pd.to_numeric(_bw['ç¢ºå®šç€é †'], errors='coerce')
    _bw = _bw[_bw['ç¢ºå®šç€é †'].notna()]
    try:
        best_idx = _bw.groupby('é¦¬å')['ç¢ºå®šç€é †'].idxmin()
        best_bw_map = _bw.loc[best_idx].set_index('é¦¬å')['é¦¬ä½“é‡'].astype(float).to_dict()
    except Exception:
        best_bw_map = {}

# é‡è¤‡ã‚¬ãƒ¼ãƒ‰
try:
    horses.drop_duplicates('é¦¬å', keep='first', inplace=True)
except Exception:
    pass
try:
    df_score = df_score.drop_duplicates(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å'], keep='first')
except Exception:
    pass

# ===== ãƒãƒ¼ã‚¸ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    df_score.drop(columns=[dup], errors='ignore', inplace=True)
df_score = df_score.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')
if len(rate) > 0:
    use_cols = ['é¦¬å'] + [c for c in ['å‹ç‡','é€£å¯¾ç‡','è¤‡å‹ç‡','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'] if c in rate.columns]
    df_score = df_score.merge(rate[use_cols], on='é¦¬å', how='left')

# ===== ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ =====
bt_min = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_max = df_score['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True) if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in df_score else np.nan
bt_span = (bt_max - bt_min) if pd.notna(bt_min) and pd.notna(bt_max) and (bt_max > bt_min) else 1.0
CLASS_BASE_BT = {"OP": 1.50, "L": 1.38, "G3": 1.19, "G2": 1.00, "G1": 0.80}
def besttime_weight_final(grade: str, surface: str, distance_m: int, user_scale: float) -> float:
    base = CLASS_BASE_BT.get(str(grade), CLASS_BASE_BT["OP"])
    s = 1.10 if str(surface) == "ãƒ€" else 1.00
    try:
        d = int(distance_m)
        if d <= 1400:           dfac = 1.20
        elif d == 1600:         dfac = 1.10
        elif 1800 <= d <= 2200: dfac = 1.00
        elif d >= 2400:         dfac = 0.85
        else:                   dfac = 1.00
    except: dfac = 1.00
    w = base * s * dfac * float(user_scale)
    return float(np.clip(w, 0.0, 2.0))

CLASS_PTS = {'G1':10, 'G2':8, 'G3':6, 'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5, 'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}
def class_points(row) -> int:
    g = normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row:
        g = normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name = str(row.get('ã‚¯ãƒ©ã‚¹å','')) + ' ' + str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

def wfa_base_for(sex: str, age: int | None, dt: pd.Timestamp) -> float:
    try:
        a = int(age) if age is not None and not pd.isna(age) else None
    except Exception:
        a = None
    m = int(dt.month) if isinstance(dt, pd.Timestamp) else 1
    if a == 2:
        male = wfa_2_early_m if m <= 9 else wfa_2_late_m
        filly = wfa_2_early_f if m <= 9 else wfa_2_late_f
    elif a is not None and a >= 3:
        male, filly = wfa_3p_m, wfa_3p_f
    else:
        male, filly = wfa_3p_m, wfa_3p_f
    return male if sex in ("ç‰¡", "ã‚»") else filly

# ===== è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹ï¼šã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰èª­ã‚€ =====
pedi_bonus_pts: float = float(st.session_state.get('pedi:points', 0.0))
_pedi_map_session = st.session_state.get('pedi:map', {})
pedi_bonus_map = { _trim_name(k): bool(v) for k, v in dict(_pedi_map_session).items() }
def _pedi_bonus_for(name: str) -> float:
    nm = _trim_name(name)
    return float(pedi_bonus_pts) if pedi_bonus_map.get(nm, False) else 0.0

def calc_score(r):
    g = class_points(r)
    raw = g * (r['é ­æ•°'] + 1 - r['ç¢ºå®šç€é †']) + lambda_part * g

    sw  = season_w[season_of(pd.to_datetime(r['ãƒ¬ãƒ¼ã‚¹æ—¥']).month)]
    gw  = gender_w.get(r.get('æ€§åˆ¥'), 1)
    stw = style_w.get(r.get('è„šè³ª'), 1)
    fw  = frame_w.get(str(r.get('æ ')), 1)
    aw  = age_w.get(str(r.get('å¹´é½¢')), 1.0)

    gnorm = normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))
    grade_point = grade_bonus if gnorm in ['G1','G2','G3'] else 0

    agari_bonus = 0
    try:
        ao = int(r.get('ä¸Š3Fé †ä½', np.nan))
        if   ao == 1: agari_bonus = agari1_bonus
        elif ao == 2: agari_bonus = agari2_bonus
        elif ao == 3: agari_bonus = agari3_bonus
    except: pass

    body_bonus = 0
    try:
        name = r['é¦¬å']
        now_bw = float(r.get('é¦¬ä½“é‡', np.nan))
        tekitai = float(best_bw_map.get(name, np.nan))
        if not np.isnan(now_bw) and not np.isnan(tekitai) and abs(now_bw - tekitai) <= 10:
            body_bonus = bw_bonus
    except Exception: pass

    rate_bonus = 0.0
    try:
        if 'å‹ç‡' in r and pd.notna(r.get('å‹ç‡', np.nan)):   rate_bonus += win_w  * (float(r['å‹ç‡'])  / 100.0)
        if 'é€£å¯¾ç‡' in r and pd.notna(r.get('é€£å¯¾ç‡', np.nan)): rate_bonus += quin_w * (float(r['é€£å¯¾ç‡']) / 100.0)
        if 'è¤‡å‹ç‡' in r and pd.notna(r.get('è¤‡å‹ç‡', np.nan)): rate_bonus += plc_w  * (float(r['è¤‡å‹ç‡'])  / 100.0)
    except: pass

    bt_bonus = 0.0
    try:
        if pd.notna(r.get('ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’', np.nan)):
            bt_norm = (bt_max - float(r['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])) / bt_span
            bt_norm = max(0.0, min(1.0, bt_norm))
            bt_w_final = besttime_weight_final(
                grade=TARGET_GRADE, surface=TARGET_SURFACE, distance_m=int(TARGET_DISTANCE), user_scale=besttime_w
            )
            bt_bonus = bt_w_final * bt_norm
    except Exception:
        pass

    kg_pen = 0.0
    try:
        kg = float(r.get('æ–¤é‡', np.nan))
        if not np.isnan(kg):
            base = wfa_base_for(r.get('æ€§åˆ¥',''), int(r.get('å¹´é½¢')) if pd.notna(r.get('å¹´é½¢',np.nan)) else None, race_date) if use_wfa_base else 56.0
            delta = kg - float(base)
            kg_pen = (-max(0.0,  delta) * float(weight_coeff)
                      + 0.5 * max(0.0, -delta) * float(weight_coeff))
    except Exception:
        pass

    pedi_bonus = _pedi_bonus_for(r['é¦¬å'])

    total_bonus = grade_point + agari_bonus + body_bonus + rate_bonus + bt_bonus + kg_pen + pedi_bonus
    return raw * sw * gw * stw * fw * aw + total_bonus

# 1èµ°â†’æ­£è¦åŒ–
if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df_score.columns:
    st.error("ãƒ¬ãƒ¼ã‚¹æ—¥ åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Excelã®1æšç›®ã«å«ã‚ã¦ãã ã•ã„ã€‚")
    st.stop()
df_score['score_raw']  = df_score.apply(calc_score, axis=1)
if df_score['score_raw'].max() == df_score['score_raw'].min():
    df_score['score_norm'] = 50.0
else:
    df_score['score_norm'] = (
        (df_score['score_raw'] - df_score['score_raw'].min()) /
        (df_score['score_raw'].max() - df_score['score_raw'].min()) * 100
    )

# ===== æ™‚ç³»åˆ—åŠ é‡ =====
now = pd.Timestamp.today()
df_score['_days_ago'] = (now - df_score['ãƒ¬ãƒ¼ã‚¹æ—¥']).dt.days
df_score['_w'] = 0.5 ** (df_score['_days_ago'] / (half_life_m * 30.4375)) if half_life_m > 0 else 1.0

# ===== å›ã‚Šæƒ…å ±ã®ä»˜ä¸ =====
turn_df = None
df_score = _attach_turn_to_scores(df_score, turn_df, use_default=use_default_venue_map)

# ===== å³/å·¦å›ã‚Š é©æ€§ã®é›†è¨ˆ =====
g_turn = df_score[['é¦¬å','score_norm','_w','å›ã‚Š']].dropna(subset=['é¦¬å','score_norm','_w'])
g_turn['é¦¬å'] = g_turn['é¦¬å'].map(_trim_name)

def _wavg_row(s: pd.DataFrame) -> float:
    sw = float(pd.to_numeric(s['_w'], errors='coerce').sum())
    if sw <= 0:
        return float('nan')
    num = (pd.to_numeric(s['score_norm'], errors='coerce') * pd.to_numeric(s['_w'], errors='coerce')).sum()
    return float(num / sw)

def _make_weighted(df_sub: pd.DataFrame, col_name: str) -> pd.DataFrame:
    if df_sub.empty:
        return pd.DataFrame(columns=[col_name])
    res = df_sub.groupby('é¦¬å', sort=False).apply(_wavg_row)
    if isinstance(res, pd.Series):
        return res.rename(col_name).to_frame()
    elif isinstance(res, pd.DataFrame):
        if res.shape[1] == 0:
            return pd.DataFrame(columns=[col_name])
        return res.iloc[:, 0].rename(col_name).to_frame()
    else:
        return pd.DataFrame({col_name: pd.Series(res, index=pd.Index([], name='é¦¬å'))})

right = _make_weighted(g_turn[g_turn['å›ã‚Š'].astype(str) == 'å³'], 'RightZ')
left  = _make_weighted(g_turn[g_turn['å›ã‚Š'].astype(str) == 'å·¦'], 'LeftZ')

cnts = (
    g_turn.pivot_table(index='é¦¬å', columns='å›ã‚Š', values='score_norm', aggfunc='count')
         .rename(columns={'å³':'nR','å·¦':'nL'})
) if not g_turn.empty else pd.DataFrame()

turn_pref = pd.concat([right,left,cnts], axis=1).reset_index() \
    if len(right)+len(left)>0 else pd.DataFrame(columns=['é¦¬å','RightZ','LeftZ','nR','nL'])

if len(turn_pref) > 0:
    turn_pref['é¦¬å'] = turn_pref['é¦¬å'].map(_trim_name)

for c in ['RightZ','LeftZ','nR','nL']:
    if c not in turn_pref.columns:
        turn_pref[c] = np.nan

def _pref_label(row):
    R, L = row['RightZ'], row['LeftZ']
    nR, nL = row['nR'], row['nL']
    if pd.notna(R) and pd.notna(L):
        if (R - L) >= turn_gap_thr: return 'å³'
        if (L - R) >= turn_gap_thr: return 'å·¦'
        return 'ä¸­ç«‹'
    if pd.notna(R) and (nR >= 1): return 'å³?'
    if pd.notna(L) and (nL >= 1): return 'å·¦?'
    return 'ä¸æ˜'

turn_pref['TurnPref'] = turn_pref.apply(_pref_label, axis=1) if len(turn_pref)>0 else []
turn_pref['TurnGap']  = (turn_pref['RightZ'].fillna(0) - turn_pref['LeftZ'].fillna(0)) if len(turn_pref)>0 else []

def _pref_pts(row):
    lab = str(row['TurnPref'])
    if TARGET_TURN == 'å³':
        if lab == 'å³':  return 1.0
        if lab == 'å·¦':  return -1.0
        if lab == 'å³?': return 0.5
        if lab == 'å·¦?': return -0.5
        return 0.0
    else:
        if lab == 'å·¦':  return 1.0
        if lab == 'å³':  return -1.0
        if lab == 'å·¦?': return 0.5
        if lab == 'å³?': return -0.5
        return 0.0

if len(turn_pref)>0:
    turn_pref['TurnPrefPts'] = turn_pref.apply(_pref_pts, axis=1)
else:
    turn_pref['TurnPrefPts'] = []

# ===== é¦¬ã”ã¨ã®é›†è¨ˆ =====
def w_mean(x, w):
    x = np.asarray(x, dtype=float); w = np.asarray(w, dtype=float)
    s = w.sum()
    return float((x*w).sum()/s) if s>0 else np.nan

agg = []
for name, g in df_score.groupby('é¦¬å'):
    avg  = g['score_norm'].mean()
    std  = g['score_norm'].std(ddof=0)
    wavg = w_mean(g['score_norm'], g['_w'])
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g) >= 2 else np.nan
    agg.append({'é¦¬å':name,'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg = pd.DataFrame(agg)

# â€”â€” WStd ã®ç©´åŸ‹ã‚
wstd_nontrivial = df_agg.loc[df_agg['Nrun']>=2, 'WStd']
default_std = float(wstd_nontrivial.median()) if (wstd_nontrivial.notna().any()) else 6.0
df_agg['WStd'] = df_agg['WStd'].fillna(default_std)
min_floor = max(1.0, default_std*0.6)
df_agg.loc[df_agg['WStd'] < min_floor, 'WStd'] = min_floor

# ===== è„šè³ªçµ±åˆï¼ˆæ‰‹å…¥åŠ›å„ªå…ˆâ†’è‡ªå‹•ï¼‰ =====
for df in [horses, df_agg]:
    if 'é¦¬å' in df.columns: df['é¦¬å'] = df['é¦¬å'].map(_trim_name)

# â˜… å¿µã®ãŸã‚ã“ã“ã§ã‚‚æ­£è¦åŒ–
if 'è„šè³ª' in horses.columns:
    horses['è„šè³ª'] = horses['è„šè³ª'].map(normalize_style)

name_list = df_agg['é¦¬å'].tolist()
combined_style = pd.Series(index=name_list, dtype=object)
if 'è„šè³ª' in horses.columns:
    combined_style.update(horses.set_index('é¦¬å')['è„šè³ª'])
if not df_style.empty and auto_style_on:
    pred_series = df_style.set_index('é¦¬å')['æ¨å®šè„šè³ª']
    if AUTO_OVERWRITE:
        combined_style.update(pred_series)
    else:
        mask_blank = combined_style.isna() | combined_style.astype(str).str.strip().eq('')
        combined_style.loc[mask_blank] = pred_series.reindex(combined_style.index)[mask_blank]
combined_style = combined_style.fillna('')
df_agg['è„šè³ª'] = df_agg['é¦¬å'].map(combined_style)

# ===== Pè¡Œåˆ—ï¼ˆè„šè³ªç¢ºç‡ï¼‰â†’ ãƒšãƒ¼ã‚¹MC =====
Hn = len(name_list)
P = np.zeros((Hn, 4), dtype=float)
pmap = None
if not df_style.empty and 'é¦¬å' in df_style.columns:
    df_prob = df_style.rename(columns={
        'p_å·®è¾¼': 'p_å·®ã—','p_è¿½ã„è¾¼ã¿': 'p_è¿½è¾¼','p_è¿½è¾¼ã¿': 'p_è¿½è¾¼',
        'p_é€ƒã’ç‡': 'p_é€ƒã’','p_å…ˆè¡Œç‡': 'p_å…ˆè¡Œ','p_å·®ã—ç‡': 'p_å·®ã—','p_è¿½è¾¼ç‡': 'p_è¿½è¾¼',
    })
    need_cols = ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']
    if set(need_cols).issubset(df_prob.columns):
        for c in need_cols:
            df_prob[c] = pd.to_numeric(df_prob[c], errors='coerce').fillna(0.0)
        pmap = (df_prob[['é¦¬å'] + need_cols].set_index('é¦¬å')[need_cols])
        row_sums = pmap.sum(axis=1).replace(0, np.nan)
        pmap = pmap.div(row_sums, axis=0).fillna(0.0)

for i, nm in enumerate(name_list):
    stl = combined_style.get(nm, '')
    if not AUTO_OVERWRITE and (stl in STYLES):
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0; continue
    if pmap is not None and nm in pmap.index:
        P[i, :] = pmap.loc[nm, ['p_é€ƒã’','p_å…ˆè¡Œ','p_å·®ã—','p_è¿½è¾¼']].to_numpy(dtype=float)
        if P[i, :].sum() == 0:
            if stl in STYLES: P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
            else: P[i, :] = np.array([0.25,0.25,0.25,0.25])
    elif stl in STYLES:
        P[i, :] = 0.0; P[i, STYLES.index(stl)] = 1.0
    else:
        P[i, :] = np.array([0.25,0.25,0.25,0.25])

mark_rule = {
    'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
    'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
    'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
}
mark_to_pts = {'â—':2, 'ã€‡':1, 'â—‹':1, 'â–³':0, 'Ã—':-1}

rng_pace = np.random.default_rng(int(mc_seed) + 12345)
sum_pts = np.zeros(Hn, dtype=float)
pace_counter = {'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
for _ in range(int(pace_mc_draws)):
    sampled = [rng_pace.choice(4, p=P[i]) for i in range(Hn)]
    nige  = sum(1 for s in sampled if s==0)
    sengo = sum(1 for s in sampled if s==1)
    epi = (epi_alpha*nige + epi_beta*sengo) / max(1, Hn)
    if   epi >= thr_hi:   pace_t = "ãƒã‚¤ãƒšãƒ¼ã‚¹"
    elif epi >= thr_mid:  pace_t = "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
    elif epi >= thr_slow: pace_t = "ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    else:                 pace_t = "ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹"
    pace_counter[pace_t] += 1
    mk = mark_rule[pace_t]
    for i, s in enumerate(sampled):
        sum_pts[i] += mark_to_pts[ mk[STYLES[s]] ]
df_agg['PacePts'] = sum_pts / max(1, int(pace_mc_draws))
pace_type = max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else "ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹"
if pace_mode == "å›ºå®šï¼ˆæ‰‹å‹•ï¼‰":
    pace_type = pace_fixed
    v_pts = np.array([mark_to_pts[ mark_rule[pace_type][st] ] for st in STYLES], dtype=float)
    df_agg['PacePts'] = (P @ v_pts)

# ===== å›ã‚Šé©æ€§ã‚’ df_agg ã¸ãƒãƒ¼ã‚¸ =====
if len(turn_pref)>0:
    df_agg = df_agg.merge(
        turn_pref[['é¦¬å','RightZ','LeftZ','nR','nL','TurnGap','TurnPref','TurnPrefPts']],
        on='é¦¬å', how='left'
    )
else:
    for c in ['RightZ','LeftZ','nR','nL','TurnGap','TurnPref','TurnPrefPts']:
        df_agg[c] = np.nan
    df_agg['TurnPrefPts'] = df_agg['TurnPrefPts'].fillna(0.0)
df_agg['TurnPrefPts'] = df_agg['TurnPrefPts'].fillna(0.0)

# ===== æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆæ­£è¦åŒ–ã¯å†…éƒ¨æŒ‡æ¨™ï¼‰ =====
df_agg['RecencyZ'] = z_score(df_agg['WAvgZ'])
df_agg['StabZ']    = z_score(-df_agg['WStd'])
df_agg['FinalRaw'] = (
    df_agg['RecencyZ']
    + stab_weight * df_agg['StabZ']
    + pace_gain * df_agg['PacePts']
    + turn_gain * df_agg['TurnPrefPts']
)
df_agg['FinalZ']   = z_score(df_agg['FinalRaw'])

# ===== NEW: AR100ï¼ˆBä¸­å¿ƒåŒ–ã®ç·šå½¢ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ =====
S = df_agg['FinalRaw'].to_numpy(dtype=float)
if np.all(~np.isfinite(S)) or len(S)==0:
    med = 0.0; qa = 1.0
else:
    med = float(np.nanmedian(S))
    # Aä»¥ä¸Šå‰²åˆp: ä¸Šä½(1-p)åˆ†ä½ã‚’Aå¢ƒç•Œ70ç‚¹ã«åˆã‚ã›ã‚‹
    p = max(0.01, min(0.99, 1.0 - float(band_A_share)/100.0))
    qa = float(np.nanquantile(S, p))  # ã“ã“ã‚ˆã‚Šä¸ŠãŒAä»¥ä¸Šã«ã—ãŸã„
denom = (qa - med)
a = (70.0 - float(band_mid_target)) / (denom if abs(denom) > 1e-8 else 1.0)
df_agg['AR100'] = band_mid_target + a * (df_agg['FinalRaw'] - med)
df_agg['AR100'] = df_agg['AR100'].clip(band_clip_lo, band_clip_hi)

def to_band(x):
    try:
        v = float(x)
    except:
        return ''
    if v >= 90: return 'SS'
    if v >= 80: return 'S'
    if v >= 70: return 'A'
    if v >= 60: return 'B'
    if v >= 50: return 'C'
    return 'E'
df_agg['Band'] = df_agg['AR100'].map(to_band)

# ===== å‹ç‡MC =====
S2 = df_agg['FinalRaw'].to_numpy(dtype=float)
S2 = (S2 - np.nanmean(S2)) / (np.nanstd(S2) + 1e-9)
W = df_agg['WStd'].to_numpy(dtype=float)
W = (W - W.min()) / (W.max() - W.min() + 1e-9)
n = len(S2)
rng = np.random.default_rng(int(mc_seed))
gumbel = rng.gumbel(loc=0.0, scale=1.0, size=(mc_iters, n))
noise  = (mc_tau * W)[None, :] * rng.standard_normal((mc_iters, n))
U = mc_beta * S2[None, :] + noise + gumbel
rank_idx = np.argsort(-U, axis=1)
win_counts  = np.bincount(rank_idx[:, 0], minlength=n).astype(float)
top3_counts = np.zeros(n, dtype=float)
for k in range(3):
    top3_counts += np.bincount(rank_idx[:, k], minlength=n).astype(float)
p_win  = win_counts  / mc_iters
p_top3 = top3_counts / mc_iters
df_agg['å‹ç‡%_MC']   = (p_win  * 100).round(2)
df_agg['è¤‡å‹ç‡%_MC'] = (p_top3 * 100).round(2)

# ä¸Šä½æŠ½å‡ºã¨å°ï¼ˆå†…éƒ¨ç”¨ï¼‰
CUTOFF = 50.0
cand = df_agg[df_agg['FinalZ'] >= CUTOFF].sort_values('FinalZ', ascending=False).copy()
topN = cand.head(6).copy()
marks = ['â—','ã€‡','â–²','â˜†','â–³','â–³']
if len(topN)>0: topN['å°'] = marks[:len(topN)]

# ===== æ•£å¸ƒå›³ï¼ˆç¸¦è»¸å …ç‰¢åŒ–ï¼‰ =====
st.markdown("### æ•£å¸ƒå›³ï¼ˆæœ€çµ‚åå·®å€¤ Ã— å®‰å®šåº¦ï¼‰")
if ALT_AVAILABLE and len(df_agg)>0:
    w_all = df_agg['WStd'].replace([np.inf,-np.inf], np.nan).dropna()
    if len(w_all) >= 2:
        q10, q25, q75, q90 = np.percentile(w_all, [10,25,75,90])
        iqr = max(0.1, q75 - q25)
        y_lo = max(0.0, q10 - 0.3*iqr)
        y_hi = q90 + 0.3*iqr
        if (y_hi - y_lo) < 4.0:
            mid = (y_hi + y_lo)/2
            y_lo, y_hi = max(0.0, mid-2.5), mid+2.5
    else:
        y_lo, y_hi = (0.0, 5.0)

    x_mid = 50.0
    y_mid = float(df_agg['WStd'].mean())
    rect = alt.Chart(pd.DataFrame([
        {'x1': df_agg['FinalZ'].min(), 'x2': x_mid, 'y1': y_mid, 'y2': y_hi},
        {'x1': x_mid, 'x2': df_agg['FinalZ'].max(), 'y1': y_mid, 'y2': y_hi},
        {'x1': df_agg['FinalZ'].min(), 'x2': x_mid, 'y1': y_lo, 'y2': y_mid},
        {'x1': x_mid, 'x2': df_agg['FinalZ'].max(), 'y1': y_lo, 'y2': y_mid},
    ])).mark_rect(opacity=0.07).encode(x='x1:Q', x2='x2:Q', y='y1:Q', y2='y2:Q')
    points = alt.Chart(df_agg).mark_circle(size=100).encode(
        x=alt.X('FinalZ:Q', title='æœ€çµ‚åå·®å€¤'),
        y=alt.Y('WStd:Q',  title='åŠ é‡æ¨™æº–åå·®ï¼ˆå°ã•ã„ã»ã©å®‰å®šï¼‰', scale=alt.Scale(domain=(float(y_lo), float(y_hi)))),
        tooltip=['é¦¬å','AR100','Band','WAvgZ','WStd','RecencyZ','StabZ','PacePts','TurnPref','FinalZ','å‹ç‡%_MC']
    )
    labels = alt.Chart(df_agg).mark_text(dx=6, dy=-6, fontSize=10, color='#ffffff').encode(
        x='FinalZ:Q', y='WStd:Q', text='é¦¬å:N'
    )
    vline = alt.Chart(pd.DataFrame({'x':[x_mid]})).mark_rule(color='gray').encode(x='x:Q')
    hline = alt.Chart(pd.DataFrame({'y':[y_mid]})).mark_rule(color='gray').encode(y='y:Q')
    chart = (rect + points + labels + vline + hline).properties(width=700, height=420).interactive()
    st.altair_chart(chart, use_container_width=True)
else:
    st.table(df_agg[['é¦¬å','FinalZ','WStd']].sort_values('FinalZ', ascending=False).head(20))

# ===== horses2ï¼ˆçŸ­è©•ï¼‰ =====
å°map = dict(zip(topN.get('é¦¬å', pd.Series(dtype=str)), topN.get('å°', pd.Series(dtype=str))))
merge_cols = [c for c in ['é¦¬å','WAvgZ','WStd','FinalZ','è„šè³ª','PacePts'] if c in df_agg.columns]
horses2 = horses.merge(df_agg[merge_cols], on='é¦¬å', how='left') if merge_cols else horses.copy()
for col, default in [('å°',''), ('è„šè³ª',''), ('çŸ­è©•',''), ('WAvgZ', np.nan), ('WStd', np.nan), ('FinalZ', np.nan), ('PacePts', np.nan)]:
    if col not in horses2.columns: horses2[col] = default
horses2['å°'] = horses2['é¦¬å'].map(å°map).fillna('')

def ai_comment(row):
    base = ""
    if row['å°'] == 'â—':
        base += "æœ¬å‘½è©•ä¾¡ã€‚" + ("é«˜ã„å®‰å®šæ„Ÿã§ä¿¡é ¼åº¦æŠœç¾¤ã€‚" if pd.notna(row['WStd']) and row['WStd'] <= 8 else "èƒ½åŠ›ä¸Šä½ã‚‚ãƒ ãƒ©ã‚ã‚Šã€‚")
    elif row['å°'] == 'ã€‡':
        base += "å¯¾æŠ—è©•ä¾¡ã€‚" + ("è¿‘èµ°å®‰å®šã—ã¦ãŠã‚Šè»¸å€™è£œã€‚" if pd.notna(row['WStd']) and row['WStd'] <= 10 else "å±•é–‹ã²ã¨ã¤ã§é€†è»¢ã‚‚ã€‚")
    elif row['å°'] in ['â–²','â˜†']:
        base += "ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸€è§’ã€‚" + ("ãƒ ãƒ©ãŒã‚ã‚Šä¸€ç™ºã‚¿ã‚¤ãƒ—ã€‚" if pd.notna(row['WStd']) and row['WStd'] > 15 else "å®‰å®šå‹ã§å …å®Ÿã€‚")
    elif row['å°'] == 'â–³':
        base += "æŠ¼ã•ãˆå€™è£œã€‚" + ("å …å®Ÿã ãŒå‹ã¡åˆ‡ã‚‹ã¾ã§ã¯ï¼Ÿ" if pd.notna(row['WStd']) and row['WStd'] < 12 else "å±•é–‹æ¬¡ç¬¬ã§æµ®ä¸Šã‚‚ã€‚")
    style = str(row.get('è„šè³ª','')).strip()
    base += {
        "é€ƒã’":"ãƒãƒŠã‚’å¥ªãˆã‚Œã°ç²˜ã‚Šè¾¼ã¿ååˆ†ã€‚",
        "å…ˆè¡Œ":"å…ˆè¡ŒåŠ›ã‚’æ´»ã‹ã—ã¦ä¸Šä½äº‰ã„ã€‚",
        "å·®ã—":"å±•é–‹ãŒå‘ã‘ã°æœ«è„šå¼·çƒˆã€‚",
        "è¿½è¾¼":"ç›´ç·šå‹è² ã®ä¸€æ’ƒã«æœŸå¾…ã€‚"
    }.get(style, "")
    return base
try:
    horses2['çŸ­è©•'] = horses2.apply(ai_comment, axis=1)
except Exception:
    if 'çŸ­è©•' not in horses2: horses2['çŸ­è©•'] = ""

# ===== 4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³é…ç½®å›³ã®é–¢æ•°ï¼ˆå®‰å…¨åŒ–ï¼‰ =====
def _corner__wrap_name(name: str, width: int = 4) -> str:
    s = str(name).strip().replace("\u3000", " ")
    s = s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    return "\n".join([s[i:i+width] for i in range(0, len(s), width)]) if s else s
def _corner__zone_from_style(sty: str) -> str:
    sty = str(sty)
    if 'é€ƒ' in sty or 'å…ˆ' in sty: return 'å‰'
    if 'å·®' in sty:               return 'ä¸­'
    if 'è¿½' in sty:               return 'å¾Œ'
    return 'ä¸­'
def _corner__to_int(x):
    try:
        return int(str(x).translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789')))
    except Exception:
        return None
def _corner__path_from_draw(draw: int | None, field_size: int) -> str:
    if not field_size or field_size <= 0: field_size = 16
    if draw is None:
        return "ä¸­"
    lo = max(1, int(np.ceil(field_size * 0.25)))
    hi = int(np.floor(field_size * 0.75))
    if draw <= lo:   return "å†…"
    if draw >  hi:   return "å¤–"
    return "ä¸­"
def _corner__cell_offsets(n: int) -> list[tuple[float, float]]:
    if n <= 0: return []
    out = [(0.0, 0.0)]
    if n == 1: return out
    step_r = 0.018; step_t = 0.10
    i = 1
    while len(out) < n:
        ring = (i-1)//6 + 1
        pos  = (i-1)%6
        dr = ((-1)**ring) * step_r * ring
        dtheta = (pos - 2.5) * step_t
        out.append((dr, dtheta))
        i += 1
    return out[:n]
def _corner__draw_track(ax):
    ax.add_patch(Rectangle((0.05, 0.22), 0.55, 0.48, facecolor="#97b42b", edgecolor="none", alpha=0.92))
    cx, cy = 0.60, 0.46
    radii = [0.40, 0.34, 0.28, 0.22]
    lane_colors = ["#6f8f0c", "#7aa012", "#86b71a"]
    for i in range(3):
        ax.add_patch(Wedge((cx, cy), r=radii[i], theta1=-90, theta2=0,
                           width=radii[i]-radii[i+1], facecolor=lane_colors[i], edgecolor="none", alpha=0.98))
    return (cx, cy, radii)
def render_corner_positions_nowrace(horses_df: pd.DataFrame,
                                    combined_style_series: pd.Series,
                                    title: str = "4ã‚³ãƒ¼ãƒŠãƒ¼æƒ³å®šãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆä»Šãƒ¬ãƒ¼ã‚¹ï¼‰",
                                    circle_radius_ax: float | None = None,
                                    name_wrap: int = 4) -> plt.Figure:
    df = horses_df.copy()
    if 'é¦¬å' not in df.columns:
        raise ValueError("horses_df ã«ã€é¦¬åã€åˆ—ãŒå¿…è¦ã§ã™")
    sty = combined_style_series.reindex(df['é¦¬å']).fillna('')
    df['zone4'] = sty.map(_corner__zone_from_style).fillna('ä¸­')
    FS = max(1, len(df))
    # â˜… drawï¼ˆé¦¬ç•ª/æ ï¼‰ã‚’å®‰å…¨ã«ç®—å‡ºï¼ˆpd.NA/Noneã§ã‚‚è½ã¡ãªã„ï¼‰
    if 'ç•ª' in df.columns:
        draw = df['ç•ª'].map(_corner__to_int)
    elif 'æ ' in df.columns:
        _wk = df['æ '].map(_corner__to_int)
        group = int(np.ceil(FS/8))
        draw = _wk.apply(lambda w: ((w-1)*group + 1) if pd.notna(w) else None)
    else:
        draw = pd.Series([i+1 for i in range(FS)], index=df.index, dtype=object)
    df['path4'] = draw.apply(lambda x: _corner__path_from_draw(int(x) if (x is not None and pd.notna(x)) else None, FS))

    fig, ax = plt.subplots(figsize=(6.6, 5.2), dpi=140)
    ax.set_axis_off()
    cx, cy, radii = _corner__draw_track(ax)

    ax.text(0.5, 0.93, title, ha="center", va="center", fontsize=12, weight="bold", fontproperties=jp_font)
    ax.text(0.5, 0.87, "ãƒ•ãƒ©ãƒƒãƒˆ", ha="center", va="center", fontsize=10, color="#c33", fontproperties=jp_font)
    ax.text(0.06, 0.17, "ã‚»ãƒ«ï¼é€²è·¯ï¼ˆå†…/ä¸­/å¤–ï¼‰Ã—ä½ç½®ï¼ˆå‰/ä¸­/å¾Œï¼‰", fontsize=9, fontproperties=jp_font)

    lane_to_r = {"å†…": radii[2] + 0.025, "ä¸­": radii[1] - 0.025, "å¤–": radii[0] - 0.040}
    zone_to_theta = {"å¾Œ": -60, "ä¸­": -40, "å‰": -20}
    if circle_radius_ax is None:
        circle_radius_ax = 0.035 if FS <= 14 else (0.032 if FS <= 16 else 0.029)

    for (lane, zone), g in df.groupby(['path4','zone4'], dropna=False):
        if lane not in lane_to_r or zone not in zone_to_theta:
            continue
        base_r = lane_to_r[lane]
        base_theta = np.deg2rad(zone_to_theta[zone])
        offsets = _corner__cell_offsets(len(g))
        for (dr, dth), (_, row) in zip(offsets, g.iterrows()):
            r = base_r + dr
            th = base_theta + dth
            x = cx + r * np.cos(th)
            y = cy + r * np.sin(th)
            circle = Circle((x, y), radius=circle_radius_ax, transform=ax.transAxes,
                            facecolor="white", edgecolor="#222", linewidth=1.2, zorder=6)
            ax.add_patch(circle)
            nm = _corner__wrap_name(row['é¦¬å'], name_wrap)
            ax.text(x, y, nm, ha="center", va="center", fontsize=8.5, zorder=7, fontproperties=jp_font)

    ax.set_xlim(0, 1); ax.set_ylim(0, 1)
    return fig

# ======================== çµæœã‚¿ãƒ– ========================
tab_dash, tab_prob, tab_pace, tab_bets, tab_all, tab_pedi = st.tabs(
    ["ğŸ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ“ˆ å‹ç‡", "ğŸ§­ å±•é–‹", "ğŸ« è²·ã„ç›®", "ğŸ“ å…¨é ­ã‚³ãƒ¡ãƒ³ãƒˆ", "ğŸ§¬ è¡€çµ±HTML"]
)

with tab_dash:
    st.subheader("ã‚µãƒãƒªãƒ¼")
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("æƒ³å®šãƒšãƒ¼ã‚¹", locals().get("pace_type","â€”"))
    c2.metric("å‡ºèµ°é ­æ•°", len(horses))
    c3.metric("æœ¬ãƒ¬ãƒ¼ã‚¹å›ã‚Š", f"{TARGET_TURN}")
    if len(topN) > 0:
        c4.metric("â— FinalZ", f"{topN.iloc[0]['FinalZ']:.1f}")
        try:
            win_pct = float(df_agg.loc[df_agg['é¦¬å']==topN.iloc[0]['é¦¬å'],'å‹ç‡%_MC'].iloc[0])
            c5.metric("â— æ¨å®šå‹ç‡", f"{win_pct:.1f}%")
        except Exception:
            c5.metric("â— æ¨å®šå‹ç‡", "â€”")

    # Bandåˆ†å¸ƒã‚µãƒãƒªãƒ¼
    st.markdown("#### Bandåˆ†å¸ƒ")
    band_counts = df_agg['Band'].value_counts().reindex(['SS','S','A','B','C','E']).fillna(0).astype(int)
    dd = pd.DataFrame({'Band':band_counts.index, 'é ­æ•°': band_counts.values})
    st.dataframe(dd, use_container_width=True, height=H(dd, 200))

    st.markdown("#### ä¸Šä½é¦¬ï¼ˆFinalZâ‰§50ãƒ»æœ€å¤§6é ­ï¼‰")
    _top = topN.merge(df_agg[['é¦¬å','å‹ç‡%_MC','TurnPref','AR100','Band']], on='é¦¬å', how='left')
    show_cols = [c for c in ['é¦¬å','å°','AR100','Band','FinalZ','WAvgZ','WStd','PacePts','TurnPref','å‹ç‡%_MC'] if c in _top.columns]
    st.dataframe(_top[show_cols], use_container_width=True, height=H(_top, 260))

    st.markdown("#### å›ã‚Šé©æ€§ï¼ˆä»Šå›ã®å›ã‚Šã‚’åŸºæº–ï¼‰")
    if {'RightZ','LeftZ','TurnPref','TurnGap'}.issubset(df_agg.columns):
        tv = df_agg[['é¦¬å','RightZ','LeftZ','TurnGap','TurnPref']].copy()
        tv = tv.sort_values('TurnGap', ascending=(TARGET_TURN=='å·¦')).reset_index(drop=True)
        st.dataframe(tv, use_container_width=True, height=H(tv, 260))
    else:
        st.info("å›ã‚Šé©æ€§ã‚’ç®—å‡ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

with tab_prob:
    st.subheader("æ¨å®šå‹ç‡ãƒ»è¤‡å‹ç‡ï¼ˆãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ï¼‰")
    prob_view = (
        df_agg[['é¦¬å','AR100','Band','FinalZ','WAvgZ','WStd','PacePts','TurnPref','å‹ç‡%_MC','è¤‡å‹ç‡%_MC']]
        .sort_values(['Band','AR100','å‹ç‡%_MC'], ascending=[True,False,False]).reset_index(drop=True)
    )
    _pv = prob_view.copy()
    for c in ['å‹ç‡%_MC','è¤‡å‹ç‡%_MC']:
        if c in _pv: _pv[c] = _pv[c].map(lambda x: f"{x:.2f}%")
    st.dataframe(_pv, use_container_width=True, height=H(prob_view, 420))

with tab_pace:
    st.subheader("å±•é–‹ãƒ»è„šè³ªã‚µãƒãƒªãƒ¼")
    st.caption(f"æƒ³å®šãƒšãƒ¼ã‚¹: {locals().get('pace_type','â€”')}ï¼ˆ{'å›ºå®š' if pace_mode=='å›ºå®šï¼ˆæ‰‹å‹•ï¼‰' else 'è‡ªå‹•MC'}ï¼‰")

    df_map = horses.copy()
    if 'è„šè³ª' not in df_map.columns:
        df_map['è„šè³ª'] = ''
    # æ‰‹å…¥åŠ›ãŒã‚ã‚Œã°å„ªå…ˆã€ç©ºæ¬„ã¯ combined_style ã§è£œå®Œ
    auto_st = df_map['é¦¬å'].map(combined_style)
    cond_filled = df_map['è„šè³ª'].astype(str).str.strip().ne('')
    df_map.loc[~cond_filled, 'è„šè³ª'] = auto_st.loc[~cond_filled]
    # æ­£è¦åŒ–ã—ã¦æœªå®šç¾©ã¯ç©ºã¸
    df_map['è„šè³ª'] = df_map['è„šè³ª'].map(normalize_style)
    df_map['è„šè³ª'] = df_map['è„šè³ª'].fillna('').where(df_map['è„šè³ª'].isin(STYLES), other='')

    style_counts = df_map['è„šè³ª'].value_counts().reindex(STYLES).fillna(0).astype(int)
    total_heads = int(style_counts.sum()) if style_counts.sum() > 0 else 1
    style_pct = (style_counts / total_heads * 100).round(1)

    pace_summary = pd.DataFrame([{
        'æƒ³å®šãƒšãƒ¼ã‚¹': locals().get('pace_type','â€”'),
        'é€ƒã’':  f"{style_counts['é€ƒã’']}é ­ï¼ˆ{style_pct['é€ƒã’']}%ï¼‰",
        'å…ˆè¡Œ':  f"{style_counts['å…ˆè¡Œ']}é ­ï¼ˆ{style_pct['å…ˆè¡Œ']}%ï¼‰",
        'å·®ã—':  f"{style_counts['å·®ã—']}é ­ï¼ˆ{style_pct['å·®ã—']}%ï¼‰",
        'è¿½è¾¼':  f"{style_counts['è¿½è¾¼']}é ­ï¼ˆ{style_pct['è¿½è¾¼']}%ï¼‰",
    }])
    st.table(pace_summary)

    st.markdown("#### 4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³é…ç½®å›³ï¼ˆä»Šãƒ¬ãƒ¼ã‚¹ãƒ»æƒ³å®šï¼‰")
    try:
        fig_corner = render_corner_positions_nowrace(
            horses_df=horses,
            combined_style_series=combined_style,
            title=f"4ã‚³ãƒ¼ãƒŠãƒ¼æƒ³å®šãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆ{locals().get('pace_type','â€”')}ï¼{TARGET_TURN}å›ã‚Šï¼‰"
        )
        st.pyplot(fig_corner, use_container_width=True)
    except Exception as e:
        st.info(f"4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³é…ç½®å›³ã¯ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # é¦¬ç•ªÃ—è„šè³ªã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å›³ï¼ˆæ—¢å­˜ï¼‰
    def _normalize_ban(x):
        return pd.to_numeric(str(x).translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™','0123456789')), errors='coerce')
    if 'ç•ª' in df_map.columns:
        df_map['_ban'] = _normalize_ban(df_map['ç•ª'])
        loc_df = df_map.dropna(subset=['_ban']).copy()
        loc_df = loc_df[loc_df['è„šè³ª'].isin(STYLES)]
        if not loc_df.empty:
            loc_df['_ban'] = loc_df['_ban'].astype(int)
            loc_df = loc_df.sort_values('_ban')

            fig, ax = plt.subplots(figsize=(10, 3))
            colors = {'é€ƒã’':'red', 'å…ˆè¡Œ':'orange', 'å·®ã—':'green', 'è¿½è¾¼':'blue'}
            for _, row in loc_df.iterrows():
                x = int(row['_ban']); y = STYLES.index(row['è„šè³ª'])
                ax.scatter(x, y, color=colors[row['è„šè³ª']], s=200)
                ax.text(
                    x, y, str(row['é¦¬å']),
                    ha='center', va='center', color='white', fontsize=9, weight='bold',
                    bbox=dict(facecolor=colors[row['è„šè³ª']], alpha=0.7, boxstyle='round'),
                    fontproperties=jp_font
                )
            ax.set_yticks([0,1,2,3]); ax.set_yticklabels(STYLES, fontproperties=jp_font)
            xs = sorted(loc_df['_ban'].unique())
            ax.set_xticks(xs); ax.set_xticklabels([f"{i}ç•ª" for i in xs], fontproperties=jp_font)
            ax.set_xlabel("é¦¬ç•ª", fontproperties=jp_font); ax.set_ylabel("è„šè³ª", fontproperties=jp_font)
            ax.set_title(f"å±•é–‹ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ{locals().get('pace_type','â€”')}æƒ³å®šï¼‰", fontproperties=jp_font)
            st.pyplot(fig)
        else:
            st.info("é¦¬ç•ªã¾ãŸã¯è„šè³ªãŒæœªå…¥åŠ›ã®ãŸã‚ã€é…ç½®å›³ã¯çœç•¥ã—ã¾ã—ãŸã€‚")
    else:
        st.info("å‡ºèµ°è¡¨ã«ã€ç•ªã€åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€é…ç½®å›³ã¯çœç•¥ã—ã¾ã—ãŸã€‚")

    st.markdown("#### å›ã‚Šé©æ€§ã‚µãƒãƒªãƒ¼ï¼ˆæ™‚é–“åŠ é‡ã®éå»èµ°ã‚¹ã‚³ã‚¢ã§æ¨å®šï¼‰")
    if {'RightZ','LeftZ','TurnPref','TurnGap'}.issubset(df_agg.columns):
        tv = df_agg[['é¦¬å','RightZ','LeftZ','TurnGap','TurnPref']].copy()
        tv = tv.sort_values('TurnGap', ascending=(TARGET_TURN=='å·¦')).reset_index(drop=True)
        st.dataframe(tv, use_container_width=True, height=H(tv, 260))
    else:
        st.info("å›ã‚Šé©æ€§ã‚’ç®—å‡ºã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

with tab_bets:
    # === è¦‹é€ã‚Šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚ãªãŸã®æ–¹é‡ï¼‰ ===
    allow_bet = bool((df_agg['AR100'] >= 70).any())
    if not allow_bet:
        st.subheader("ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã¯ã€è¦‹é€ã‚Šã€")
        st.info("æ–¹é‡ï¼š**Aä»¥ä¸ŠãŒ1é ­ã§ã‚‚ã„ãŸã‚‰è²·ã†ï¼Bä»¥ä¸‹ã—ã‹ã„ãªã„ãªã‚‰è¦‹é€ã‚Š**ã€‚æœ¬ãƒ¬ãƒ¼ã‚¹ã¯Aä»¥ä¸ŠãŒ0é ­ã®ãŸã‚è¦‹é€ã‚Šåˆ¤å®šã§ã™ã€‚")
    else:
        h1 = topN.iloc[0]['é¦¬å'] if len(topN) >= 1 else None
        h2 = topN.iloc[1]['é¦¬å'] if len(topN) >= 2 else None
        symbols = topN.get('å°', pd.Series([], dtype=str)).tolist()
        names   = topN['é¦¬å'].tolist()
        others_names   = names[1:] if len(names) > 1 else []
        others_symbols = symbols[1:] if len(symbols) > 1 else []

        three = ['é¦¬é€£','ãƒ¯ã‚¤ãƒ‰','é¦¬å˜']
        def round_to_unit(x, unit): return int(np.floor(x / unit) * unit)

        main_share = 0.5
        pur1 = round_to_unit(total_budget * main_share * (1/4), int(min_unit))  # å˜å‹
        pur2 = round_to_unit(total_budget * main_share * (3/4), int(min_unit))  # è¤‡å‹
        rem  = total_budget - (pur1 + pur2)

        win_each   = round_to_unit(pur1 / 2, int(min_unit))
        place_each = round_to_unit(pur2 / 2, int(min_unit))

        st.subheader("â–  è³‡é‡‘é…åˆ† (å³å¯†åˆè¨ˆ)")
        st.write(f"åˆè¨ˆäºˆç®—ï¼š{total_budget:,}å††  å˜å‹ï¼š{pur1:,}å††  è¤‡å‹ï¼š{pur2:,}å††  æ®‹ï¼š{rem:,}å††  [å˜ä½:{min_unit}å††]")

        bets = []
        if h1 is not None:
            bets += [{'åˆ¸ç¨®':'å˜å‹','å°':'â—','é¦¬':h1,'ç›¸æ‰‹':'','é‡‘é¡':win_each},
                     {'åˆ¸ç¨®':'è¤‡å‹','å°':'â—','é¦¬':h1,'ç›¸æ‰‹':'','é‡‘é¡':place_each}]
        if h2 is not None:
            bets += [{'åˆ¸ç¨®':'å˜å‹','å°':'ã€‡','é¦¬':h2,'ç›¸æ‰‹':'','é‡‘é¡':win_each},
                     {'åˆ¸ç¨®':'è¤‡å‹','å°':'ã€‡','é¦¬':h2,'ç›¸æ‰‹':'','é‡‘é¡':place_each}]

        finalZ_map = df_agg.set_index('é¦¬å')['FinalZ'].to_dict()
        pair_candidates, tri_candidates, tri1_candidates = [], [], []
        if h1 is not None and len(others_names) > 0:
            for nm, mk in zip(others_names, others_symbols):
                score = finalZ_map.get(nm, 0)
                pair_candidates.append(('ãƒ¯ã‚¤ãƒ‰', f'â—â€“{mk}', h1, nm, score))
                pair_candidates.append(('é¦¬é€£', f'â—â€“{mk}', h1, nm, score))
                pair_candidates.append(('é¦¬å˜', f'â—â†’{mk}', h1, nm, score))
            for a, b in combinations(others_names, 2):
                score = finalZ_map.get(a,0) + finalZ_map.get(b,0)
                tri_candidates.append(('ä¸‰é€£è¤‡','â—-ã€‡â–²â˜†â–³â–³', h1, f"{a}ï¼{b}", score))
            second_opts = others_names[:2]
            for s in second_opts:
                for t in others_names:
                    if t == s: continue
                    score = finalZ_map.get(s,0) + 0.7*finalZ_map.get(t,0)
                    tri1_candidates.append(('ä¸‰é€£å˜ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³','â—-ã€‡â–²-ã€‡â–²â˜†â–³â–³', h1, f"{s}ï¼{t}", score))

        if scenario == 'é€šå¸¸':
            with st.expander("é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ãƒ»é¦¬å˜ ã‹ã‚‰ï¼‘åˆ¸ç¨®ã‚’é¸æŠ", expanded=True):
                choice = st.radio("è³¼å…¥åˆ¸ç¨®", options=three, index=1)
                st.write(f"â–¶ {choice} ã«æ®‹ã‚Š {rem:,}å†† ã‚’å……å½“")
            cand = [c for c in pair_candidates if c[0]==choice]
            cand = sorted(cand, key=lambda x: x[-1], reverse=True)[:int(max_lines)]
            K = len(cand)
            if K > 0 and rem >= int(min_unit):
                base = round_to_unit(rem / K, int(min_unit))
                amounts = [base]*K
                leftover = rem - base*K
                i = 0
                while leftover >= int(min_unit) and i < K:
                    amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
                for (typ, mks, base_h, pair_h, _), amt in zip(cand, amounts):
                    bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
            else:
                st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

        elif scenario == 'ã¡ã‚‡ã„ä½™è£•':
            st.write("â–¶ æ®‹ã‚Šäºˆç®—ã‚’ ãƒ¯ã‚¤ãƒ‰ ï¼‹ ä¸‰é€£è¤‡ ã§é…åˆ†")
            cand_wide = sorted([c for c in pair_candidates if c[0]=='ãƒ¯ã‚¤ãƒ‰'], key=lambda x: x[-1], reverse=True)
            cand_tri  = sorted(tri_candidates, key=lambda x: x[-1], reverse=True)
            cut_w = min(len(cand_wide), int(max_lines)//2 if int(max_lines)>1 else 1)
            cut_t = min(len(cand_tri),  int(max_lines) - cut_w)
            cand_wide, cand_tri = cand_wide[:cut_w], cand_tri[:cut_t]
            K = len(cand_wide) + len(cand_tri)
            if K>0 and rem >= int(min_unit):
                base = round_to_unit(rem / K, int(min_unit))
                amounts = [base]*K
                leftover = rem - base*K
                i = 0
                while leftover >= int(min_unit) and i < K:
                    amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
                all_cand = cand_wide + cand_tri
                for (typ, mks, base_h, pair_h, _), amt in zip(all_cand, amounts):
                    bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
            else:
                st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

        elif scenario == 'ä½™è£•':
            st.write("â–¶ æ®‹ã‚Šäºˆç®—ã‚’ ãƒ¯ã‚¤ãƒ‰ ï¼‹ ä¸‰é€£è¤‡ ï¼‹ ä¸‰é€£å˜ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ ã§é…åˆ†")
            cand_wide = sorted([c for c in pair_candidates if c[0]=='ãƒ¯ã‚¤ãƒ‰'], key=lambda x: x[-1], reverse=True)
            cand_tri  = sorted(tri_candidates, key=lambda x: x[-1], reverse=True)
            cand_tri1 = sorted(tri1_candidates, key=lambda x: x[-1], reverse=True)
            r_w, r_t, r_t1 = 2, 2, 1
            denom = r_w + r_t + r_t1
            q_w = max(1, (int(max_lines) * r_w)//denom)
            q_t = max(1, (int(max_lines) * r_t)//denom)
            q_t1= max(1, int(max_lines) - q_w - q_t)
            cand_wide, cand_tri, cand_tri1 = cand_wide[:q_w], cand_tri[:q_t], cand_tri1[:q_t1]
            K = len(cand_wide) + len(cand_tri) + len(cand_tri1)
            if K>0 and rem >= int(min_unit):
                base = round_to_unit(rem / K, int(min_unit))
                amounts = [base]*K
                leftover = rem - base*K
                i = 0
                while leftover >= int(min_unit) and i < K:
                    amounts[i] += int(min_unit); leftover -= int(min_unit); i += 1
                all_cand = cand_wide + cand_tri + cand_tri1
                for (typ, mks, base_h, pair_h, _), amt in zip(all_cand, amounts):
                    bets.append({'åˆ¸ç¨®':typ,'å°':mks,'é¦¬':base_h,'ç›¸æ‰‹':pair_h,'é‡‘é¡':int(amt)})
            else:
                st.info("é€£ç³»ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç›¸æ‰‹ä¸è¶³ or äºˆç®—ä¸è¶³ï¼‰")

        _df = pd.DataFrame(bets)
        spent = int(_df['é‡‘é¡'].fillna(0).replace('',0).sum()) if len(_df)>0 else 0
        diff = total_budget - spent
        if diff != 0 and len(_df) > 0:
            for idx in _df.index:
                cur = int(_df.at[idx,'é‡‘é¡'])
                new = cur + diff
                if new >= 0 and new % int(min_unit) == 0:
                    _df.at[idx,'é‡‘é¡'] = new
                    break

        _df_disp = _df.copy()
        if 'é‡‘é¡' in _df_disp.columns and len(_df_disp) > 0:
            def fmt_money(x):
                try:
                    xv = float(x)
                    if np.isnan(xv) or int(xv) <= 0: return ""
                    return f"{int(xv):,}å††"
                except Exception:
                    return ""
            _df_disp['é‡‘é¡'] = _df_disp['é‡‘é¡'].map(fmt_money)
        st.subheader("â–  æœ€çµ‚è²·ã„ç›®ä¸€è¦§ï¼ˆå…¨åˆ¸ç¨®ã¾ã¨ã‚ï¼‰")
        if len(_df_disp) == 0:
            st.info("ç¾åœ¨ã€è²·ã„ç›®ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.table(safe_take(_df_disp, ['åˆ¸ç¨®','å°','é¦¬','ç›¸æ‰‹','é‡‘é¡']))

with tab_all:
    st.subheader("å…¨é ­AIè¨ºæ–­ã‚³ãƒ¡ãƒ³ãƒˆ")
    q = st.text_input("é¦¬åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")
    _all = horses2.merge(df_agg[['é¦¬å','TurnPref','AR100','Band']], on='é¦¬å', how='left')
    _all = _all[[c for c in ['é¦¬å','å°','è„šè³ª','çŸ­è©•','AR100','Band','WAvgZ','WStd','TurnPref'] if c in _all.columns]]
    if q.strip():
        _all = _all[_all['é¦¬å'].astype(str).str.contains(q.strip(), case=False, na=False)]
    if _all.empty:
        st.info("ã‚³ãƒ¡ãƒ³ãƒˆè¡¨ç¤ºå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(_all, use_container_width=True, height=H(_all, 420))

# ======================== è¡€çµ±HTML æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«å®šç¾©ï¼‰ ========================
def _detect_charset_from_head(raw: bytes) -> str | None:
    if raw.startswith(b"\xef\xbb\xbf"):  # UTF-8 BOM
        return "utf-8-sig"
    if raw.startswith(b"\xff\xfe"):
        return "utf-16-le"
    if raw.startswith(b"\xfe\xff"):
        return "utf-16-be"
    head_txt = raw[:4096].decode("ascii", "ignore")
    m1 = re.search(r'charset\s*=\s*[\'"]?([\w\-]+)', head_txt, flags=re.I)
    return m1.group(1).lower() if m1 else None

def _decode_html_bytes(raw: bytes, preferred: str | None = None) -> str:
    declared = _detect_charset_from_head(raw)
    cands = [c for c in [preferred, declared,
                         "cp932", "shift_jis", "utf-8", "utf-8-sig",
                         "euc_jp", "iso2022_jp", "utf-16", "utf-16-le", "utf-16-be"] if c]
    seen = set()
    for enc in [c for c in cands if not (c in seen or seen.add(c))]:
        try:
            txt = raw.decode(enc)
            # æ–‡å­—åŒ–ã‘ãŒé…·ã„utf-8ã‚’å¼¾ãè»½ã„ã‚¬ãƒ¼ãƒ‰
            if enc.startswith("utf-8") and txt.count("ï¿½") > 10:
                continue
            return txt
        except Exception:
            continue
    return raw.decode("utf-8", errors="replace")

def _to_pedigree_url(u: str) -> str:
    m_id = re.search(r"race_id=(\d{12})", u)
    if m_id and "pedigree" not in u:
        return f"https://race.netkeiba.com/race/shutuba/pedigree.html?race_id={m_id.group(1)}"
    return u

def _fetch_url_html(u: str) -> tuple[str, str | None]:
    try:
        import requests
        used = _to_pedigree_url(u)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Referer": "https://race.netkeiba.com/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        }
        r = requests.get(used, headers=headers, timeout=15, allow_redirects=True)
        if r.status_code == 200 and len(r.content) > 500:
            return _decode_html_bytes(r.content), None
        return "", f"HTTP {r.status_code} / bytes={len(r.content)}"
    except Exception as e:
        return "", f"{type(e).__name__}: {e}"

def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = ["".join([str(c) for c in tup if "Unnamed" not in str(c)]).strip()
                      for tup in df.columns]
    else:
        df.columns = [str(c).strip() for c in df.columns]
    return df

def _promote_header_row_if_needed(df: pd.DataFrame) -> pd.DataFrame:
    cols = [str(c) for c in df.columns]
    if any(re.search("é¦¬å", c) for c in cols):
        return df
    for i in range(min(6, len(df))):
        row = df.iloc[i].astype(str)
        if row.str.contains("é¦¬å").any():
            new_cols = row.str.replace(r"\s+", "", regex=True).tolist()
            df = df.iloc[i+1:].copy()
            df.columns = new_cols
            break
    return df

def _extract_pedi_tables_from_html(html_text: str) -> list[pd.DataFrame]:
    try:
        tables = pd.read_html(html_text, flavor="lxml")
    except Exception:
        try:
            tables = pd.read_html(html_text, flavor="bs4")
        except Exception:
            tables = []
    fixed = []
    for t in tables:
        t = _flatten_columns(t)
        t = _promote_header_row_if_needed(t)
        if any(re.search(r"(é¦¬å|^é¦¬$)", str(c)) for c in t.columns):
            fixed.append(t.reset_index(drop=True))
    return fixed

# ======================== è¡€çµ±HTMLï¼ˆãƒ“ãƒ¥ãƒ¼ï¼‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´â†’ãƒœãƒ¼ãƒŠã‚¹ï¼‰ ========================
with tab_pedi:
    st.subheader("è¡€çµ±HTMLãƒ“ãƒ¥ãƒ¼ã‚¢ + ãƒœãƒ¼ãƒŠã‚¹ä»˜ä¸")
    st.caption("NetKeibaç­‰ã®è¡€çµ±ãƒšãƒ¼ã‚¸HTMLã‚’è¡¨ç¤º/è§£æã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ã®é¦¬ã¸ãƒœãƒ¼ãƒŠã‚¹ã‚’ä»˜ä¸ã—ã¾ã™ã€‚URLã§ã‚‚OKã€‚")

    m = st.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘ï¼ˆHTML/URLï¼‰", "HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True)

    src_url: str | None = None
    html_text = ""

    if m == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘ï¼ˆHTML/URLï¼‰":
        html_txt = st.text_area(
            "HTMLï¼ˆã‚½ãƒ¼ã‚¹å…¨æ–‡ or URLï¼‰ã‚’è²¼ã‚Šä»˜ã‘",
            height=220,
            placeholder="<html>...</html> ã¾ãŸã¯ https://race.netkeiba.com/.../pedigree.html"
        )
        val = (html_txt or "").strip()
        if re.match(r"^https?://", val):
            src_url = val
            with st.spinner("URLã‹ã‚‰å–å¾—ä¸­â€¦"):
                html_text, err = _fetch_url_html(val)
            if err:
                st.warning(
                    "URLã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§ã€ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜ï¼ˆ.htmlï¼‰ã€ã—ã¦ã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚"
                    f"ï¼ˆè©³ç´°: {err}ï¼‰"
                )
        else:
            html_text = val
    else:
        up = st.file_uploader("è¡€çµ±HTMLãƒ•ã‚¡ã‚¤ãƒ«", type=["html", "htm"], key="pedi_html_up")
        if up:
            raw  = up.read()
            html_text = _decode_html_bytes(raw)

    if html_text.strip() and COMPONENTS:
        components.html(html_text, height=700, scrolling=True)
    elif html_text.strip() and not COMPONENTS:
        st.code(html_text[:8000], language="html")

    dfs = _extract_pedi_tables_from_html(html_text) if html_text.strip() else []

    if (not dfs) and src_url:
        try:
            used = _to_pedigree_url(src_url)
            raw_tables = pd.read_html(used)
            for t in raw_tables:
                t = _flatten_columns(t)
                t = _promote_header_row_if_needed(t)
                if any(re.search(r"(é¦¬å|^é¦¬$)", str(c)) for c in t.columns):
                    dfs.append(t.reset_index(drop=True))
            if dfs:
                st.info("ãƒšãƒ¼ã‚¸åŸ‹ã‚è¾¼ã¿ã¯ä¸å¯ã§ã—ãŸãŒã€URLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"URLã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    st.markdown("### ğŸ§¬ è¡€çµ±ãƒœãƒ¼ãƒŠã‚¹è¨­å®šï¼ˆä¸‹ã§ä¸€è‡´ã—ãŸé¦¬ã«åŠ ç‚¹ï¼‰")
    default_pts = int(st.session_state.get('pedi:points', 3))
    points = st.slider("ä¸€è‡´ã—ãŸé¦¬ã¸ã®ãƒœãƒ¼ãƒŠã‚¹ç‚¹", 0, 20, default_pts)

    if dfs:
        idx = st.selectbox(
            "å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ",
            options=list(range(len(dfs))),
            format_func=lambda i: f"Table {i+1}ï¼ˆåˆ—: {', '.join(map(str, dfs[i].columns[:6]))} â€¦ï¼‰"
        )
        dfp = dfs[idx]

        name_candidates = [c for c in dfp.columns if re.search(r"(é¦¬å|^é¦¬$)", str(c))]
        name_col = st.selectbox(
            "é¦¬ååˆ—",
            options=dfp.columns.tolist(),
            index=dfp.columns.tolist().index(name_candidates[0]) if name_candidates else 0
        )

        known = ["çˆ¶ã‚¿ã‚¤ãƒ—å","çˆ¶å","æ¯çˆ¶ã‚¿ã‚¤ãƒ—å","æ¯çˆ¶å","çˆ¶ç³»","æ¯çˆ¶ç³»","çˆ¶","æ¯çˆ¶"]
        candidate_cols = [c for c in known if c in dfp.columns] or [c for c in dfp.columns if c != name_col]

        st.caption("â€» ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã€é¸æŠã—ãŸåˆ—ã®ã©ã‚Œã‹ã«ä¸€è‡´ã—ãŸã€é¦¬åã€ã¸åŠ ç‚¹ã—ã¾ã™ã€‚è¤‡æ•°è¡ŒOKã€‚")
        keys_text = st.text_area("è¡€çµ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1è¡Œ1ãƒ¯ãƒ¼ãƒ‰ï¼‰",
                                 value=st.session_state.get('pedi:keys', ""), height=120)
        match_cols = st.multiselect("ç…§åˆå¯¾è±¡ã®åˆ—", candidate_cols,
                                    default=[c for c in known if c in candidate_cols] or candidate_cols)
        method = st.radio("ç…§åˆæ–¹æ³•", ["éƒ¨åˆ†ä¸€è‡´", "å®Œå…¨ä¸€è‡´"], index=0, horizontal=True)

        def _norm(s: str) -> str:
            s = str(s)
            s = s.translate(_fwid).replace('\u3000', ' ').strip()
            return re.sub(r'\s+', '', s)

        keys = [k for k in (keys_text.splitlines() if keys_text else []) if k.strip()]
        keys_norm = [_norm(k) for k in keys]

        matched_names: list[str] = []
        if keys and match_cols:
            for _, row in dfp.iterrows():
                nm = _trim_name(row.get(name_col, ""))
                row_texts_norm = [_norm(row.get(c, "")) for c in match_cols]
                if method == "å®Œå…¨ä¸€è‡´":
                    hit = any(r == k for r in row_texts_norm for k in keys_norm)
                else:
                    hit = any(k in r for r in row_texts_norm for k in keys_norm)
                if hit and nm:
                    matched_names.append(nm)

        st.session_state['pedi:map'] = { _trim_name(n): True for n in matched_names }
        st.session_state['pedi:points'] = int(points)
        st.session_state['pedi:keys'] = keys_text

        colL, colR = st.columns([2,3])
        with colL:
            st.write("ä¸€è‡´ã—ãŸé¦¬ï¼ˆåŠ ç‚¹å¯¾è±¡ï¼‰")
            if matched_names:
                st.table(pd.DataFrame({'é¦¬å': matched_names}))
            else:
                st.info("ç¾åœ¨ã€ä¸€è‡´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰/ç…§åˆåˆ—/ç…§åˆæ–¹æ³•ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        with colR:
            st.info(f"è¨­å®šï¼šãƒœãƒ¼ãƒŠã‚¹ {points} ç‚¹ / ç…§åˆåˆ— {', '.join(map(str, match_cols)) if match_cols else 'ï¼ˆæœªé¸æŠï¼‰'} / {method}")

    else:
        st.info("é¦¬ååˆ—ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚URLãŒå–ã‚Œãªã„ç’°å¢ƒã§ã¯ã€ãƒšãƒ¼ã‚¸ã‚’ã€å®Œå…¨ä¿å­˜ï¼ˆ.htmlï¼‰ã€ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
