# -*- coding: utf-8 -*- 
# ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆAUTOçµ±åˆç‰ˆ + ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼‰

from __future__ import annotations

# ===== æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
import os, sys, io, re, json, math

# ===== è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å„ªå…ˆè§£æ±ºï¼ˆâ† ã“ã“ã‚’æœ€åˆã«ï¼‰=====
BASE = os.path.dirname(os.path.abspath(__file__))
if BASE not in sys.path:
    sys.path.insert(0, BASE)

# ãƒªãƒã‚¸ãƒˆãƒªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆui_style.py ãŒã‚ã‚‹å ´æ‰€ï¼‰ã‚‚ãƒ‘ã‚¹ã«å…¥ã‚Œã‚‹
ROOT = os.path.abspath(os.path.join(BASE, '..'))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

# ===== ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ =====
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

# === 2æ­³æˆ¦æ¤œå‡º ===
def detect_2yo_race(horses_df, race_df) -> bool:
    import pandas as pd
    # å‡ºèµ°è¡¨ã®å¹´é½¢ãŒã™ã¹ã¦2æ­³ã€ã¾ãŸã¯ç«¶èµ°åã«ã€Œ2æ­³/äºŒæ­³/2æ‰ã€
    cond_age = False
    if isinstance(horses_df, pd.DataFrame) and 'å¹´é½¢' in horses_df.columns:
        ages = pd.to_numeric(horses_df['å¹´é½¢'], errors='coerce').dropna()
        if len(ages) and ages.max() <= 2:
            cond_age = True
    name0 = ''
    if isinstance(race_df, pd.DataFrame) and 'ç«¶èµ°å' in race_df.columns and len(race_df):
        name0 = str(race_df['ç«¶èµ°å'].iloc[0])
    cond_name = any(k in name0 for k in ['2æ­³','äºŒæ­³','2æ‰'])
    return bool(cond_age or cond_name)

# â† ãƒ‘ã‚¹ã‚’é€šã—ãŸ â€œå¾Œâ€ ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ import
from ui_style import topbar, card, pill, score_bar
from course_geometry import register_all_turf, get_course_geom
from physics_sprint1 import add_phys_s1_features
from race_volatility import compute_race_volatility

# ===== Streamlit å…ˆã«ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆUIã‚’ä½¿ã†å‰ã«å‘¼ã¶ï¼‰=====
st.set_page_config(page_title="Rikeiba", layout="wide")

# ===== ã‚³ãƒ¼ã‚¹å¹¾ä½•ã®åˆæœŸåŒ–ï¼ˆ1å›ã ã‘ï¼‰=====
@st.cache_resource
def _boot_course_geom(version: int = 1):
    # ç›´å‰ã®ç™»éŒ²ã‚’ã‚¯ãƒªã‚¢ï¼ˆå®šç¾©ãŒã‚ã‚Œã°ä½¿ã†ï¼‰
    try:
        from course_geometry.registry import clear_registry
        clear_registry()
    except Exception:
        # clear_registry ãŒç„¡ã‘ã‚Œã°ç„¡è¦–ï¼ˆ_add ãŒä¸Šæ›¸ãã—ã¦ãã‚Œã‚‹æƒ³å®šï¼‰
        pass

    # ã™ã¹ã¦ã® *_turf.py ã‚’ç™»éŒ²
    from course_geometry import register_all_turf
    register_all_turf()
    return True

# â† æ•°å­—ã‚’ä¸Šã’ã‚‹ã¨ Streamlit ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç ´æ£„ã•ã‚Œã¦å†ç™»éŒ²ã•ã‚Œã‚‹
_boot_course_geom(version=39)


# â€» races_df ã«å¯¾ã—ã¦ add_phys_s1_features ã‚’â€œã“ã“ã§ã¯â€å®Ÿè¡Œã—ãªã„ã“ã¨ã€‚
#   å®Ÿéš›ã®å®Ÿè¡Œã¯ UI å´ï¼ˆä¾‹ï¼šğŸ§ª PhysS1 ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒœã‚¿ãƒ³ï¼‰ã§è¡Œã†ã€‚

# ---- optional depsï¼ˆç„¡ãã¦ã‚‚å‹•ãç³»ï¼‰----
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

try:
    from sklearn.isotonic import IsotonicRegression
    SK_ISO = True
except Exception:
    SK_ISO = False

# ===== Matplotlibï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹å®‰å…¨åŒ– & æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼‰=====
import matplotlib
matplotlib.use("Agg")  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ç’°å¢ƒï¼ˆStreamlit Cloud ç­‰ï¼‰ã§å®‰å…¨
import matplotlib.pyplot as plt
from matplotlib import font_manager

plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'IPAexGothic','IPAGothic','Noto Sans CJK JP','Yu Gothic UI','Meiryo','Hiragino Sans','MS Gothic'
]

@st.cache_resource
def get_jp_font():
    for p in [
        'ipaexg.ttf',
        '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf',
        '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc',
        '/System/Library/Fonts/Hiragino Sans W3.ttc',
        'C:/Windows/Fonts/meiryo.ttc',
    ]:
        if os.path.exists(p):
            try:
                font_manager.fontManager.addfont(p)
            except Exception:
                pass
            return font_manager.FontProperties(fname=p)
    return None

_jp_font = get_jp_font()
if _jp_font is not None:
    try:
        plt.rcParams['font.family'] = _jp_font.get_name()
    except Exception:
        pass

# ===== å°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fw = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

STYLE_ALIASES = {
    'è¿½ã„è¾¼ã¿':'è¿½è¾¼','è¿½è¾¼ã¿':'è¿½è¾¼','ãŠã„ã“ã¿':'è¿½è¾¼','ãŠã„è¾¼ã¿':'è¿½è¾¼',
    'ã•ã—':'å·®ã—','å·®è¾¼':'å·®ã—','å·®è¾¼ã¿':'å·®ã—',
    'ã›ã‚“ã“ã†':'å…ˆè¡Œ','å…ˆè¡Œ ':'å…ˆè¡Œ','å…ˆè¡Œã€€':'å…ˆè¡Œ',
    'ã«ã’':'é€ƒã’','é€ƒã’ ':'é€ƒã’','é€ƒã’ã€€':'é€ƒã’'
}

def normalize_style(s: str) -> str:
    s = str(s).replace('ã€€','').strip().translate(_fw)
    s = STYLE_ALIASES.get(s, s)
    return s if s in STYLES else ''

# ===== è¡¨ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆæ è‰²ï¼‰=====
WAKU_COLORS = {
    1: "#FFFFFF",  # ç™½
    2: "#000000",  # é»’
    3: "#FF0000",  # èµ¤
    4: "#0000FF",  # é’
    5: "#FFFF00",  # é»„
    6: "#008000",  # ç·‘
    7: "#FF7F00",  # æ©™
    8: "#FF69B4",  # æ¡ƒ
}

def _text_color_for_bg(hexcode: str) -> str:
    """èƒŒæ™¯è‰²ã«å¯¾ã—ã¦è¦‹ã‚„ã™ã„æ–‡å­—è‰²(é»’/ç™½)ã‚’è¿”ã™"""
    hexcode = hexcode.lstrip("#")
    try:
        r, g, b = [int(hexcode[i:i+2], 16) for i in (0, 2, 4)]
    except ValueError:
        return "#000000"
    lum = 0.2126 * r + 0.7152 * g + 0.0722 * b
    return "#000000" if lum > 140 else "#FFFFFF"

def style_rank_table(df: pd.DataFrame):
    """æ ã‚»ãƒ«ã‚’æ è‰²ã§å¡—ã‚Šã€æ /é¦¬ç•ªã‚’æ•´æ•°è¡¨ç¤ºã«ã—ãŸ Styler ã‚’è¿”ã™"""
    d = df.copy()
    format_cols = []
    for col in ("æ ", "é¦¬ç•ª"):
        if col in d.columns:
            vals = pd.to_numeric(d[col], errors="coerce")
            d[col] = vals
            format_cols.append(col)

    def color_waku_col(s: pd.Series):
        out = []
        for v in s:
            if pd.isna(v):
                out.append("")
                continue
            key = str(int(round(float(v))))
            bg = WAKU_COLORS.get(int(key), "#FFFFFF")
            fg = _text_color_for_bg(bg)
            out.append(f"background-color:{bg}; color:{fg}; font-weight:700; text-align:center;")
        return out

    def fmt_int(x):
        return "" if pd.isna(x) else f"{int(round(float(x)))}"

    styler = d.style
    if format_cols:
        styler = styler.format({c: fmt_int for c in format_cols})
    if "æ " in d.columns:
        styler = styler.apply(color_waku_col, subset=["æ "])
    if "é †ä½" in d.columns:
        def bold_top(s: pd.Series):
            out = []
            for v in s:
                try:
                    out.append("font-weight:700" if int(v) <= 7 else "")
                except Exception:
                    out.append("")
            return out
        styler = styler.apply(bold_top, subset=["é †ä½"])
    try:
        styler = styler.hide(axis="index")
    except Exception:
        pass
    return styler

def plot_scatter_waku(df: pd.DataFrame):
    """é¦¬ç•ªÃ—AR100 æ•£å¸ƒå›³ï¼ˆç‚¹è‰²ï¼æ è‰², å‡¡ä¾‹1ï½8, ãƒ©ãƒ™ãƒ«é‡ãªã‚Šå›é¿ã®ã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰"""
    d = df.copy()

    # æ /é¦¬ç•ªã¯æ•´æ•°åŒ–ï¼ˆè¡¨ç¤ºã‚‚ãšã‚Œãªã„ã‚ˆã†ã«ï¼‰
    for col in ("æ ", "é¦¬ç•ª"):
        if col in d.columns:
            d[col] = pd.to_numeric(d[col], errors="coerce").round(0).astype("Int64")

    # æ è‰²ã‚­ãƒ¼ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’ç”¨æ„
    if "æ " in d.columns:
        d["æ _str"] = d["æ "].astype("Int64").astype(str)
    else:
        d["æ _str"] = pd.Series([""] * len(d))

    # Xè»¸åˆ—ã®æ±ºå®š
    x_col = "é¦¬ç•ª" if "é¦¬ç•ª" in d.columns else (d.columns[0] if len(d.columns) else None)
    if x_col is None:
        return px.scatter()

    # â† ã“ã“ãŒIndentationErrorã®åŸå› ã€‚æ”¹è¡Œã—ã¦åŒã˜éšå±¤ã«æˆ»ã™
    hover_data = {"é¦¬å": True, "AR100": ":.1f"}
    if "é¦¬ç•ª" in d.columns:
        hover_data["é¦¬ç•ª"] = True
    if "æ " in d.columns:
        hover_data["æ "] = True

    fig = px.scatter(
        d,
        x=x_col,
        y="AR100",
        color="æ _str",
        color_discrete_map={str(k): v for k, v in WAKU_COLORS.items()},  # æ è‰²ã¨1å¯¾1å¯¾å¿œ
        category_orders={"æ _str": [str(i) for i in range(1, 9)]},
        hover_data=hover_data,
        labels={"æ _str": "æ "},
    )
    fig.update_traces(marker=dict(size=12, line=dict(width=1, color="rgba(0,0,0,0.35)")))
    fig.update_layout(
        template="simple_white",
        legend_title_text="æ ",
        xaxis=dict(title="é¦¬ç•ª", tickmode="linear", dtick=1, tickfont=dict(size=12)),
        yaxis=dict(title="AR100", tickfont=dict(size=12)),
        margin=dict(l=40, r=20, t=10, b=40),
    )
    return fig

def plot_ar_scatter(
    df: pd.DataFrame,
    x_col: str,
    y_col: str,
    name_col: str | None = None,
    waku_col: str | None = None,
    show_text: bool = False,
):
    """AR100 ã¨ä»»æ„ã®æ•°å€¤åˆ—ã‚’æ•£å¸ƒå›³ã§è¡¨ç¤ºã™ã‚‹ï¼ˆæ è‰²ãƒ»ãƒ©ãƒ™ãƒ«å¯¾å¿œç‰ˆï¼‰"""

    if x_col not in df.columns or y_col not in df.columns:
        return px.scatter()

    d = df.copy()

    # æ•°å€¤åˆ—ã®å‰å‡¦ç†
    d[y_col] = pd.to_numeric(d[y_col], errors="coerce")
    x_numeric = pd.to_numeric(d[x_col], errors="coerce")
    if x_numeric.notna().any():
        d["_x_val"] = x_numeric
        x_for_plot = "_x_val"
    else:
        x_for_plot = x_col

    # æ¬ æã‚’é™¤å¤–
    d = d.dropna(subset=[y_col])
    if d.empty:
        return px.scatter()

    color_key = None
    scatter_kwargs = {}

    # æ è‰²ã‚’ã‚«ãƒ©ãƒ¼ã«ä½¿ç”¨
    if waku_col and waku_col in d.columns:
        d[waku_col] = pd.to_numeric(d[waku_col], errors="coerce").round(0).astype("Int64")
        d["_waku_str"] = d[waku_col].astype("Int64").astype(str)
        color_key = "_waku_str"
        scatter_kwargs.update(
            color=color_key,
            color_discrete_map={str(k): v for k, v in WAKU_COLORS.items()},
            category_orders={color_key: [str(i) for i in range(1, 9)]},
            labels={color_key: "æ "},
        )

    # Hover æƒ…å ±ã‚’æ•´ç†
    hover_data: dict[str, object] = {x_col: True, y_col: ":.1f"}
    if name_col and name_col in d.columns:
        hover_data[name_col] = True
    if waku_col and waku_col in d.columns:
        hover_data[waku_col] = True

    if show_text and name_col and name_col in d.columns:
        scatter_kwargs["text"] = name_col

    fig = px.scatter(
        d,
        x=x_for_plot,
        y=y_col,
        hover_data=hover_data,
        **scatter_kwargs,
    )

    if show_text and name_col and name_col in d.columns:
        fig.update_traces(textposition="top center", textfont=dict(size=12))

    fig.update_traces(marker=dict(size=12, line=dict(width=1, color="rgba(0,0,0,0.35)")))
    fig.update_layout(
        template="simple_white",
        margin=dict(l=40, r=20, t=10, b=40),
    )
    if color_key:
        fig.update_layout(legend_title_text="æ ")

    # è»¸ãƒ©ãƒ™ãƒ«ãƒ»åˆ»ã¿å¹…ã‚’èª¿æ•´
    xaxis_conf = dict(title=x_col)
    if x_for_plot == "_x_val":
        xaxis_conf.update(tickmode="linear", dtick=1)
    fig.update_xaxes(**xaxis_conf)
    fig.update_yaxes(title=y_col)

    return fig

def build_marks_text(
    df: pd.DataFrame,
    *,
    name_col: str = "é¦¬å",
    mark_col: str = "å°",
    ar_col: str = "AR100",
    ar_label: str = "AR",
    ar_only_for_top: bool = False,
) -> str:
    """
    ã‚³ãƒ”ãƒšç”¨ã®å°ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œã‚‹ï¼ˆARã¯å°æ•°2æ¡ï¼‰
    å‡ºåŠ›é †: â— â†’ ã€‡ â†’ â–² â†’ â˜† â†’ â–³ï¼ˆ'æ¶ˆ'ç­‰ã¯ç„¡è¦–ï¼‰
    
    Parameters
    ----------
    df : pd.DataFrame
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
    name_col : str, default "é¦¬å"
        é¦¬åã‚’ç¤ºã™åˆ—åã€‚
    mark_col : str, default "å°"
        å°ã‚’ç¤ºã™åˆ—åã€‚
    ar_col : str, default "AR100"
        AR å€¤ã‚’ç¤ºã™åˆ—åã€‚
    ar_label : str, default "AR"
        AR å€¤ã®ãƒ©ãƒ™ãƒ«æ¥é ­è¾ã€‚
    ar_only_for_top : bool, default False
        True ã®å ´åˆã¯æœ€ä¸Šä½ã®å°ï¼ˆâ—ï¼‰ã«å¯¾ã—ã¦ã®ã¿ AR ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    required = {mark_col, name_col}
    if not required.issubset(df.columns):
        return ""
    has_ar = ar_col in df.columns
    order = ["â—", "ã€‡", "â–²", "â˜†", "â–³"]
    lines = []
    for sym in order:
        sub = df.loc[df[mark_col] == sym].copy()
        if "é †ä½" in sub.columns:
            sub = sub.sort_values("é †ä½")
        for _, r in sub.iterrows():
            name = str(r[name_col]).strip()
            show_ar = has_ar and (not ar_only_for_top or sym == order[0])
            line = f"{sym}{name}"
            if show_ar:
                try:
                    ar_val = float(r[ar_col])
                except Exception:
                    ar_val = float("nan")
                ar_txt = (
                    f"{ar_label}{ar_val:.2f}" if np.isfinite(ar_val) else f"{ar_label}--"
                )
                line = f"{line}ã€€{ar_txt}"
            lines.append(line)
    return "\n".join(lines)

def render_final_view(df_ranked: pd.DataFrame):
    """æœ€çµ‚ä¸€è¦§ãƒ†ãƒ¼ãƒ–ãƒ« â†’ æ•£å¸ƒå›³ â†’ ã‚³ãƒ”ãƒšç”¨å° ã‚’ä¸€æ‹¬è¡¨ç¤º"""
    st.subheader("ğŸ æœ€çµ‚ä¸€è¦§")
    st.dataframe(style_rank_table(df_ranked), use_container_width=True)

    fig = plot_scatter_waku(df_ranked)
    st.plotly_chart(fig, use_container_width=True)

    st.subheader("å°ï¼ˆã‚³ãƒ”ãƒšç”¨ï¼‰")
    st.code(build_marks_text(df_ranked), language="text")

# ===== å…±é€šé–¢æ•°ç¾¤ =====
def season_of(m: int) -> str:
    if 3 <= m <= 5:  return 'æ˜¥'
    if 6 <= m <= 8:  return 'å¤'
    if 9 <= m <= 11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    s = pd.to_numeric(s, errors='coerce')
    std = s.std(ddof=0)
    if not np.isfinite(std) or std == 0:
        return pd.Series([50] * len(s), index=s.index)
    return 50 + 10 * (s - s.mean()) / std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2)) + float('0.' + m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2)) + int(m.group(3)) / 10
    try:
        return float(s)
    except Exception:
        return np.nan

def _trim_name(x):
    try:
        return str(x).replace('\u3000', ' ').strip()
    except Exception:
        return str(x)

def w_std_unbiased(x, w, ddof=1):
    x = np.asarray(x, float)
    w = np.asarray(w, float)
    sw = w.sum()
    if not np.isfinite(sw) or sw <= 0:
        return np.nan
    m = np.sum(w * x) / sw
    var = np.sum(w * (x - m) ** 2) / sw
    n_eff = (sw ** 2) / np.sum(w ** 2) if np.sum(w ** 2) > 0 else 0
    if ddof and n_eff > ddof:
        var *= n_eff / (n_eff - ddof)
    return float(np.sqrt(max(var, 0.0)))

def ndcg_by_race(frame: pd.DataFrame, scores, k: int = 3) -> float:
    f = frame[['race_id', 'y']].copy().reset_index(drop=True)
    s = np.asarray(scores, float)
    if len(s) != len(f):
        s = s[:len(f)]
    vals = []
    for _, idx in f.groupby('race_id').groups.items():
        idx = np.asarray(list(idx), int)
        y_true = np.nan_to_num(f.loc[idx, 'y'].astype(float).to_numpy(), nan=0.0)
        y_pred = np.nan_to_num(s[idx].astype(float), nan=0.0)
        m = len(idx)
        if m == 0:
            continue
        if m == 1:
            vals.append(1.0 if y_true[0] > 0 else 0.0)
            continue
        kk = int(min(max(1, k), m))
        order = np.argsort(-y_pred)
        gains = (2.0 ** y_true[order] - 1.0)
        discounts = 1.0 / np.log2(np.arange(2, m + 2))
        dcg = float(np.sum(gains[:kk] * discounts[:kk]))
        order_best = np.argsort(-y_true)
        gains_best = (2.0 ** y_true[order_best] - 1.0)
        idcg = float(np.sum(gains_best[:kk] * discounts[:kk]))
        vals.append(dcg / idcg if idcg > 0 else 0.0)
    return float(np.mean(vals)) if vals else float('nan')

def safe_iso_predict(ir, p_vec: np.ndarray) -> np.ndarray:
    x = np.asarray(p_vec, float)
    x = np.nan_to_num(x, nan=1.0 / max(len(x), 1), posinf=1 - 1e-6, neginf=1e-6)
    x = np.clip(x, 1e-6, 1 - 1e-6)
    try:
        y = ir.predict(x)
        y = np.nan_to_num(y, nan=x.mean(), posinf=1 - 1e-6, neginf=1e-6)
        y = np.clip(y, 1e-6, 1 - 1e-6)
        s = y.sum()
        return (y / s) if s > 0 else x
    except Exception:
        return x

# ===== Bradleyâ€“Terry: ãƒ©ãƒ³ã‚­ãƒ³ã‚°â†’ãƒšã‚¢å‹æ•—â†’MMæ›´æ–° =====
def _build_pairwise_from_ranks(df_rank: pd.DataFrame,
                               rid_col: str, name_col: str, rank_col: str,
                               weight_col: str | None = None):
    """
    å„ãƒ¬ãƒ¼ã‚¹å†…ã®ç€é †ã‹ã‚‰ãƒšã‚¢å‹æ•—ã‚«ã‚¦ãƒ³ãƒˆ n_ij ã‚’æ§‹ç¯‰ã€‚
    n_ij: i ãŒ j ã«å‹ã£ãŸå›æ•°ï¼ˆé‡ã¿ä»˜ãï¼‰ã€‚å¯¾ç§°ãª m_ij = n_ij + n_ji ã‚‚è¿”ã™ã€‚
    """
    # åå‰â†’index
    names = pd.Index(sorted(df_rank[name_col].dropna().astype(str).unique()))
    idx = {n:i for i,n in enumerate(names)}
    n = len(names)
    if n == 0:
        return names, np.zeros((0,0)), np.zeros((0,0))

    N = np.zeros((n, n), float)  # n_ij
    for rid, g in df_rank.groupby(rid_col):
        g = g.dropna(subset=[name_col, rank_col]).copy()
        if g.empty:
            continue
        # ä½ã„æ•°å€¤ãŒä¸Šä½ã¨ã„ã†ä»®å®šï¼ˆâ€œç¢ºå®šç€é †â€ï¼‰
        g = g.sort_values(rank_col)
        # é‡ã¿ï¼ˆæ™‚ç³»åˆ—ãªã©ï¼‰ã‚’ race å˜ä½ã§æŒã¡ãŸã„ãªã‚‰ã“ã“ã§ race_w ã‚’ä½œæˆ
        if weight_col and weight_col in g.columns:
            w = float(pd.to_numeric(g[weight_col], errors='coerce').mean() or 1.0)
        else:
            w = 1.0

        # ä¸Šä½ã¯ä¸‹ä½ã™ã¹ã¦ã«å‹åˆ©
        order = g[name_col].astype(str).tolist()
        m = len(order)
        for a in range(m):
            ia = idx.get(order[a]); 
            if ia is None: 
                continue
            for b in range(a+1, m):
                ib = idx.get(order[b])
                if ib is None:
                    continue
                N[ia, ib] += w  # ia beats ib
    # åˆè¨ˆæ¯”è¼ƒå›æ•° m_ij
    M = N + N.T
    return names, N, M

def fit_bradley_terry(names: pd.Index, N: np.ndarray, M: np.ndarray,
                      max_iter: int = 200, tol: float = 1e-10):
    """
    Hunter(2004)ã®MMæ›´æ–°å¼ï¼š
      w_i^{new} = S_i / sum_j ( M_ij / (w_i + w_j) ),  S_i = sum_j N_ij
    åˆæœŸã¯ä¸€æ§˜ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸å®šæ€§ã¯ sum(w)=1 ã«è¦æ ¼åŒ–ã€‚
    """
    n = len(names)
    if n == 0:
        return pd.Series([], index=names, dtype=float)

    w = np.ones(n, float) / n
    # å‹æ•°
    S = N.sum(axis=1)

    for _ in range(max_iter):
        wsum = w[:, None] + w[None, :]
        denom = (M / np.maximum(wsum, 1e-12)).sum(axis=1)
        w_new = S / np.maximum(denom, 1e-12)
        w_new = np.maximum(w_new, 1e-12)
        w_new /= w_new.sum()

        if np.max(np.abs(w_new - w)) < tol:
            w = w_new
            break
        w = w_new

    return pd.Series(w, index=names, dtype=float)


from types import SimpleNamespace

def _canon_surface(s: str) -> str:
    s = str(s)
    if s.startswith("èŠ") or "turf" in s.lower(): return "èŠ"
    if s.startswith("ãƒ€") or "dirt" in s.lower(): return "ãƒ€"
    return s

def _fallback_geom(course_id: str, surface: str, distance_m: int, layout: str, rail: str):
    """
    æœ€ä½é™ã®å±æ€§ã ã‘ã‚’æŒã¤ç°¡æ˜“ã‚¸ã‚ªãƒ¡ãƒˆãƒªï¼ˆPhysS1 Sprint1ã«å¿…è¦ãªåˆ†ã ã‘ï¼‰ã€‚
    â€» finish_grade_pct ã¯ä¸æ˜â‡’0%ã€start_to_first_turn_m ã¯ä¿å®ˆçš„ã« 800m
    """
    return SimpleNamespace(
        course_id=course_id,
        surface=surface,
        distance_m=int(distance_m),
        layout=layout,
        rail_state=rail,
        # Sprint1 ã§å‚ç…§ã™ã‚‹å±æ€§
        track_width_min_m=25.0,
        track_width_max_m=25.0,
        num_turns=2,
        start_to_first_turn_m=800.0,
        finish_grade_pct=0.0,
    )

def resolve_course_geom(course_id: str, surface: str, distance_m: int, layout: str, rail: str,
                        *, dist_tol: int = 300, step: int = 100, allow_fallback: bool = True):
    """
    1) æŒ‡å®šãã®ã¾ã¾ â†’ 2) æŸµã ã‘å¤‰æ›´ â†’ 3) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå¤‰æ›´ â†’ 4) è·é›¢Â±tolæ¢ç´¢
    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° allow_fallback=True ãªã‚‰ã€Œç°¡æ˜“ã‚¸ã‚ªãƒ¡ãƒˆãƒªã€ã‚’è¿”ã™ã€‚
    æˆ»ã‚Šå€¤: (layout_ok, rail_ok, geom, used_distance, is_fallback)
    """
    surface = _canon_surface(surface)
    used_distance = int(distance_m)

    try:
        g = get_course_geom(course_id, surface, int(distance_m), layout, rail)
        if g is not None:
            return layout, rail, g, used_distance, False
    except Exception:
        pass

    for r in ["A", "B", "C", "D", ""]:
        if r == rail: 
            continue
        try:
            g = get_course_geom(course_id, surface, int(distance_m), layout, r)
            if g is not None:
                return layout, r, g, used_distance, False
        except Exception:
            continue

    cand_layouts = (LAYOUT_OPTS.get(course_id) if 'LAYOUT_OPTS' in globals() else ["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"]) or ["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"]
    for lay in cand_layouts:
        if lay == layout:
            continue
        for r in ["A", "B", "C", "D", ""]:
            try:
                g = get_course_geom(course_id, surface, int(distance_m), lay, r)
                if g is not None:
                    return lay, r, g, used_distance, False
            except Exception:
                continue

    dlist = [distance_m]
    for d in range(step, dist_tol + step, step):
        dlist.extend([distance_m - d, distance_m + d])
    dlist = [int(d) for d in dlist if 800 <= d <= 3600]

    for d2 in dlist:
        for lay in cand_layouts:
            for r in ["A", "B", "C", "D", ""]:
                try:
                    g = get_course_geom(course_id, surface, int(d2), lay, r)
                    if g is not None:
                        return lay, r, g, int(d2), False
                except Exception:
                    continue

    if allow_fallback and surface == "èŠ":
        # â† æœ€çµ‚æ‰‹æ®µï¼šç°¡æ˜“ã‚¸ã‚ªãƒ¡ãƒˆãƒªã§ç¶šè¡Œï¼ˆäº¬éƒ½3000mã®ã‚ˆã†ãªæœªç™»éŒ²è·é›¢å¯¾ç­–ï¼‰
        g = _fallback_geom(course_id, surface, int(distance_m), layout, rail)
        return layout, rail, g, int(distance_m), True

    return None, None, None, None, False




# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ =====
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®šï¼ˆAUTOçµ±åˆï¼‰")
MODE = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ["AUTOï¼ˆæ¨å¥¨ï¼‰","æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰"], index=0, horizontal=True)

# === 2æ­³æˆ¦ãƒ¢ãƒ¼ãƒ‰ UI ===
try:
    TWOYO_AUTO = detect_2yo_race(s1, s0)  # s1=å‡ºèµ°è¡¨DF, s0=ãƒ¬ãƒ¼ã‚¹æƒ…å ±DF
except Exception:
    TWOYO_AUTO = False

TWOYO_MODE = False
USE_MC = True

with st.sidebar.expander("ğŸ¼ 2æ­³æˆ¦ãƒ¢ãƒ¼ãƒ‰", expanded=TWOYO_AUTO):
    TWOYO_MODE = st.checkbox(
        "2æ­³æˆ¦æœ€é©åŒ–ï¼ˆå±¥æ­´ä¾å­˜ã‚’å¼±ã‚ã‚‹ï¼‰",
        value=TWOYO_AUTO,
        help="Turn/Dist/å®‰å®šæ€§ãªã©å±¥æ­´ä¾å­˜ã®åŠ¹ãã‚’æŠ‘ãˆã€ã‚¹ãƒšã‚¯ãƒˆãƒ«/èª¿æ•™/å¹¾ä½•ã‚’é‡è¦–ã—ã¾ã™ã€‚"
    )
    USE_MC = st.checkbox(
        "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ï¼ˆPace/ç€é †ï¼‰ã‚’ä½¿ã†",
        value=not TWOYO_MODE,
        help="2æ­³æˆ¦ã§ã¯OFFæ¨å¥¨ã€‚OFFãªã‚‰PacePtsã‚’0å›ºå®šãƒ»ç€é †MCã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
    )

with st.sidebar.expander("ğŸ”° åŸºæœ¬", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
    agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
    agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
    agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)

with st.sidebar.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶", expanded=True):
    grade_opts = ["G1", "G2", "G3", "L", "OP", "3å‹ã‚¯ãƒ©ã‚¹"]
    TARGET_GRADE = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", grade_opts, index=grade_opts.index("OP"))
    TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ","ãƒ€"], index=0)
    TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", 1000, 3600, 1800, 100)
    
with st.sidebar.expander("ğŸ“ æœ¬ãƒ¬ãƒ¼ã‚¹å¹¾ä½•ï¼ˆã‚³ãƒ¼ã‚¹è¨­å®šï¼‰", expanded=True):
    VENUES = ["æœ­å¹Œ","å‡½é¤¨","ç¦å³¶","æ–°æ½Ÿ","æ±äº¬","ä¸­å±±","ä¸­äº¬","äº¬éƒ½","é˜ªç¥","å°å€‰"]
    COURSE_ID = st.selectbox("ç«¶é¦¬å ´", VENUES, index=VENUES.index("æ±äº¬"))

    LAYOUT_OPTS = {
        "æœ­å¹Œ":["å†…å›ã‚Š"], "å‡½é¤¨":["å†…å›ã‚Š"], "ç¦å³¶":["å†…å›ã‚Š"],
        "æ–°æ½Ÿ":["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"], "æ±äº¬":["å¤–å›ã‚Š"],
        "ä¸­å±±":["å†…å›ã‚Š","å¤–å›ã‚Š"], "ä¸­äº¬":["å¤–å›ã‚Š"],
        "äº¬éƒ½":["å†…å›ã‚Š","å¤–å›ã‚Š"], "é˜ªç¥":["å†…å›ã‚Š","å¤–å›ã‚Š"], "å°å€‰":["å†…å›ã‚Š"]
    }
    LAYOUT = st.selectbox("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ", LAYOUT_OPTS[COURSE_ID], key="layout_select")
   
with st.sidebar.expander("ğŸ“Š AR100èª¿æ•´", expanded=False):
    AR_MODE = st.radio(
        "AR100ã®è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰",
        [
            "é †ä½ãƒ™ãƒ¼ã‚¹ï¼ˆå¾“æ¥ï¼‰",
            "ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ï¼ˆZå€¤, å¾“æ¥ï¼‰",
            "å³æ ¼ï¼ˆerfï¼‹å°¾éƒ¨ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰",  # â† æ–°è¦
        ],
        index=2,
        help="å³æ ¼=ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å†…Zã‚’å¼·ãåœ§ç¸®ã—ã€ã•ã‚‰ã«ä¸Šä½ã®ã”ãä¸€éƒ¨ã ã‘ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§100ã«è¿‘ã¥ã"
    )

    # é †ä½ãƒ™ãƒ¼ã‚¹ç”¨
    AR_RANK_GAMMA = st.slider("é †ä½åœ§ç¸® Î³ï¼ˆå°=å·®ç¸®ã‚€ï¼‰", 0.60, 1.20, 0.85, 0.01)

    # å³æ ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”¨ï¼ˆæœ¬ä½“ï¼‰
    AR_STRICT_S = st.slider("å³ã—ã• sï¼ˆå¤§=ã•ã‚‰ã«å³ã—ã„ï¼‰", 0.60, 2.00, 1.10, 0.05)
    AR_BASE_A   = st.slider("åŸºç¤ãƒ¬ãƒ³ã‚¸ Aï¼ˆæŒ¯å¹…ï¼‰", 30.0, 48.0, 45.0, 0.5,
                            help="50Â±A*erf(...) ã®Â±Aã€‚A=45ã ã¨ãŠãŠã‚€ã­ 5ã€œ95 ãƒ¬ãƒ³ã‚¸")
    # ä¸ç¢ºå®Ÿæ€§ãƒšãƒŠãƒ«ãƒ†ã‚£
    AR_SHRINK_SIGMA = st.slider("ä¸ç¢ºå®Ÿæ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆÏƒå¯„ä¸ï¼‰", 0.0, 2.0, 0.6, 0.05)
    AR_SHRINK_NEFF  = st.slider("ãƒ‡ãƒ¼ã‚¿è–„ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆn_effå¯„ä¸ï¼‰", 0.0, 2.0, 0.8, 0.05)

    # å°¾éƒ¨ï¼ˆãƒˆãƒƒãƒ—ã ã‘100ã«å¯„ã›ã‚‹é€£ç¶šãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
    TAIL_THRESH = st.slider("å°¾éƒ¨ç™ºç«ã—ãã„å€¤ Tï¼ˆZï¼‰", 1.8, 3.5, 2.8, 0.1,
                            help="ã“ã®Zã‚’è¶…ãˆã‚‹ã¨ä¸ŠæŒ¯ã‚Œå´ã®100å¯„ã›ãŒã‚†ã£ãã‚ŠåŠ¹ãå§‹ã‚ã‚‹ï¼ˆä¾‹: 2.8ã€œ3.2ï¼‰")
    TAIL_TAU    = st.slider("å°¾éƒ¨ã®æŸ”ã‚‰ã‹ã• Ï„ï¼ˆå¤§=ãªã ã‚‰ã‹ï¼‰", 0.2, 1.2, 0.6, 0.05)
    TAIL_POW    = st.slider("å°¾éƒ¨ã®å³ã—ã• pï¼ˆå¤§=ã•ã‚‰ã«ãƒ¬ã‚¢ï¼‰", 1.0, 4.0, 2.0, 0.1)

# ===== ã“ã“ã‹ã‚‰ ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼ˆFFT+DTWï¼‰ã‚’æœ¬ç·šã«çµ±åˆ =====
with st.sidebar.expander("ğŸ“¡ ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨­å®š", expanded=True):
    spectral_weight_ui = st.slider("ã‚¹ãƒšã‚¯ãƒˆãƒ«é©åˆä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    templ_tol_m = st.slider("ãƒ†ãƒ³ãƒ—ãƒ¬è·é›¢è¨±å®¹å¹…(Â±m)", 50, 400, 100, 25)

# ===== ç‰©ç†ï¼ˆèª¿æ•™ï¼‰ãƒ–ãƒ­ãƒƒã‚¯ =====
with st.sidebar.expander("ğŸ‡ ç‰©ç†ï¼ˆèª¿æ•™ï¼‰", expanded=True):
    USE_PHYSICS = st.checkbox("ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã†ï¼ˆèª¿æ•™Ã—åŠ›å­¦ï¼‰", True)
    # ã‚¹ãƒšã‚¯ãƒˆãƒ« : ç‰©ç† ã®æ¯”ç‡ï¼ˆåˆæˆã®â€œé…åˆ†â€ï¼‰
    spec_phys_ratio = st.slider("ã‚¹ãƒšã‚¯ãƒˆãƒ« : ç‰©ç† ã®æ¯”ç‡", 0.0, 1.0, 0.6, 0.05)
    spec_ratio = float(spec_phys_ratio)
    phys_ratio = 1.0 - spec_ratio

    # ä»»æ„ã®åˆæœŸå€¤ï¼ˆåŠ¹ããŒè‰¯ã„å®Ÿæˆ¦å€¤ï¼‰
    Crr_wood = st.number_input("Crrï¼ˆè»¢ãŒã‚ŠæŠµæŠ—ï¼‰: ã‚¦ãƒƒãƒ‰", 0.0, 0.06, 0.020, 0.001, help="æ¨å¥¨: 0.020")
    Crr_hill = st.number_input("Crrï¼ˆè»¢ãŒã‚ŠæŠµæŠ—ï¼‰: å‚è·¯", 0.0, 0.06, 0.014, 0.001, help="æ¨å¥¨: 0.014")
    CdA      = st.number_input("CdAï¼ˆç©ºåŠ›ãƒ•ãƒ­ãƒ³ãƒˆ[mÂ²]ï¼‰", 0.2, 1.6, 0.80, 0.05, help="æ¨å¥¨: 0.8")
    rho_air  = st.number_input("ç©ºæ°—å¯†åº¦ Ï[kg/mÂ³]", 0.8, 1.5, 1.20, 0.01)
    Pmax_wkg = st.number_input("æœ€å¤§ç™ºæ®å‡ºåŠ› Pmax[W/kg]", 10.0, 30.0, 20.0, 0.5)
    Emax_jkg = st.number_input("å¯ç”¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ Emax[J/kg/800m]", 600.0, 4000.0, 1800.0, 50.0)
    half_life_train_days = st.slider("èª¿æ•™å¯„ä¸ã®åŠæ¸›æœŸï¼ˆæ—¥ï¼‰", 3, 60, 18, 1)


with st.sidebar.expander("âš–ï¸ è‡ªå‹•ãƒãƒ©ãƒ³ã‚µ", expanded=False):
    USE_AUTO_BALANCER = st.checkbox(
        "è‡ªå‹•ãƒãƒ©ãƒ³ã‚µã‚’æœ‰åŠ¹åŒ–ï¼ˆFinalRawã‚’å†æ¨™æº–åŒ–ã— Spec/Phys ã‚’è‡ªå‹•æ··åˆï¼‰",
        value=False,
        help="OFFæ¨å¥¨ã€‚ONã«ã™ã‚‹ã¨ç›¸å¯¾é †ä½ã ã‘ãŒåŠ¹ãã‚„ã™ããªã‚Šã€ç‰¹æ€§é‡ã¿ã®åŠ¹ããŒå°ã•ãè¦‹ãˆã¾ã™ã€‚"
    )

    # ç¾åœ¨ã®è¨­å®šã§æœ‰åŠ¹ãªæŸµã ã‘ã«çµã‚‹
    surface_ui = "èŠ" if TARGET_SURFACE == "èŠ" else "ãƒ€"
    dist_ui = int(TARGET_DISTANCE)

    valid_rails = []
    for r in ["A", "B", "C", "D", ""]:
        try:
            if get_course_geom(COURSE_ID, surface_ui, dist_ui, LAYOUT, r) is not None:
                valid_rails.append(r or "ï¼ˆæŒ‡å®šãªã—ï¼‰")
        except Exception:
            pass

    # å€™è£œãŒç„¡ã‘ã‚Œã°ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è‡ªå‹•åˆ‡æ›¿ã‚’è©¦ã™
    if not valid_rails:
        switched = False
        for lay2 in (LAYOUT_OPTS.get(COURSE_ID) or ["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"]):
            if lay2 == LAYOUT:
                continue
            vr2 = []
            for r in ["A","B","C","D",""]:
                try:
                    if get_course_geom(COURSE_ID, surface_ui, dist_ui, lay2, r) is not None:
                        vr2.append(r or "ï¼ˆæŒ‡å®šãªã—ï¼‰")
                except Exception:
                    pass
            if vr2:
                st.warning(f"é¸æŠãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã€{LAYOUT}ã€ã§ã¯ç™»éŒ²ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€{lay2}ã€ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸã€‚")
                st.session_state['layout_select'] = lay2
                st.rerun()
                switched = True
                break

        if not switched:
            valid_rails = ["ï¼ˆæŒ‡å®šãªã—ï¼‰"]
            st.caption("â€» ã“ã®è·é›¢ã§ã¯ç™»éŒ²ã•ã‚ŒãŸæŸµåŒºåˆ†ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¸Šã®ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã§åˆ©ç”¨å¯èƒ½ãªçµ„åˆã›ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # â† ã“ã“ã‹ã‚‰ã¯å¸¸ã«ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å‡ºã™ï¼ˆæœªå®šç¾©é˜²æ­¢ï¼‰
    rail_label = st.selectbox("ã‚³ãƒ¼ã‚¹åŒºåˆ†ï¼ˆA/B/C/Dï¼‰", valid_rails, index=0, key="rail_select")
    RAIL = "" if rail_label == "ï¼ˆæŒ‡å®šãªã—ï¼‰" else rail_label

    DEFAULT_VENUE_TURN = {'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦','ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'}
    _turn_default = DEFAULT_VENUE_TURN.get(COURSE_ID, 'å³')
    TARGET_TURN = st.radio("å›ã‚Š", ["å³","å·¦"],
                           index=(0 if _turn_default == "å³" else 1),
                           horizontal=True, key="turn_geom")

    TODAY_BAND = st.select_slider("é€šéå¸¯åŸŸï¼ˆæš«å®šï¼‰", options=["å†…","ä¸­","å¤–"], value="ä¸­", key="band_today")


with st.sidebar.expander("ğŸ§® ç‰©ç†(Sprint1)ã®é‡ã¿", expanded=True):
    PHYS_S1_GAIN = st.slider("PhysS1åŠ ç‚¹ã®å¼·ã•", 0.0, 3.0, 1.0, 0.1)

with st.sidebar.expander("ğŸ›  å®‰å®šåŒ–/è£œæ­£", expanded=True):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)
    
with st.sidebar.expander("ğŸ§© ç‰¹æ€§é‡ã¿ï¼ˆä»»æ„ï¼‰", expanded=False):
    # â”€â”€ æ€§åˆ¥ï¼ˆ0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    SEX_MALE  = st.slider("æ€§åˆ¥: ç‰¡ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")
    SEX_FEMA  = st.slider("æ€§åˆ¥: ç‰ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")
    SEX_GELD  = st.slider("æ€§åˆ¥: ã‚»ãƒ³ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")

    # â”€â”€ è„šè³ªï¼ˆ0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    STL_NIGE   = st.slider("è„šè³ª: é€ƒã’ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_SENKO  = st.slider("è„šè³ª: å…ˆè¡Œã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_SASHI  = st.slider("è„šè³ª: å·®ã—ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_OIKOMI = st.slider("è„šè³ª: è¿½è¾¼ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")

    # â”€â”€ å¹´é½¢ï¼ˆãƒ”ãƒ¼ã‚¯ã¯æ•´æ•°ã®ã¾ã¾ / æ¸›è¡°å¼·ã•ã¯0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    AGE_PEAK   = st.slider("å¹´é½¢ã®ãƒ”ãƒ¼ã‚¯ï¼ˆÂ±ã§æ¸›è¡°ï¼‰", 2, 8, 4)
    AGE_SLOPE  = st.slider("å¹´é½¢ã®æ¸›è¡°å¼·ã•", 0.0, 2.0, 0.5, 0.01, format="%.2f")

    # â”€â”€ æ ãƒã‚¤ã‚¢ã‚¹å¼·ã•ã‚‚0.00ã€œ2.00ã€0.01åˆ»ã¿ã«ï¼ˆæ–¹å‘ã¯ãã®ã¾ã¾ï¼‰â”€â”€
    WAKU_DIR   = st.radio("æ ãƒã‚¤ã‚¢ã‚¹æ–¹å‘", ["ãªã—","å†…æœ‰åˆ©","å¤–æœ‰åˆ©"], index=0, horizontal=True)
    WAKU_STR   = st.slider("æ ãƒã‚¤ã‚¢ã‚¹å¼·ã•", 0.0, 2.0, 1.0, 0.01, format="%.2f")

with st.sidebar.expander("ğŸ“ ç¢ºç‡æ ¡æ­£", expanded=False):
    do_calib = st.checkbox("ç­‰æ¸©å›å¸°ã§å‹ç‡ã‚’æ ¡æ­£", value=False)

with st.sidebar.expander("ğŸ› æ‰‹å‹•ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰", expanded=(MODE=="æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰")):
    besttime_w_manual = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿(æ‰‹å‹•)", 0.0, 2.0, 1.0)
    dist_bw_m_manual  = st.slider("è·é›¢å¸¯ã®å¹…[æ‰‹å‹•]", 50, 600, 200, 25)
    mc_beta_manual    = st.slider("PLæ¸©åº¦Î²(æ‰‹å‹•)", 0.3, 5.0, 1.4, 0.1)

with st.sidebar.expander("ğŸ–¥ è¡¨ç¤º", expanded=False):
    FULL_TABLE_VIEW = st.checkbox("å…¨é ­è¡¨ç¤ºï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç„¡ã—ï¼‰", True)
    MAX_TABLE_HEIGHT = st.slider("æœ€å¤§é«˜ã•(px)", 800, 10000, 5000, 200)
    SHOW_CORNER = st.checkbox("4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã‚’è¡¨ç¤º", False)


if st.button("ğŸ§ª PhysS1 ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"):
    surface_ui = "èŠ" if TARGET_SURFACE == "èŠ" else "ãƒ€"
    dist_ui = int(TARGET_DISTANCE)

    lay_ok, rail_ok, geom, used_d, is_fb = resolve_course_geom(COURSE_ID, surface_ui, dist_ui, LAYOUT, RAIL)
    st.write("geom (resolved):", geom)
    st.caption(
        f"layout: {LAYOUT} â†’ {lay_ok or 'â€”'} / rail: {RAIL or 'ï¼ˆæŒ‡å®šãªã—ï¼‰'} â†’ {rail_ok or 'ï¼ˆæŒ‡å®šãªã—ï¼‰'} / "
        f"distance: {dist_ui} â†’ {used_d or 'â€”'}" + (" ã€”fallback: ç°¡æ˜“ã‚¸ã‚ªãƒ¡ãƒˆãƒªã€•" if is_fb else "")
    )



    if geom is None:
        st.error("ã“ã®çµ„åˆã›ã®ã‚³ãƒ¼ã‚¹å¹¾ä½•ãŒæœªç™»éŒ²ã®ãŸã‚ PhysS1 ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚ä¸‹ã®ä¸€è¦§ã‹ã‚‰å­˜åœ¨ã™ã‚‹çµ„åˆã›ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")

        # è¿‘å‚è·é›¢ã§ã‚‚æ¢ã—ã¦ä¸€è¦§ã«å‡ºã™ï¼ˆÂ±300mï¼‰
        avail = []
        for d2 in range(max(800, dist_ui-300), min(3600, dist_ui+300)+1, 100):
            for lay in (LAYOUT_OPTS.get(COURSE_ID) or ["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"]):
                for r in ["A","B","C","D",""]:
                    try:
                        g = get_course_geom(COURSE_ID, surface_ui, int(d2), lay, r)
                        if g is not None:
                            avail.append({"è·é›¢[m]": d2, "ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ": lay, "æŸµ": (r or "ï¼ˆæŒ‡å®šãªã—ï¼‰")})
                    except Exception:
                        pass
        if avail:
            st.dataframe(pd.DataFrame(avail).drop_duplicates().sort_values(["è·é›¢[m]","ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ","æŸµ"]))
        else:
            st.info("â€» è¿‘å‚è·é›¢(Â±300m)ã§ã‚‚ç™»éŒ²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ¥ã®è·é›¢ãƒ»ã‚³ãƒ¼ã‚¹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        st.stop()
    # ã“ã“ã¾ã§æ¥ãŸã‚‰ geom ã‚ã‚Š â†’ å¾“æ¥é€šã‚Š DataFrame ä½œæˆ & å®Ÿè¡Œ
    ...


    # ã“ã“ã¾ã§æ¥ãŸã‚‰ geom ãŒã‚ã‚‹
    races_df_today_dbg = pd.DataFrame([{
        'race_id':'DBG',
        'course_id':COURSE_ID,
        'surface': surface_ui,
        'distance_m': dist_ui,
        'layout': lay_ok,
        'rail_state': rail_ok,
        'band': TODAY_BAND,
        'num_turns': 2
    }])

    try:
        out = add_phys_s1_features(races_df_today_dbg, group_cols=(), band_col="band", verbose=True)
        st.write("physåˆ—:", [c for c in out.columns if c.startswith("phys_")])
        st.dataframe(out)
    except Exception as e:
        st.error(f"PhysS1å¤±æ•—: {e}")



# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
topbar("Rikeiba", "â‘ Excelèª­è¾¼ â†’ â‘¡èª¿æ•´ â†’ â‘¢åˆ†æ â†’ â‘£çµæœå‡ºåŠ›ï¼ˆJSONï¼‰")
st.markdown("### Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰")
excel_file = st.file_uploader("Excelï¼ˆ.xlsxï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

st.subheader("ï¼ˆä»»æ„ï¼‰èª¿æ•™ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
wood_file = st.file_uploader("ã‚¦ãƒƒãƒ‰ãƒãƒƒãƒ—èª¿æ•™ï¼ˆ.xlsxï¼‰", type=['xlsx'], key="wood_x")
hill_file = st.file_uploader("å‚è·¯èª¿æ•™ï¼ˆ.xlsxï¼‰", type=['xlsx'], key="hill_x")


@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    # 3æšç›®ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€ï¼ˆç„¡ã‘ã‚Œã°ç©ºDFï¼‰
    try:
        s2 = pd.read_excel(xls, sheet_name=2)
    except Exception:
        s2 = pd.DataFrame()
    return s0, s1, s2

sheet0, sheet1, sheet2 = load_excel_bytes(excel_file.getvalue())


# ===== èª¿æ•™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†æ­£è¦åŒ– =====
def _read_train_xlsx(file, kind: str) -> pd.DataFrame:
    """
    èª¿æ•™Excelï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯ï¼‰ã‹ã‚‰ä¸‹è¨˜åˆ—ã‚’æŠ½å‡ºã—ã¦çµ±ä¸€ã™ã‚‹:
      - é¦¬å(str), æ—¥ä»˜(datetime64), å ´æ‰€(str/ä»»æ„), _kind('wood'|'hill'),
        _intensity(str/ä»»æ„), _lap_sec(list[4] of float)
    ãƒ»Lap1..Lap4 or Time1..Time4ï¼ˆåŒºé–“ï¼‰å„ªå…ˆã€‚ãªã‘ã‚Œã° 4F/3F/2F/1Fï¼ˆç´¯è¨ˆï¼‰â†’å·®åˆ†åŒ–ã€‚
    ãƒ»ã€Œ12.1-11.8-â€¦ã€ç­‰ã®æ–‡å­—åˆ—ã‚‚è¨±å®¹ã€‚
    ãƒ»æœ«å°¾NaNã¯æœ€å¾Œã®æœ‰åŠ¹å€¤ã§å‰æ–¹è£œå®Œã€‚
    """
    import numpy as np, pandas as pd, io, re

    if file is None:
        return pd.DataFrame()

    try:
        xls = pd.ExcelFile(io.BytesIO(file.getvalue()))
    except Exception:
        return pd.DataFrame()

    def _norm_cols(d: pd.DataFrame) -> pd.DataFrame:
        return d.rename(columns=lambda c: str(c).strip())

    name_pat = r'é¦¬å|åå‰|å‡ºèµ°é¦¬|horse|Horse'
    date_pat = r'æ—¥ä»˜|å¹´æœˆæ—¥|èª¿æ•™æ—¥|æ—¥æ™‚|å®Ÿæ–½æ—¥|æ¸¬å®šæ—¥|è¨˜éŒ²æ—¥|date|Date'

    def _to_num_like(s):
        ser = pd.Series(s).astype(str).str.replace(',', '').str.replace('\u3000', ' ').str.strip()
        num = ser.str.extract(r'([-+]?\d+(?:\.\d+)?)', expand=False)
        return pd.to_numeric(num, errors='coerce')

    def _smart_parse_date(col: pd.Series) -> pd.Series:
        s = col
        if pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):
            s = s.astype('Int64').astype(str).str.replace('<NA>', '')
        s = s.astype(str).str.strip()
        msk = s.str.match(r'^\d{8}$')
        out = pd.to_datetime(
            s.where(~msk, s.str.slice(0,4)+'-'+s.str.slice(4,6)+'-'+s.str.slice(6,8)),
            errors='coerce'
        )
        out = out.fillna(pd.to_datetime(s, errors='coerce'))
        return out

    def _split4_from_cum(t4, t3, t2, t1):
        vals = [np.nan, np.nan, np.nan, np.nan]
        vals[3] = t1
        if np.isfinite(t4) and np.isfinite(t3):
            vals[0] = t4 - t3
        elif np.isfinite(t4) and np.isfinite(t1):
            vals[0] = (t4 - t1) / 3.0
        if np.isfinite(t3) and np.isfinite(t2):
            vals[1] = t3 - t2
        elif np.isfinite(t3) and not np.isfinite(t2):
            base = t1 if np.isfinite(t1) else t3 / 3.0
            vals[1] = (t3 - base) / 2.0
        elif np.isfinite(t4) and np.isfinite(t1) and not np.isfinite(t3):
            vals[1] = (t4 - t1) / 3.0
        if np.isfinite(t2) and np.isfinite(t1):
            vals[2] = t2 - t1
        elif np.isfinite(t3) and not np.isfinite(t2):
            base = t1 if np.isfinite(t1) else t3 / 3.0
            vals[2] = (t3 - base) / 2.0
        elif np.isfinite(t4) and np.isfinite(t1):
            vals[2] = (t4 - t1) / 3.0

        arr = np.array(vals, float)
        if np.isnan(arr).any():
            good = np.where(np.isfinite(arr))[0]
            if good.size:
                last = arr[good[-1]]
                arr = np.where(np.isfinite(arr), arr, last)
        return arr

    def _parse_one(df_in: pd.DataFrame) -> pd.DataFrame:
        df = _norm_cols(df_in.copy())

        name_col = next((c for c in df.columns if re.search(name_pat, c, flags=re.I)), None)
        date_col = next((c for c in df.columns if re.search(date_pat, c, flags=re.I)), None)
        if name_col is None or date_col is None:
            return pd.DataFrame()

        df['é¦¬å'] = df[name_col].astype(str).str.replace('\u3000',' ').str.strip()
        df['æ—¥ä»˜'] = _smart_parse_date(df[date_col])

        seg_cols = []
        for i in range(1, 5):
            cand = [c for c in df.columns if re.search(fr'(^|\b)Lap{i}(\b|$)', c, flags=re.I)]
            if not cand:
                cand = [c for c in df.columns if re.search(fr'(^|\b)(L|Time){i}(\b|$)', c, flags=re.I)]
            seg_cols.append(cand)
        has_segment = all(len(x) > 0 for x in seg_cols)

        def _find_first(pats):
            for p in pats:
                cc = [c for c in df.columns if re.search(p, c, flags=re.I)]
                if cc:
                    return cc[0]
            return None

        c4 = _find_first([r'(^|\b)4\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'800\s*m', r'(^|[^0-9])4ï¼¦'])
        c3 = _find_first([r'(^|\b)3\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'600\s*m', r'(^|[^0-9])3ï¼¦'])
        c2 = _find_first([r'(^|\b)2\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'400\s*m', r'(^|[^0-9])2ï¼¦'])
        c1 = _find_first([r'(^|\b)1\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'200\s*m', r'(ï¾—ï½½ï¾„|æœ«|ä¸ŠãŒã‚Š).{0,3}1\s*[fï¼¦ï½†]'])
        has_cum = any([c4, c3, c2, c1])

        laps = np.full((len(df), 4), np.nan, float)

        if has_segment:
            for i in range(4):
                laps[:, i] = _to_num_like(df[seg_cols[i][0]]).to_numpy(float)
            if np.all(np.diff(laps, axis=1) < 0):
                t4, t3, t2, t1 = laps.T
                laps[:, 0] = t4 - t3
                laps[:, 1] = t3 - t2
                laps[:, 2] = t2 - t1
                laps[:, 3] = t1
        elif has_cum:
            T4 = _to_num_like(df[c4]) if c4 else pd.Series(np.nan, index=df.index)
            T3 = _to_num_like(df[c3]) if c3 else pd.Series(np.nan, index=df.index)
            T2 = _to_num_like(df[c2]) if c2 else pd.Series(np.nan, index=df.index)
            T1 = _to_num_like(df[c1]) if c1 else pd.Series(np.nan, index=df.index)
            arr = []
            for i in range(len(df)):
                arr.append(_split4_from_cum(T4.iloc[i], T3.iloc[i], T2.iloc[i], T1.iloc[i]))
            laps = np.vstack(arr)
        else:
            str_col = next(
                (c for c in df.columns if re.search(r'ãƒ©ãƒƒãƒ—|åŒºé–“|æ™‚è¨ˆ|ã‚¿ã‚¤ãƒ ', c) and df[c].astype(str).str.contains('-').any()),
                None
            )
            if str_col:
                def _parse_seq(s):
                    xs = re.findall(r'(\d+(?:\.\d+)?)', str(s))
                    xs = [float(x) for x in xs[:4]]
                    while len(xs) < 4:
                        xs.insert(0, np.nan)
                    return xs[-4:]
                laps = np.vstack(df[str_col].apply(_parse_seq).to_list()).astype(float)
            else:
                return pd.DataFrame()

        st_col = next((c for c in df.columns if re.search(r'å¼·å¼±|å†…å®¹|é¦¬ãªã‚Š|ä¸€æ¯|å¼·ã‚|ä»•æ›ã‘|è»½ã‚|æµã—', c)), None)
        intensity = df[st_col].astype(str) if st_col else pd.Series([''] * len(df), index=df.index)

        place_col = next((c for c in df.columns if re.search(r'å ´æ‰€|æ‰€å±|ãƒˆãƒ¬ã‚»ãƒ³|ç¾æµ¦|æ —æ±', c)), None)

        out = pd.DataFrame({
            'é¦¬å': df['é¦¬å'],
            'æ—¥ä»˜': df['æ—¥ä»˜'],
            'å ´æ‰€': (df[place_col].astype(str) if place_col else ""),
            '_kind': kind,
            '_intensity': intensity,
            '_lap_sec': list(laps)
        })

        mask = np.isfinite(laps).any(axis=1)
        out = out[mask].dropna(subset=['é¦¬å', 'æ—¥ä»˜'])
        return out

    frames = []
    for sh in xls.sheet_names:
        try:
            df0 = pd.read_excel(xls, sheet_name=sh, header=0)
            parsed = _parse_one(df0)
            if not parsed.empty:
                frames.append(parsed)
        except Exception:
            continue

    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()



# ===== ã‚³ãƒ¼ã‚¹æ–­é¢ï¼ˆå‚è·¯ã®å‚¾æ–œãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ =====
def _slope_profile(kind: str):
    # è¿”ã‚Šå€¤: list of (length_m, grade_perm) ã‚’è·é›¢é †ã«
    if kind == 'hill_ritto':
        # æ —æ±ï¼š300m 2.0â€°? â†’ 2.0%, ç¶šã„ã¦ 570m 3.5%, 100m 4.5%, 115m 1.25%
        # å˜ä½ã¯ã€Œ%ã€ï¼ 0.02 ãªã©
        return [(300, 0.020), (570, 0.035), (100, 0.045), (115, 0.0125)]
    elif kind == 'hill_miho':
        # ç¾æµ¦ï¼šä¸»è¨ˆæ¸¬800måŒºé–“ã®ä»£è¡¨å€¤ã¨ã—ã¦ 3.0% è¿‘å‚ã‚’ä¸€å®šã¨ã¿ãªã™ï¼ˆçµ‚ç«¯ä»˜è¿‘ã¯4.688%ï¼‰
        # 800mã‚’ 600m@3.0% + 200m@4.7% ã«
        return [(600, 0.030), (200, 0.04688)]
    else:
        return [(800, 0.000)]  # ãƒ•ãƒ©ãƒƒãƒˆ

def _intensity_gain(txt: str) -> float:
    s = str(txt)
    if re.search(r'ä¸€æ¯|å¼·ã‚', s): return 1.12
    if re.search(r'å¼·', s):       return 1.08
    if re.search(r'é¦¬ãªã‚Š', s):   return 0.94
    if re.search(r'è»½ã‚|æµã—', s):return 0.90
    return 1.00

def _seg_energy_wkg(v, a, g, grade, Crr, CdA, rho):
    # v[m/s], a[m/s2], ä½“é‡ã¯å¾Œã§æ›ã‘ã‚‹ã®ã§ W/kg ã¨ J/kg ã§è¿”ã™
    # æŠµæŠ—ï¼šè»¢ãŒã‚Š + é‡åŠ› + ç©ºåŠ›
    Fr = Crr * 9.80665 * np.cos(np.arctan(grade))            # â‰ˆ Crr*g
    Fg = g * grade                                           # g*sinÎ¸ â‰’ g*grade
    Fa = 0.5 * rho * CdA * v*v / 500.0                       # ä¿‚æ•°ç¸®å°ºï¼ˆç¾¤ã‚Œæµãƒ»å§¿å‹¢ç·©å’Œã®å¹³å‡çš„ä½æ¸›ï¼‰
    P_over_m = v*(Fr + Fg) + v*a                             # W/kg
    P_over_m += Fa*v/75.0                                    # ç©ºåŠ›å¯„ä¸ã‚’å¼±ã‚ã«ï¼ˆå®Ÿæ¸¬åˆã‚ã›ï¼‰
    return max(P_over_m, 0.0)

def _derive_training_metrics(train_df: pd.DataFrame,
                             s0_races: pd.DataFrame,
                             Crr_wood, Crr_hill, CdA, rho,
                             Pmax_wkg, Emax_jkg, half_life_days: int):
    """
    å„èª¿æ•™1æœ¬â†’ EAP[J/kg/m], PeakWkg[W/kg], EffReserve ã‚’è¨ˆç®—ã€‚
    é¦¬ä½“é‡ã¯ã€Œãã®èª¿æ•™æ—¥ã®ç›´è¿‘â€œå‰â€ãƒ¬ãƒ¼ã‚¹ã®é¦¬ä½“é‡ã€ã‚’å‚ç…§ï¼ˆãªã‘ã‚Œã°å…¨ä½“ä¸­å¤®å€¤ï¼‰ã€‚
    4FÃ—200mæƒ³å®šã€‚å‚è·¯ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«æ²¿ã£ã¦ grade ã‚’ä»˜ä¸ã€‚
    """
    if train_df.empty:
        return pd.DataFrame(columns=['é¦¬å','æ—¥ä»˜','EAP','PeakWkg','EffReserve','PhysicsZ'])

    # å‚ç…§ä½“é‡ãƒãƒƒãƒ—
    bw_map = {}
    if {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é¦¬ä½“é‡'}.issubset(s0_races.columns):
        tmp = s0_races[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é¦¬ä½“é‡']].dropna().sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'])
        for name, g in tmp.groupby('é¦¬å'):
            bw_map[name] = list(zip(g['ãƒ¬ãƒ¼ã‚¹æ—¥'].to_numpy(), g['é¦¬ä½“é‡'].to_numpy()))
    bw_median = float(pd.to_numeric(s0_races.get('é¦¬ä½“é‡', pd.Series([480])), errors='coerce')
                      .median(skipna=True) or 480.0)

    out = []
    for _, r in train_df.iterrows():
        name = r['é¦¬å']
        day  = pd.to_datetime(r['æ—¥ä»˜'])

        laps = np.array(r['_lap_sec'], dtype=float)
        if laps.size != 4 or np.isnan(laps).all():
            continue

        # æ¬ æã¯ã€Œæœ€å¾Œã«è¦³æ¸¬ã§ããŸåŒºé–“ã€ã§å‰æ–¹è£œå®Œ
        laps = np.where(np.isfinite(laps), laps, np.nan)
        if np.isnan(laps).any():
            good = np.where(np.isfinite(laps))[0]
            if good.size == 0:
                continue
            fill_val = float(laps[good[-1]])
            laps = np.where(np.isfinite(laps), laps, fill_val)

        # èª¿æ•™æ—¥ã®ç›´å‰ãƒ¬ãƒ¼ã‚¹ã®ä½“é‡ï¼ˆç„¡ã‘ã‚Œã°å…¨ä½“ä¸­å¤®å€¤ï¼‰
        bw = bw_median
        if name in bw_map:
            prev = [w for (d, w) in bw_map[name] if pd.to_datetime(d) <= day]
            if prev:
                bw = float(pd.to_numeric(prev[-1], errors='coerce') or bw_median)

        # é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ï¼ˆ200mã”ã¨ï¼‰
        d = 200.0
        v = d / laps
        a = np.diff(v, prepend=v[0]) / laps

        # ã‚³ãƒ¼ã‚¹ç¨®åˆ¥ï¼ˆå‚è·¯/ã‚¦ãƒƒãƒ‰ï¼‰ã¨å‹¾é…
        kind  = str(r.get('_kind', 'wood'))
        place = str(r.get('å ´æ‰€', ''))
        if kind == 'hill':
            is_miho  = bool(re.search(r'ç¾æµ¦|miho', place, flags=re.I))
            prof_key = 'hill_miho' if is_miho else 'hill_ritto'
            prof = _slope_profile(prof_key)

            grades = []
            remain = 800.0
            for L, G in prof:
                take = min(L, remain)
                nseg = int(round(take / 200.0))
                grades += [G] * max(1, nseg)
                remain -= take
                if remain <= 0:
                    break
            if not grades:
                grades = [0.03] * 4
            while len(grades) < 4:
                grades.append(grades[-1])
            grade = np.array(grades[:4], float)
            Crr = Crr_hill
        else:
            grade = np.zeros(4, float)
            Crr   = Crr_wood

        # å¼·å¼±ã«ã‚ˆã‚‹ä¿‚æ•°
        gain = _intensity_gain(r.get('_intensity', ''))

        # åŒºé–“ã‚ãŸã‚Šã®å‡ºåŠ›å¯†åº¦ï¼ˆW/kgï¼‰
        P = np.array([_seg_energy_wkg(v[i], a[i], 9.80665, grade[i], Crr, CdA, rho) * gain
                      for i in range(4)], float)

        # æŒ‡æ¨™
        PeakWkg    = float(P.max())
        work_jkg   = float(np.sum(P * laps))               # J/kg
        EAP        = float(work_jkg / 800.0)               # J/kg/m
        EffReserve = float(max(0.0, Emax_jkg - work_jkg)) / Emax_jkg  # 0..1

        out.append({'é¦¬å': name, 'æ—¥ä»˜': day,
                    'EAP': EAP, 'PeakWkg': PeakWkg, 'EffReserve': EffReserve})

    df = pd.DataFrame(out)
    if df.empty:
        return df

    # ç›´è¿‘é‡ã¿ä»˜ã‘ â†’ PhysicsZ
    df = df.sort_values(['é¦¬å','æ—¥ä»˜'])
    today = pd.Timestamp.today()
    df['_w'] = 0.5 ** ((today - df['æ—¥ä»˜']).dt.days.clip(lower=0) / float(max(1, half_life_days)))

    agg = (df.groupby('é¦¬å')
             .apply(lambda g: pd.Series({
                 'EAP':        np.average(g['EAP'],        weights=g['_w']),
                 'PeakWkg':    np.average(g['PeakWkg'],    weights=g['_w']),
                 'EffReserve': np.average(g['EffReserve'], weights=g['_w']),
             }))
             .reset_index())

    # ã€Œå°ã•ã„ã»ã©è‰¯ã„ã€EAP ã‚’åè»¢ã—ã¦ZåŒ–ï¼ˆå¹³å‡50, Ïƒ10ï¼‰
    agg['PhysicsCore'] = -pd.to_numeric(agg['EAP'], errors='coerce')
    mu = float(agg['PhysicsCore'].mean())
    sd = float(agg['PhysicsCore'].std(ddof=0) or 1.0)
    agg['PhysicsZ'] = (agg['PhysicsCore'] - mu) / sd * 10 + 50

    return agg[['é¦¬å','EAP','PeakWkg','EffReserve','PhysicsZ']]


# ===== åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè»½é‡ï¼‰ =====

def _norm_col(s: str) -> str:
    s=str(s).strip(); s=re.sub(r'\s+','',s)
    return s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')).replace('ï¼ˆ','(').replace('ï¼‰',')')

def _auto_pick(df, pats):
    cmap={c:_norm_col(c) for c in df.columns}
    for c, n in cmap.items():
        for p in pats:
            if re.search(p, n, flags=re.I):
                return c
    return None

def _map_ui(df, patterns, required, title, key_prefix):
    cols = list(df.columns)
    auto = {k: _auto_pick(df, pats) for k, pats in patterns.items()}
    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=False):
        mapping = {}
        for k, _ in patterns.items():
            options = ['<æœªé¸æŠ>'] + cols
            default = auto.get(k) if auto.get(k) in cols else '<æœªé¸æŠ>'
            mapping[k] = st.selectbox(k, options, index=options.index(default), key=f"map:{key_prefix}:{k}")
    for k in required:
        if mapping.get(k, '<æœªé¸æŠ>') == '<æœªé¸æŠ>':
            st.error(f"{title} å¿…é ˆåˆ—ãŒæœªé¸æŠ: {k}")
            st.stop()
    return {k: (v if v != '<æœªé¸æŠ>' else None) for k, v in mapping.items()}

# ã“ã“ã« PCI / PCI3 / Ave-3F ã‚‚æ‹¾ã†ã‚­ãƒ¼ã‚’è¿½åŠ 
# --- â‘  åˆ—åã®æ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€F/ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€ï¼‰ã‚’å…ˆã« ---
def _normalize_cols(df):
    trans = str.maketrans({
        'ï¼':'0','ï¼‘':'1','ï¼’':'2','ï¼“':'3','ï¼”':'4','ï¼•':'5','ï¼–':'6','ï¼—':'7','ï¼˜':'8','ï¼™':'9',
        'ï¼¦':'F','ï½†':'f','ãƒ¼':'-','ï¼':'-','ã€€':' '
    })
    out = df.copy()
    out.columns = [str(c).translate(trans).strip() for c in out.columns]
    return out

sheet0 = _normalize_cols(sheet0)

# === sheet0ï¼ˆéå»èµ°ï¼‰ã‚’å…ˆã«æ­£è¦åŒ–ã—ã¦å¿…é ˆåˆ—ã‚’ä½œã‚‹ ===
def _normalize_sheet0(df: pd.DataFrame) -> pd.DataFrame:
    s = df.copy()

    # 3) èµ°ç ´ã‚¿ã‚¤ãƒ  â†’ ç§’ï¼ˆä¾‹: "1:33.5" / "33.5"ï¼‰
    if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' not in s.columns and 'èµ°ç ´ã‚¿ã‚¤ãƒ ' in s.columns:
        def _to_sec(x):
            if pd.isna(x): return pd.NA
            t = str(x).strip()
            # mm:ss.s / m:ss.s / ss.s ã«å¯¾å¿œ
            if ':' in t:
                mm, ss = t.split(':', 1)
                try:
                    return float(ss) + 60.0 * float(mm)
                except:
                    return pd.NA
            try:
                return float(t)
            except:
                return pd.NA
        s['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = s['èµ°ç ´ã‚¿ã‚¤ãƒ '].map(_to_sec)

    if 'ãƒ¬ãƒ¼ã‚¹æ—¥' in s.columns:
        ser = s['ãƒ¬ãƒ¼ã‚¹æ—¥']
        if pd.api.types.is_numeric_dtype(ser):
            # Excelã‚·ãƒªã‚¢ãƒ«å¯¾ç­–
            s['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(ser, unit='d', origin='1899-12-30', errors='coerce')
        else:
            s['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(ser, errors='coerce')
        # tz ç„¡ã—ã«çµ±ä¸€ï¼ˆæ··åœ¨å¯¾ç­–ï¼‰
        try:  s['ãƒ¬ãƒ¼ã‚¹æ—¥'] = s['ãƒ¬ãƒ¼ã‚¹æ—¥'].dt.tz_convert(None)
        except: pass
        try:  s['ãƒ¬ãƒ¼ã‚¹æ—¥'] = s['ãƒ¬ãƒ¼ã‚¹æ—¥'].dt.tz_localize(None)
        except: pass

    return s
# ã“ã“ã§æ­£è¦åŒ–
sheet0 = _normalize_sheet0(sheet0)

# ï¼ˆä»¥é™ã¯ãã®ã¾ã¾å›ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã§OKï¼‰
PAT_S0 = {
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥':[r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜(?!S)|å¹´æœˆæ—¥|æ–½è¡Œæ—¥|é–‹å‚¬æ—¥'],
    'ç«¶èµ°å':[r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å':[r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°':[r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †':[r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ':[r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3F'],
    'ä¸Š3Fé †ä½':[r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’':[r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼|4è§’é€šéé †'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’':[r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢':[r'è·é›¢'],
    'èŠãƒ»ãƒ€':[r'èŠ.?ãƒ».?ãƒ€|èŠãƒ€|ã‚³ãƒ¼ã‚¹|é¦¬å ´ç¨®åˆ¥|Surface'],
    'é¦¬å ´':[r'é¦¬å ´(?!.*æŒ‡æ•°)|é¦¬å ´çŠ¶æ…‹'],
    'å ´å':[r'å ´å|å ´æ‰€|ç«¶é¦¬å ´|é–‹å‚¬(åœ°|å ´|å ´æ‰€)'],
    'PCI':[r'\bPCI(?!G)|ï¼°ï¼£ï¼©'],
    'PCI3':[r'\bPCI3\b|ï¼°ï¼£ï¼©3'],
    'Ave-3F':[r'Ave[-_]?3F|å¹³å‡.*3F'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']

MAP_S0 = {k: _auto_pick(sheet0, v) for k,v in PAT_S0.items()}
missing = [k for k in REQ_S0 if MAP_S0.get(k) is None or sheet0[MAP_S0[k]].isna().all()]
if missing:
    MAP_S0 = _map_ui(sheet0, PAT_S0, REQ_S0, 'sheet0ï¼ˆéå»èµ°ï¼‰', 's0')

s0 = pd.DataFrame({k: sheet0[col] for k,col in MAP_S0.items() if col in sheet0.columns})



# --- â‘£ s0 ã®ä½œæˆï¼ˆåˆæˆåˆ—ã‚’ã“ã“ã§å®Ÿä½“åŒ–ï¼‰ ---
s0 = pd.DataFrame()
for k, col in MAP_S0.items():
    if col == '__SYNTH_YMD__':
        y = pd.to_numeric(sheet0['å¹´'], errors='coerce').astype('Int64')
        m = pd.to_numeric(sheet0['æœˆ'], errors='coerce').astype('Int64')
        d = pd.to_numeric(sheet0['æ—¥'], errors='coerce').astype('Int64')
        s0['ãƒ¬ãƒ¼ã‚¹æ—¥'] = pd.to_datetime(
            y.astype(str) + '-' + m.astype(str) + '-' + d.astype(str),
            errors='coerce'
        )
    elif col and col in sheet0.columns:
        s0[k] = sheet0[col]

# --- â‘¤ èµ°ç ´ã‚¿ã‚¤ãƒ  â†’ ç§’ ã«æ­£è¦åŒ–ï¼ˆå…ƒãŒ mm:ss.s æ–‡å­—åˆ—æƒ³å®šï¼‰ ---
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' not in s0.columns and 'èµ°ç ´ã‚¿ã‚¤ãƒ ' in sheet0.columns:
    import re, numpy as np
    def _to_sec(x):
        s = str(x)
        m = re.match(r'(?:(\d+):)?(\d+)(?:\.(\d+))?$', s)
        if not m: return np.nan
        mm = int(m.group(1) or 0)
        ss = int(m.group(2) or 0)
        frac = float('0.' + (m.group(3) or '0'))
        return mm*60 + ss + frac
    s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = pd.to_numeric(sheet0['èµ°ç ´ã‚¿ã‚¤ãƒ '], errors='coerce')
    mask = s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].isna()
    if mask.any():
        s0.loc[mask, 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'] = sheet0.loc[mask, 'èµ°ç ´ã‚¿ã‚¤ãƒ '].map(_to_sec)

# --- â‘¥ å‹ã®æ•´å‚™ï¼ˆæ•°å€¤åŒ–ã§ãã‚‹ã‚‚ã®ï¼‰ ---
for col in ['é ­æ•°','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½']:
    if col in s0.columns:
        s0[col] = pd.to_numeric(s0[col], errors='coerce')


# ã‚·ãƒ¼ãƒˆ1
PAT_S1={
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª':[r'è„šè³ª'],
    'å‹ç‡':[r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡':[r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡':[r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ':[r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1=['é¦¬å','æ ','ç•ª']  # â† æ€§åˆ¥/å¹´é½¢ã¯å¿…é ˆã‹ã‚‰å¤–ã™
MAP_S1 = {k: _auto_pick(sheet1, v) for k,v in PAT_S1.items()}
miss1=[k for k in REQ_S1 if MAP_S1.get(k) is None]
if miss1:
    MAP_S1=_map_ui(sheet1, PAT_S1, REQ_S1, 'sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰', 's1')

s1=pd.DataFrame()
for k, col in MAP_S1.items():
    if col and col in sheet1.columns:
        s1[k]=sheet1[col]

for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in s1: s1[c]=pd.to_numeric(s1[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in s1: s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']=s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# å…ˆé ­ç©ºè¡Œ/ç©ºé¦¬åé™¤å»
s1 = s1.replace(r'^\s*$', np.nan, regex=True).dropna(how='all')
if 'é¦¬å' in s1:
    s1['é¦¬å']=s1['é¦¬å'].astype(str).str.replace('\u3000',' ').str.strip()
    s1=s1[s1['é¦¬å'].ne('')]
s1=s1.reset_index(drop=True)

# === NEW: sheet2 ã‹ã‚‰ æ€§åˆ¥/å¹´é½¢ ã‚’è£œå®Œ ===
if sheet2 is not None and isinstance(sheet2, pd.DataFrame) and not sheet2.empty:
    PAT_S2 = {
        'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬|Horse|horse'],
        'æ€§åˆ¥':[r'æ€§åˆ¥'],
        'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢']
    }
    MAP_S2 = {k: _auto_pick(sheet2, v) for k, v in PAT_S2.items()}
    s2 = pd.DataFrame()
    for k, col in MAP_S2.items():
        if col and col in sheet2.columns:
            s2[k] = sheet2[col]

    if not s2.empty and 'é¦¬å' in s2.columns:
        s2['é¦¬å'] = s2['é¦¬å'].astype(str).str.replace('\u3000', ' ').str.strip()
        if 'å¹´é½¢' in s2.columns:
            s2['å¹´é½¢'] = pd.to_numeric(s2['å¹´é½¢'], errors='coerce')

        s1 = s1.merge(s2[['é¦¬å','æ€§åˆ¥','å¹´é½¢']].drop_duplicates('é¦¬å'), on='é¦¬å', how='left', suffixes=('', '_s2'))
        if 'æ€§åˆ¥_s2' in s1.columns:
            s1['æ€§åˆ¥'] = s1['æ€§åˆ¥'].where(s1['æ€§åˆ¥'].astype(str).str.strip().ne(''), s1['æ€§åˆ¥_s2'])
            s1.drop(columns=['æ€§åˆ¥_s2'], inplace=True)
        if 'å¹´é½¢_s2' in s1.columns:
            s1['å¹´é½¢'] = s1['å¹´é½¢'].fillna(s1['å¹´é½¢_s2'])
            s1.drop(columns=['å¹´é½¢_s2'], inplace=True)


# è„šè³ªã‚¨ãƒ‡ã‚£ã‚¿
if 'è„šè³ª' not in s1.columns: s1['è„šè³ª']=''
if 'æ–¤é‡' not in s1.columns: s1['æ–¤é‡']=np.nan
if 'é¦¬ä½“é‡' not in s1.columns: s1['é¦¬ä½“é‡']=np.nan

st.subheader("é¦¬ä¸€è¦§ï¼ˆå¿…è¦ãªã‚‰è„šè³ª/æ–¤é‡/ä½“é‡ã‚’èª¿æ•´ï¼‰")

def auto_style_from_history(df: pd.DataFrame, n_recent=5, hl_days=180):
    # å¿…é ˆåˆ—ãŒãªã‘ã‚Œã°ç©ºã§è¿”ã™ï¼ˆè½ã¡ãªã„ã‚ˆã†ã«ï¼‰
    need = {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
    if not need.issubset(df.columns):
        return pd.DataFrame({'é¦¬å': [], 'æ¨å®šè„šè³ª': []})

    # ä»»æ„åˆ—ï¼ˆä¸Š3Fé †ä½ï¼‰ã¯å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰é¸ã¶
    base_cols = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’']
    if 'ä¸Š3Fé †ä½' in df.columns:
        base_cols.append('ä¸Š3Fé †ä½')

    t = (
        df[base_cols]
        .dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'])
        .copy()
    )

    # ä¸¦ã¹æ›¿ãˆã¨æœ€è¿‘nä»¶
    t = t.sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    t['_rn'] = t.groupby('é¦¬å').cumcount() + 1
    t = t[t['_rn'] <= n_recent].copy()

    today = pd.Timestamp.today()
    t['_days'] = (today - pd.to_datetime(t['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    t['_w'] = 0.5 ** (t['_days'] / float(hl_days))

    # 4è§’ä½ç½®â†’å…ˆè¡Œåº¦ï¼ˆ0ã€œ1ï¼‰
    denom = (pd.to_numeric(t['é ­æ•°'], errors='coerce') - 1).replace(0, np.nan)
    pos_ratio = (pd.to_numeric(t['é€šé4è§’'], errors='coerce') - 1) / denom
    pos_ratio = pos_ratio.clip(0, 1).fillna(0.5)

    # ä¸ŠãŒã‚Šé †ä½ãŒã‚ã‚Œã°çµ‚ã„è„šå¯„ä¸ã€ãªã‘ã‚Œã°0
    if 'ä¸Š3Fé †ä½' in t.columns:
        ag = pd.to_numeric(t['ä¸Š3Fé †ä½'], errors='coerce')
        close = ((3.5 - ag) / 3.5).clip(0, 1).fillna(0.0)
    else:
        close = pd.Series(0.0, index=t.index)

    # ãƒ­ã‚¸ãƒƒãƒˆ
    b = {'é€ƒã’': -1.2, 'å…ˆè¡Œ': 0.6, 'å·®ã—': 0.3, 'è¿½è¾¼': -0.7}
    t['L_é€ƒã’'] = b['é€ƒã’'] + 1.6*(1 - pos_ratio) - 1.2*close
    t['L_å…ˆè¡Œ'] = b['å…ˆè¡Œ'] + 1.1*(1 - pos_ratio) - 0.1*close
    t['L_å·®ã—'] = b['å·®ã—'] + 1.1*(pos_ratio)     + 0.9*close
    t['L_è¿½è¾¼'] = b['è¿½è¾¼'] + 1.6*(pos_ratio)     + 0.5*close

    # é¦¬ã”ã¨é‡ã¿å¹³å‡ â†’ softmax â†’ æœ€é »è„šè³ª
    rows = []
    for name, g in t.groupby('é¦¬å'):
        w = g['_w'].to_numpy(); sw = w.sum()
        if sw <= 0: 
            continue
        vec = np.array([
            float((g['L_é€ƒã’']*w).sum()/sw),
            float((g['L_å…ˆè¡Œ']*w).sum()/sw),
            float((g['L_å·®ã—']*w).sum()/sw),
            float((g['L_è¿½è¾¼']*w).sum()/sw),
        ])
        vec = vec - vec.max()
        p = np.exp(vec); p /= p.sum()
        rows.append([name, STYLES[int(np.argmax(p))]])
    return pd.DataFrame(rows, columns=['é¦¬å','æ¨å®šè„šè³ª'])

pred_style = auto_style_from_history(s0.copy())

s1['è„šè³ª']=s1['è„šè³ª'].map(normalize_style)
if not pred_style.empty:
    s1=s1.merge(pred_style, on='é¦¬å', how='left')
    s1['è„šè³ª']=s1['è„šè³ª'].where(s1['è„šè³ª'].astype(str).str.strip().ne(''), s1['æ¨å®šè„šè³ª'])
    s1.drop(columns=['æ¨å®šè„šè³ª'], inplace=True)

H = (lambda n: int(min(MAX_TABLE_HEIGHT, 38 + 35*max(1,int(n)) + 28)) if FULL_TABLE_VIEW else 460)
edit = st.data_editor(
    s1[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
    column_config={
        'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
        'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
        'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1),
    },
    use_container_width=True,
    num_rows='static',
    height=H(len(s1)),
    hide_index=True,
)
horses = edit.copy()

# === NEW: sheet0 ã‹ã‚‰ å‹ç‡/é€£å¯¾ç‡/è¤‡å‹ç‡ ã‚’è‡ªå‹•é›†è¨ˆï¼ˆå±¥æ­´ãƒ™ãƒ¼ã‚¹ï¼‰ ===
rate_src = s0.dropna(subset=['é¦¬å','ç¢ºå®šç€é †']).copy()
rate_src['ç¢ºå®šç€é †'] = pd.to_numeric(rate_src['ç¢ºå®šç€é †'], errors='coerce')
rate_src = rate_src[rate_src['ç¢ºå®šç€é †'] >= 1]

hist_rate = pd.DataFrame(columns=['é¦¬å','å‹ç‡%_HIST','é€£å¯¾ç‡%_HIST','è¤‡å‹ç‡%_HIST','å‡ºèµ°æ•°_HIST'])
if not rate_src.empty:
    grp = rate_src.groupby('é¦¬å')['ç¢ºå®šç€é †']
    hist_rate = pd.DataFrame({
        'å‹ç‡%_HIST': (grp.apply(lambda s: (s == 1).mean()) * 100.0).round(2),
        'é€£å¯¾ç‡%_HIST': (grp.apply(lambda s: (s <= 2).mean()) * 100.0).round(2),
        'è¤‡å‹ç‡%_HIST': (grp.apply(lambda s: (s <= 3).mean()) * 100.0).round(2),
        'å‡ºèµ°æ•°_HIST': grp.size(),
    }).reset_index()

# horses ã«å±¥æ­´ç‡ã‚’ãƒãƒ¼ã‚¸ï¼ˆæ—¢å­˜ã® sheet1 ã®æ•°å€¤ã¯ä¿æŒã—ã€ã“ã¡ã‚‰ã¯åˆ¥åã§è¿½åŠ ï¼‰
if not hist_rate.empty:
    horses = horses.merge(hist_rate, on='é¦¬å', how='left')

# ===== å…¥åŠ›ãƒã‚§ãƒƒã‚¯ =====
problems=[]
for c in ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']:
    if c not in s0.columns: problems.append(f"sheet0 å¿…é ˆåˆ—ãŒä¸è¶³: {c}")
if 'é€šé4è§’' in s0.columns and 'é ­æ•°' in s0.columns:
    tmp=s0[['é€šé4è§’','é ­æ•°']].dropna()
    if len(tmp)>0 and ((tmp['é€šé4è§’']<1)|(tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
        problems.append('sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–')
if problems:
    st.warning("å…¥åŠ›ãƒã‚§ãƒƒã‚¯:\n- "+"\n- ".join(problems))

# ===== ãƒãƒ¼ã‚¸ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    s0.drop(columns=[dup], errors='ignore', inplace=True)
df = s0.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')

# ===== 1èµ°ã‚¹ã‚³ã‚¢ =====
CLASS_PTS={'G1':10,'G2':8,'G3':6,'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5,'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}

def normalize_grade_text(x: str|None) -> str|None:
    if x is None or (isinstance(x,float) and np.isnan(x)): return None
    s=str(x).translate(_fw)
    s=(s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
        .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s=re.sub(r'G\s*III','G3',s,flags=re.I)
    s=re.sub(r'G\s*II','G2',s,flags=re.I)
    s=re.sub(r'G\s*I','G1',s,flags=re.I)
    s=re.sub(r'ï¼ªï¼°ï¼®','Jpn',s,flags=re.I)
    s=re.sub(r'JPN','Jpn',s,flags=re.I)
    s=re.sub(r'Jpn\s*III','Jpn3',s,flags=re.I)
    s=re.sub(r'Jpn\s*II','Jpn2',s,flags=re.I)
    s=re.sub(r'Jpn\s*I','Jpn1',s,flags=re.I)
    m=re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def class_points(row) -> int:
    g=normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row: g=normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name=str(row.get('ã‚¯ãƒ©ã‚¹å',''))+' '+str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df.columns:
    st.error('ãƒ¬ãƒ¼ã‚¹æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'); st.stop()

_df = df.copy()

def calc_score(r):
    gpt = class_points(r)
    # åŸºæœ¬ï¼šç€é †é€†è»¢ãƒã‚¤ãƒ³ãƒˆ + å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹Î»
    base = gpt * (pd.to_numeric(r['é ­æ•°'], errors='coerce') + 1 - pd.to_numeric(r['ç¢ºå®šç€é †'], errors='coerce'))
    base = float(base) if np.isfinite(base) else 0.0
    base += float(lambda_part) * gpt

    # é‡è³ãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼é€£å‹•ï¼‰
    gtxt = normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))
    bonus_grade = int(grade_bonus) if gtxt in ['G1','G2','G3'] else 0

    # ä¸ŠãŒã‚Šãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼é€£å‹•ï¼‰
    ao = pd.to_numeric(r.get('ä¸Š3Fé †ä½', np.nan), errors='coerce')
    if ao == 1:    bonus_agari = int(agari1_bonus)
    elif ao == 2:  bonus_agari = int(agari2_bonus)
    elif ao == 3:  bonus_agari = int(agari3_bonus)
    else:          bonus_agari = 0

    return base + bonus_grade + bonus_agari

_df['score_raw'] = _df.apply(calc_score, axis=1)

if _df['score_raw'].max()==_df['score_raw'].min():
    _df['score_norm']=50.0
else:
    _df['score_norm'] = (_df['score_raw'] - _df['score_raw'].min()) / (_df['score_raw'].max()-_df['score_raw'].min())*100


now = pd.Timestamp.today()
half_life_m_default = 6.0
_race_dates = pd.to_datetime(_df['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
try:
    if _race_dates.dt.tz is not None:
        _race_dates = _race_dates.dt.tz_convert(None)
except (TypeError, AttributeError):
    try:
        _race_dates = _race_dates.dt.tz_localize(None)
    except (TypeError, AttributeError):
        pass
_df['_days_ago'] = (now - _race_dates).dt.days
_df['_w'] = 0.5 ** (_df['_days_ago'] / (half_life_m*30.4375 if half_life_m>0 else half_life_m_default*30.4375))

# ===== A) ãƒ¬ãƒ¼ã‚¹å†…ãƒ‡ãƒ•ãƒ¬ãƒ¼ãƒˆ =====
def _make_race_id_for_hist(dfh: pd.DataFrame) -> pd.Series:
    return pd.to_datetime(dfh['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d').fillna('00000000') + '_' + dfh['ç«¶èµ°å'].astype(str).fillna('')

_df['rid_hist'] = _make_race_id_for_hist(_df)
med = _df.groupby('rid_hist')['score_norm'].transform('median')
_df['score_adj'] = _df['score_norm'] - med

# ===== ãƒ­ãƒã‚¹ãƒˆæ¨™æº–åŒ–ï¼ˆé€±Ã—ã‚¯ãƒ©ã‚¹ï¼šmedian/MADï¼‰=====
# é€±ã‚­ãƒ¼ï¼ˆW-MONï¼‰ã¨ã‚¯ãƒ©ã‚¹ã‚­ãƒ¼ï¼ˆG1/G2/G3â€¦ï¼‰
_df['WeekKey'] = pd.to_datetime(_df['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.to_period('W-MON').astype(str)

def _class_key_row(row):
    g = normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row:
        g = normalize_grade_text(row.get('ç«¶èµ°å'))
    return g or 'OTHER'

_df['ClassKey'] = _df.apply(_class_key_row, axis=1)

# group: é€±Ã—ã‚¯ãƒ©ã‚¹ã§ median/MAD
def _mad(s):
    s = pd.to_numeric(s, errors='coerce')
    med = s.median()
    mad = np.median(np.abs(s - med))
    return med, mad

grp = _df.groupby(['WeekKey','ClassKey'])['score_adj']
med = grp.transform('median')
mad = grp.transform(lambda x: np.median(np.abs(pd.to_numeric(x, errors='coerce') - np.nanmedian(pd.to_numeric(x, errors='coerce')))))
mad = mad.replace(0, np.nan)  # ã‚¼ãƒ­å‰²å›é¿
rb_z = (pd.to_numeric(_df['score_adj'], errors='coerce') - med) / (1.4826 * mad)

# åå·®å€¤ï¼ˆ50Â±10ï¼‰ã«å†™åƒï¼ˆæ¬ æã¯å…¨ä½“ã§åŸ‹ã‚ï¼‰
rb_t = 50.0 + 10.0 * rb_z
rb_t = rb_t.fillna(50.0 + 10.0 * ((pd.to_numeric(_df['score_adj'], errors='coerce') - _df['score_adj'].median()) /
                                  (1.4826 * np.median(np.abs(pd.to_numeric(_df['score_adj'], errors='coerce') - _df['score_adj'].median()) + 1e-9))))

_df['ScoreT_RB'] = rb_t.clip(0, 100)  # è¡¨ç¤ºå®‰å®šã®ãŸã‚ 0â€“100 ã«ã‚¯ãƒªãƒƒãƒ—

# ===== å³/å·¦å›ã‚Šï¼ˆæ¨å®šï¼‰ =====
DEFAULT_VENUE_TURN = {'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦','ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'}
def infer_turn_row(row):
    # ã¾ãšå ´åã§åˆ¤å®š
    venue = str(row.get('å ´å','')).strip()
    if venue in DEFAULT_VENUE_TURN:
        return DEFAULT_VENUE_TURN[venue]
    # æ¬¡ã«ç«¶èµ°åã‹ã‚‰æ¨å®šï¼ˆå¾“æ¥äº’æ›ï¼‰
    name = str(row.get('ç«¶èµ°å',''))
    for v, t in DEFAULT_VENUE_TURN.items():
        if v in name:
            return t
    return np.nan
if 'å›ã‚Š' not in _df.columns:
    _df['å›ã‚Š'] = _df.apply(infer_turn_row, axis=1)

# ===== B) Î²è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° =====
def tune_beta(df_hist: pd.DataFrame, betas=np.linspace(0.6, 2.4, 19)) -> float:
    dfh = df_hist.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid']=_make_race_id_for_hist(dfh)
    def logloss(beta: float):
        tot=0.0; n=0
        for _, g in dfh.groupby('rid'):
            x=g['score_adj'].astype(float).to_numpy()
            y=(pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int).to_numpy()
            if len(x)<2: continue
            p=np.exp(beta*(x-x.max())); s=p.sum();
            if s<=0 or not np.isfinite(s): continue
            p/=s; p=np.clip(p,1e-9,1-1e-9)
            tot+=-np.mean(y*np.log(p) + (1-y)*np.log(1-p)); n+=1
        return tot/max(n,1)
    return float(min(betas, key=logloss))

# === ã‚¿ã‚¤ãƒ åˆ†ä½å›å¸°ï¼ˆGBRï¼‰ + åŠ é‡CV ===
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GroupKFold

def _recent_weight_from_dates(dates: pd.Series, half_life_days: float) -> np.ndarray:
    days = (pd.Timestamp.today() - pd.to_datetime(dates, errors='coerce')).dt.days
    days = days.clip(lower=0).fillna(9999)
    H = max(1.0, float(half_life_days))
    return (0.5 ** (days / H)).to_numpy(float)

def _weighted_mu_sigma(X: np.ndarray, w: np.ndarray):
    w = w / max(w.sum(), 1e-12)
    mu = (w[:, None] * X).sum(axis=0)
    sd = np.sqrt(np.maximum((w[:, None] * (X - mu) ** 2).sum(axis=0), 1e-12))
    return mu, sd

def _standardize_with_mu_sigma(X: np.ndarray, mu: np.ndarray, sd: np.ndarray):
    return (X - mu) / sd

def _build_time_features(df_hist: pd.DataFrame, dist_turn_df: pd.DataFrame | None = None):
    need = {'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’', 'è·é›¢', 'æ–¤é‡', 'èŠãƒ»ãƒ€', 'ãƒ¬ãƒ¼ã‚¹æ—¥', 'é¦¬å'}
    if not need.issubset(df_hist.columns):
        return None
    d = df_hist.copy()
    d = d.dropna(subset=list(need))
    if d.empty:
        return None
    feats = ['è·é›¢', 'æ–¤é‡', 'is_dirt']
    d['is_dirt'] = d['èŠãƒ»ãƒ€'].astype(str).str.contains('ãƒ€').astype(int)
    for c in ['PCI', 'PCI3', 'Ave-3F', 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']:
        if c in d.columns:
            feats.append(c)
    if 'PCI' in d.columns:
        d['è·é›¢xPCI'] = d['è·é›¢'] * d['PCI']; feats.append('è·é›¢xPCI')
    if 'PCI3' in d.columns and 'PCI' in d.columns:
        d['PCIgap'] = d['PCI3'] - d['PCI']; feats.append('PCIgap')
    if dist_turn_df is not None and 'DistTurnZ' in dist_turn_df.columns:
        dt = dist_turn_df[['é¦¬å', 'DistTurnZ']].drop_duplicates()
        d = d.merge(dt, on='é¦¬å', how='left')
        feats.append('DistTurnZ')
    X = d[feats].astype(float).to_numpy()
    y = pd.to_numeric(d['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'], errors='coerce').to_numpy(float)
    groups = d['é¦¬å'].astype(str).to_numpy()
    dates = pd.to_datetime(d['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
    col_means = np.nanmean(X, axis=0)
    X = np.where(np.isfinite(X), X, col_means)
    return {'frame': d, 'X': X, 'y': y, 'feats': feats, 'groups': groups, 'dates': dates, 'col_means': col_means}

def fit_time_quantile_model(df_hist: pd.DataFrame, dist_turn_today_df: pd.DataFrame,
                            half_life_days: float, q_list=(0.2, 0.5, 0.8),
                            param_grid=(80, 120, 160), random_state=42):
    built = _build_time_features(df_hist, dist_turn_today_df)
    if built is None:
        return None

    X = built['X']; y = built['y']; groups = built['groups']; dates = built['dates']
    feats = built['feats']; col_means = built['col_means']

    # æ™‚ç³»åˆ—é‡ã¿ & æ¨™æº–åŒ–
    w = _recent_weight_from_dates(dates, half_life_days=max(1.0, float(half_life_days)))
    mu, sd = _weighted_mu_sigma(X, w)
    Xs = _standardize_with_mu_sigma(X, mu, sd)

    # GroupKFold ã§æœ¨æœ¬æ•°é¸å®š
    n_groups = int(len(np.unique(groups)))
    n_splits = max(2, min(5, n_groups))
    best_param, best_rmse = None, 1e18

    if n_groups >= 2 and Xs.shape[0] >= 20:
        gkf = GroupKFold(n_splits=n_splits)
        for n_est in param_grid:
            rmse_fold = []
            for tr, va in gkf.split(Xs, y, groups=groups):
                Xt, Xv = Xs[tr], Xs[va]
                yt, yv = y[tr], y[va]
                wt, wv = w[tr], w[va]
                med = GradientBoostingRegressor(loss='quantile', alpha=0.5,
                                                n_estimators=n_est, max_depth=3,
                                                random_state=random_state)
                med.fit(Xt, yt, sample_weight=wt)
                pred = med.predict(Xv)
                rmse = np.sqrt(np.average((yv - pred) ** 2, weights=wv))
                rmse_fold.append(rmse)
            score = float(np.mean(rmse_fold)) if rmse_fold else 1e18
            if score < best_rmse:
                best_rmse, best_param = score, n_est
    else:
        best_param = sorted(param_grid)[len(param_grid)//2]

    if best_param is None:
        best_param = sorted(param_grid)[len(param_grid)//2]

    models = {}
    for q in q_list:
        m = GradientBoostingRegressor(loss='quantile', alpha=q,
                                      n_estimators=best_param, max_depth=3,
                                      random_state=random_state)
        m.fit(Xs, y, sample_weight=w)
        models[q] = m

    resid = y - models[0.5].predict(Xs)
    sigma_hat = float(np.sqrt(np.maximum(np.average(resid ** 2, weights=w), 1e-6)))

    return {
        'feats': feats, 'mu': mu, 'sd': sd, 'col_means': col_means,
        'models': models, 'n_estimators': int(best_param), 'sigma_hat': sigma_hat
    }

def _field_pci_from_pace(pace_type: str) -> float:
    return {'ãƒã‚¤ãƒšãƒ¼ã‚¹': 46.0, 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹': 50.0, 'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 53.0, 'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 56.0}.get(str(pace_type), 50.0)

def build_today_design(horses_today: pd.DataFrame, s0_hist: pd.DataFrame,
                       target_distance: int, target_surface: str,
                       dist_turn_today_df: pd.DataFrame, feats: list[str],
                       pace_type: str):
    # éå»ã®æŒ‡æ•°ã®æ™‚é–“æ¸›è¡°å¹³å‡ã‚’ä½œã‚‹
    rec_w = None
    if not s0_hist.empty and 'ãƒ¬ãƒ¼ã‚¹æ—¥' in s0_hist:
        rec_w = 0.5 ** ((pd.Timestamp.today() - pd.to_datetime(s0_hist['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0) / 180.0)

    def _wmean(s, w):
        s = pd.to_numeric(s, errors='coerce')
        w = pd.to_numeric(w, errors='coerce')
        m = np.nansum(s * w); sw = np.nansum(w)
        return float(m / sw) if sw > 0 else np.nan

    pci_wmean, pci3_wmean, ave3_wmean, a3_wmedian = {}, {}, {}, {}
    if 'PCI' in s0_hist.columns:
        pci_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI'], g['_w'])).to_dict()
    if 'PCI3' in s0_hist.columns:
        pci3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI3'], g['_w'])).to_dict()
    if 'Ave-3F' in s0_hist.columns:
        ave3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['Ave-3F'], g['_w'])).to_dict()
    if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in s0_hist.columns:
        a3_wmedian = s0_hist.groupby('é¦¬å')['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].median().to_dict()

    dtt = dist_turn_today_df[['é¦¬å', 'DistTurnZ']].drop_duplicates() if ('DistTurnZ' in dist_turn_today_df.columns) \
        else pd.DataFrame({'é¦¬å': horses_today['é¦¬å'], 'DistTurnZ': np.nan})
    H = horses_today.merge(dtt, on='é¦¬å', how='left')

    rows = []
    pci_field = _field_pci_from_pace(pace_type)  # â˜… æ˜ç¤ºå¼•æ•°ã§å—ã‘å–ã‚‹

    for _, r in H.iterrows():
        name = str(r['é¦¬å'])
        x = {}
        x['è·é›¢'] = float(target_distance)
        x['æ–¤é‡'] = float(r.get('æ–¤é‡', np.nan))
        x['is_dirt'] = 1.0 if str(target_surface).startswith('ãƒ€') else 0.0

        if 'PCI' in feats:
            x['PCI'] = float(pci_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI']): x['PCI'] = pci_field
        if 'PCI3' in feats:
            x['PCI3'] = float(pci3_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI3']): x['PCI3'] = (x.get('PCI', pci_field) + 1.0)
        if 'Ave-3F' in feats:
            x['Ave-3F'] = float(ave3_wmean.get(name, np.nan))
        if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in feats:
            x['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = float(a3_wmedian.get(name, np.nan))
        if 'è·é›¢xPCI' in feats:
            x['è·é›¢xPCI'] = x['è·é›¢'] * x.get('PCI', pci_field)
        if 'PCIgap' in feats:
            x['PCIgap'] = x.get('PCI3', x.get('PCI', pci_field) + 1.0) - x.get('PCI', pci_field)
        if 'DistTurnZ' in feats:
            x['DistTurnZ'] = float(r.get('DistTurnZ', np.nan))

        rows.append({'é¦¬å': name, **x})
    return pd.DataFrame(rows)


# ===== E) è·é›¢ãƒãƒ³ãƒ‰å¹… è‡ªå‹• =====

def auto_h_m(x_all: np.ndarray) -> float:
    x = pd.to_numeric(pd.Series(x_all), errors='coerce').dropna().to_numpy(float)
    if x.size<3: return 300.0
    q75,q25=np.percentile(x,75),np.percentile(x,25)
    iqr=q75-q25
    sigma=np.std(x)
    bw=0.9*min(sigma, iqr/1.34)*(x.size**(-1/5))
    return float(np.clip(bw, 120.0, 600.0))

# ===== è·é›¢Ã—å›ã‚Š è¿‘å‚åŒ–ï¼ˆNWã€score_adjä½¿ç”¨ï¼‰ =====

def kish_neff(w: np.ndarray) -> float:
    w=np.asarray(w,float); sw=w.sum(); s2=np.sum(w**2)
    return float((sw*sw)/s2) if s2>0 else 0.0

def nw_mean(x, y, w, h):
    x=np.asarray(x,float); y=np.asarray(y,float); w=np.asarray(w,float)
    if len(x)==0: return np.nan
    K=np.exp(-0.5*(x/max(1e-9,h))**2) * w
    sK=K.sum()
    return float((K*y).sum()/sK) if sK>0 else np.nan

# æº–å‚™
hist_for_turn = _df[['é¦¬å','è·é›¢','å›ã‚Š','score_adj','_w']].dropna(subset=['é¦¬å','è·é›¢','score_adj','_w']).copy()

# hé¸å®š
h_auto = auto_h_m(hist_for_turn['è·é›¢'].to_numpy())

# æŒ‡å®šè·é›¢ãƒ»å›ã‚Šã§ã®æ¨å®š
def dist_turn_profile(name: str, df_hist: pd.DataFrame, target_d: int, target_turn: str, h_m: float, opp_turn_w: float=0.5):
    g=df_hist[df_hist['é¦¬å'].astype(str).str.strip()==str(name)].copy()
    if g.empty: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    w0 = g['_w'].to_numpy(float) * np.where(g['å›ã‚Š'].astype(str)==str(target_turn), 1.0, float(opp_turn_w))
    x  = g['è·é›¢'].to_numpy(float)
    y  = g['score_adj'].to_numpy(float)
    msk=np.isfinite(x)&np.isfinite(y)&np.isfinite(w0)
    x,y,w0=x[msk],y[msk],w0[msk]
    if x.size==0: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    z_hat = nw_mean(x-float(target_d), y, w0, h_m)
    w_eff = kish_neff(np.exp(-0.5*((x-float(target_d))/max(1e-9,h_m))**2)*w0)
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    ds=np.arange(int(target_d-600), int(target_d+600)+1, 100)
    best_d,best_val=np.nan,-1e18
    for d0 in ds:
        z=nw_mean(x-float(d0), y, w0, h_m)
        if np.isfinite(z) and z>best_val:
            best_val=float(z); best_d=int(d0)
    return {
        'DistTurnZ': float(z_hat) if np.isfinite(z_hat) else np.nan,
        'n_eff_turn': float(w_eff),
        'BestDist_turn': float(best_d) if np.isfinite(best_d) else np.nan,
        'DistTurnZ_best': float(best_val) if np.isfinite(best_val) else np.nan
    }

# ===== ã‚¹ãƒšã‚¯ãƒˆãƒ«: æ›²ç·šç”Ÿæˆ / DTW / ãƒ†ãƒ³ãƒ—ãƒ¬æ§‹ç¯‰ =====
def pseudo_curve(dist_m: float, time_s: float, a3_s: float, pci: float, pci3: float, rpci: float, pos_ratio: float,
                 n_points: int = 128) -> np.ndarray:
    """
    è·é›¢ãƒ»ç·ã‚¿ã‚¤ãƒ ãƒ»ä¸ŠãŒã‚Š3Fãªã©ã‹ã‚‰æ“¬ä¼¼é€Ÿåº¦æ›²ç·šï¼ˆæ­£è¦åŒ–ï¼‰ã‚’åˆæˆã€‚
    - è·é›¢: m, ã‚¿ã‚¤ãƒ : s
    - pos_ratio: å…ˆè¡Œåº¦ï¼ˆ1=å…ˆè¡Œã€0=å¾Œæ–¹ï¼‰
    """
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not np.isfinite(dist_m) or not np.isfinite(time_s) or time_s <= 0:
        return np.full(n_points, np.nan)

    # ãƒ™ãƒ¼ã‚¹é€Ÿåº¦ï¼ˆå¹³å‡ï¼‰
    v_avg = dist_m / time_s

    # PCI ç³»ã®å½¢çŠ¶å¯„ä¸
    pci = np.clip(pci if np.isfinite(pci) else 50.0, 35.0, 65.0)
    pci3 = pci3 if np.isfinite(pci3) else (pci + 1.0)
    rpci = rpci if np.isfinite(rpci) else 0.0  # ä»»æ„åˆ—

    x = np.linspace(0, 1, n_points)

    # å…ˆè¡Œ/å·®ã—ã«ã‚ˆã‚‹åºç›¤/çµ‚ç›¤ã‚¦ã‚§ã‚¤ãƒˆ
    lead = np.clip(pos_ratio if np.isfinite(pos_ratio) else 0.5, 0.0, 1.0)
    w_front = 0.8 + 0.6*(lead - 0.5)    # å…ˆè¡Œã»ã©åºç›¤â†‘
    w_back  = 0.8 - 0.6*(lead - 0.5)

    # PCIâ†’ä¸­ç›¤ã®ã€Œç·©ã¿/ç· ã¾ã‚Šã€ã‚’ã‚¬ã‚¦ã‚¹å½¢ã§ä»˜ä¸
    mid_center = 0.55
    mid_width  = 0.20
    mid_bump   = (50.0 - pci) / 25.0  # ãƒã‚¤ãƒšãƒ¼ã‚¹(PCIä½)ã ã¨ä¸­ç›¤è½ã¡è¾¼ã¿
    shape_mid  = 1.0 - mid_bump * np.exp(-0.5*((x - mid_center)/mid_width)**2)

    # ä¸ŠãŒã‚Š3Fï¼ˆçµ‚ç›¤ï¼‰ã«ã‚ˆã‚‹æœ«è„šå¯„ä¸
    if np.isfinite(a3_s) and a3_s > 0:
        # ã‚³ãƒ¼ã‚¹å…¨ä½“é€Ÿåº¦ã«å¯¾ã™ã‚‹çµ‚ç›¤é€Ÿåº¦æ¯”ï¼ˆç²—ã„è¿‘ä¼¼ï¼‰
        last_600 = 600.0
        if dist_m > last_600:
            v_last = last_600 / a3_s
            ratio  = np.clip(v_last / v_avg, 0.5, 1.5)
        else:
            ratio = 1.0
    else:
        ratio = 1.0 + 0.10*((pci3 - pci)/5.0)  # PCI3>PCI ãªã‚‰æœ«ä¸Šã’

    # å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è»½ã„èµ·ä¼ï¼ˆFFTç›¸å½“ã®ç‰¹å¾´ã‚’å°‘ã—æŒãŸã›ã‚‹ï¼‰
    low = 0.04*np.sin(2*np.pi*1*x)
    mid = 0.03*np.sin(2*np.pi*2*x + 1.3)
    high= 0.02*np.sin(2*np.pi*4*x + 0.7)
    wav = 1.0 + low + mid + high

    # åˆæˆ
    base = v_avg * (w_front*(1 - x) + w_back*x)   # ç›´ç·šè£œé–“ã®å‹¾é…
    v = base * shape_mid * (1.0 + (ratio - 1.0)*x**1.2) * wav

    # æ­£è¦åŒ–ï¼ˆé¢ç©=1ã«è¿‘ã¥ã‘ã‚‹ï¼‰
    v = np.maximum(v, 1e-8)
    v = v / np.trapz(v, x)
    return v

def dtw_distance(a: np.ndarray, b: np.ndarray) -> float:
    """ã‚·ãƒ³ãƒ—ãƒ«DTWï¼ˆO(N^2)ã€NaNã¯å¤§ãã„ç½°å‰‡ï¼‰"""
    if not (isinstance(a, np.ndarray) and isinstance(b, np.ndarray)):
        return np.inf
    if len(a)==0 or len(b)==0 or np.isnan(a).all() or np.isnan(b).all():
        return np.inf
    A = np.nan_to_num(a, nan=1e6)
    B = np.nan_to_num(b, nan=1e6)
    n, m = len(A), len(B)
    D = np.full((n+1, m+1), np.inf)
    D[0,0] = 0.0
    for i in range(1, n+1):
        ai = A[i-1]
        for j in range(1, m+1):
            cost = abs(ai - B[j-1])
            D[i,j] = cost + min(D[i-1,j], D[i,j-1], D[i-1,j-1])
    return float(D[n,m])

def build_template_curves(df_curves: pd.DataFrame, target_dist: int, target_surface: str,
                          tol: int = 100, n_points: int = 128):
    """
    åŒè·é›¢å¸¯(Â±tol) & åŒSurface ã® '_curve' ã‚’é›†ã‚ã¦ä¸­å¤®å€¤ãƒ†ãƒ³ãƒ—ãƒ¬ã‚’ä½œã‚‹ã€‚
    ã¤ã„ã§ã«FFTå¸¯åŸŸã‹ã‚‰ãƒ¬ãƒ¼ã‚¹å‹ Gate ã‚’æ¨å®šï¼ˆ0=æŒä¹…,1=ä¸­åº¸,2=ç¬ç™ºï¼‰ã€‚
    """
    if df_curves.empty or '_curve' not in df_curves.columns:
        return np.full(n_points, 1.0/n_points), {'Gate': 1}

    dd = df_curves.copy()
    dd['è·é›¢'] = pd.to_numeric(dd['è·é›¢'], errors='coerce')
    dd = dd[dd['è·é›¢'].between(target_dist - tol, target_dist + tol, inclusive='both')]
    dd = dd[dd['èŠãƒ»ãƒ€'].astype(str).str.startswith(str(target_surface))]

    curves = [c for c in dd['_curve'].values if isinstance(c, np.ndarray)]
    if not curves:
        return np.full(n_points, 1.0/n_points), {'Gate': 1}

    # é•·ã•ã‚’åˆã‚ã›ã¦ä¸­å¤®å€¤
    L = min(min(len(c) for c in curves), n_points)
    mat = np.vstack([np.interp(np.linspace(0,1,L), np.linspace(0,1,len(c)), c) for c in curves])
    templ = np.median(mat, axis=0)

    # FFTå¸¯åŸŸæ¯”ã‹ã‚‰ Gate ã‚’ç°¡æ˜“åˆ¤å®š
    f = np.fft.rfft(templ - templ.mean())
    pow_spec = np.abs(f)**2
    # ä½/ä¸­/é«˜ã®ç°¡æ˜“å¸¯åŸŸæ¯”
    n = len(pow_spec)
    low = pow_spec[:max(2, n//6)].sum()
    mid = pow_spec[max(2, n//6):max(4, n//3)].sum()
    high= pow_spec[max(4, n//3):].sum()

    # é«˜å‘¨æ³¢(ç¬ç™º)ãŒå¼·ã‘ã‚Œã°2ã€ä½å‘¨æ³¢(æŒä¹…)ãŒå¼·ã‘ã‚Œã°0ã€ãã®ä»–1
    if high > max(low, mid) * 1.15:
        gate = 2
    elif low > max(mid, high) * 1.15:
        gate = 0
    else:
        gate = 1

    return templ, {'Gate': gate}

def infer_gate_from_curve(curve: np.ndarray, n_points: int = 128):
    """1æœ¬ã®é€Ÿåº¦æ›²ç·šã‹ã‚‰ 0=æŒä¹… / 1=ä¸­åº¸ / 2=ç¬ç™º ã‚’FFTå¸¯åŸŸæ¯”ã§æ¨å®š"""
    if not isinstance(curve, np.ndarray) or curve.size == 0 or np.isnan(curve).all():
        return np.nan
    L = min(len(curve), n_points)
    x = np.interp(np.linspace(0,1,L), np.linspace(0,1,len(curve)), curve)
    x = x - np.nanmean(x)
    x = np.nan_to_num(x, nan=0.0)
    f = np.fft.rfft(x)
    ps = np.abs(f)**2
    n = len(ps)
    low  = ps[:max(2, n//6)].sum()
    mid  = ps[max(2, n//6):max(4, n//3)].sum()
    high = ps[max(4, n//3):].sum()
    if high > max(low, mid) * 1.15:
        return 2  # ç¬ç™º
    elif low > max(mid, high) * 1.15:
        return 0  # æŒä¹…
    else:
        return 1  # ä¸­åº¸

# ===== é¦¬ã”ã¨é›†è¨ˆ =====
agg=[]
for name, g in _df.groupby('é¦¬å'):
    avg=g['score_norm'].mean()
    std=g['score_norm'].std(ddof=0)
    wavg = np.average(g['score_norm'], weights=g['_w']) if g['_w'].sum()>0 else np.nan
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g)>=2 else np.nan
    agg.append({'é¦¬å':_trim_name(name),'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg=pd.DataFrame(agg)
if df_agg.empty:
    st.error('éå»èµ°ã®é›†è¨ˆãŒç©ºã§ã™ã€‚'); st.stop()

# WStdã®åºŠ
wstd_nontrivial=df_agg.loc[df_agg['Nrun']>=2,'WStd']
def_std=float(wstd_nontrivial.median()) if wstd_nontrivial.notna().any() else 6.0
min_floor=max(1.0, def_std*0.6)
df_agg['WStd']=df_agg['WStd'].fillna(def_std)
df_agg.loc[df_agg['WStd']<min_floor,'WStd']=min_floor

# ä»Šæ—¥æƒ…å ±
for df_ in [df_agg, horses]:
    if 'é¦¬å' in df_.columns: df_['é¦¬å']=df_['é¦¬å'].map(_trim_name)

if 'è„šè³ª' in horses.columns: horses['è„šè³ª']=horses['è„šè³ª'].map(normalize_style)

cols_to_merge = ['é¦¬å','æ ','ç•ª','è„šè³ª','æ€§åˆ¥','å¹´é½¢',
                 'å‹ç‡%_HIST','é€£å¯¾ç‡%_HIST','è¤‡å‹ç‡%_HIST']  # NEW
cols_to_merge = [c for c in cols_to_merge if c in horses.columns]
df_agg = df_agg.merge(horses[cols_to_merge], on='é¦¬å', how='left')

# === æ–°è¦: ç‰¹æ€§Ptsï¼ˆæ€§åˆ¥ãƒ»è„šè³ªãƒ»å¹´é½¢ãƒ»æ ï¼‰ ===
idx = df_agg.index

# æ€§åˆ¥
sex_map = {'ç‰¡': SEX_MALE, 'ç‰': SEX_FEMA, 'ã‚»': SEX_GELD, 'é¨™': SEX_GELD, 'ã›ã‚“': SEX_GELD}
sex_series = df_agg['æ€§åˆ¥'] if 'æ€§åˆ¥' in df_agg.columns else pd.Series(['']*len(idx), index=idx)
df_agg['SexPts'] = sex_series.astype(str).map(sex_map).fillna(0.0).astype(float)

# è„šè³ª
style_map = {'é€ƒã’': STL_NIGE, 'å…ˆè¡Œ': STL_SENKO, 'å·®ã—': STL_SASHI, 'è¿½è¾¼': STL_OIKOMI}
style_series = df_agg['è„šè³ª'] if 'è„šè³ª' in df_agg.columns else pd.Series(['']*len(idx), index=idx)
df_agg['StylePts'] = style_series.astype(str).map(style_map).fillna(0.0).astype(float)

# å¹´é½¢ï¼ˆãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®è·é›¢ã§æ¸›ç‚¹ï¼‰
age_series = pd.to_numeric(df_agg['å¹´é½¢'] if 'å¹´é½¢' in df_agg.columns else pd.Series([np.nan]*len(idx), index=idx), errors='coerce')
df_agg['AgePts'] = (-float(AGE_SLOPE) * (age_series - int(AGE_PEAK)).abs()).fillna(0.0)

# æ ï¼ˆå†…å¤–ãƒã‚¤ã‚¢ã‚¹ï¼‰
w = pd.to_numeric(df_agg['æ '] if 'æ ' in df_agg.columns else pd.Series([np.nan]*len(idx), index=idx), errors='coerce')
centered = (4.5 - w) / 3.5   # æ 1=+1, æ 8=-1ï¼ˆå†…ãŒæ­£ï¼‰
if WAKU_DIR == "å†…æœ‰åˆ©":
    waku_raw = centered
elif WAKU_DIR == "å¤–æœ‰åˆ©":
    waku_raw = -centered
else:
    waku_raw = pd.Series(0.0, index=idx)
df_agg['WakuPts'] = (float(WAKU_STR) * pd.to_numeric(waku_raw)).fillna(0.0)

# === å³/å·¦é›†è¨ˆï¼ˆscore_adjã®é‡ã¿å¹³å‡ï¼‰ ===
turn_base = (
    _df[['é¦¬å','å›ã‚Š','score_adj','_w']]
    .dropna(subset=['é¦¬å','å›ã‚Š','score_adj','_w'])
    .copy()
)

def _wavg_score(g: pd.DataFrame) -> float:
    w = pd.to_numeric(g['_w'], errors='coerce').to_numpy(float)
    x = pd.to_numeric(g['score_adj'], errors='coerce').to_numpy(float)
    sw = np.nansum(w)
    return float(np.nansum(w * x) / sw) if sw > 0 else np.nan

# å³å›ã‚Š
right = (
    turn_base[turn_base['å›ã‚Š'].astype(str) == 'å³']
    .groupby('é¦¬å', as_index=False)
    .apply(lambda g: pd.Series({'RightZ': _wavg_score(g)}))
    .reset_index(drop=True)
)

# å·¦å›ã‚Š
left = (
    turn_base[turn_base['å›ã‚Š'].astype(str) == 'å·¦']
    .groupby('é¦¬å', as_index=False)
    .apply(lambda g: pd.Series({'LeftZ': _wavg_score(g)}))
    .reset_index(drop=True)
)

# å‡ºèµ°æœ¬æ•°ã‚«ã‚¦ãƒ³ãƒˆ
counts = (
    turn_base.assign(_one=1)
    .pivot_table(index='é¦¬å', columns='å›ã‚Š', values='_one', aggfunc='sum', fill_value=0)
    .rename(columns={'å³': 'nR', 'å·¦': 'nL'})
    .reset_index()
)

# ã¾ã¨ã‚ã¦çµåˆ
turn_pref = (
    pd.merge(right, left, on='é¦¬å', how='outer')
    .merge(counts, on='é¦¬å', how='left')
    .fillna({'nR': 0, 'nL': 0})
)

# æ¬ æä¿é™º
for c in ['RightZ','LeftZ','nR','nL']:
    if c not in turn_pref.columns:
        turn_pref[c] = np.nan if c in ['RightZ','LeftZ'] else 0

# ä»¥é™ã¯å…ƒã®è¨ˆç®—ã®ã¾ã¾
turn_pref['TurnGap'] = (turn_pref['RightZ'].fillna(0) - turn_pref['LeftZ'].fillna(0))
turn_pref['n_eff_turn'] = (turn_pref['nR'].fillna(0) + turn_pref['nL'].fillna(0)).clip(lower=0)
conf = np.clip(turn_pref['n_eff_turn'] / 3.0, 0.0, 1.0)
turn_pref['TurnPrefPts'] = np.clip(turn_pref['TurnGap'] / 1.5, -1.0, 1.0) * conf

df_agg = df_agg.merge(
    turn_pref[['é¦¬å','RightZ','LeftZ','TurnGap','n_eff_turn','TurnPrefPts']],
    on='é¦¬å',
    how='left'
)



# è·é›¢Ã—å›ã‚Šï¼ˆè‡ªå‹•hï¼‰
rows=[]
for nm in df_agg['é¦¬å'].astype(str):
    prof=dist_turn_profile(nm, hist_for_turn, int(TARGET_DISTANCE), str(TARGET_TURN), h_auto, opp_turn_w=0.5)
    rows.append({'é¦¬å':nm, **prof})
_dfturn = pd.DataFrame(rows)
df_agg = df_agg.merge(_dfturn, on='é¦¬å', how='left')
neff_cols = [c for c in df_agg.columns if c.startswith('n_eff_turn')]
if neff_cols:
    neff_series = (
        df_agg[neff_cols]
        .apply(pd.to_numeric, errors='coerce')
        .bfill(axis=1)                # å…ˆã«ã‚ã‚‹åˆ—ã‚’å„ªå…ˆ
        .iloc[:, 0]                   # ä»£è¡¨åˆ—ã‚’1æœ¬ã ã‘é¸ã¶
        .fillna(0.0)
    )
    df_agg['n_eff_turn'] = neff_series
    # ä½™åˆ†ãª *_x, *_y ã¯è½ã¨ã—ã¦ãŠãã¨å¾Œæ®µã§å®‰å…¨
    for c in neff_cols:
        if c != 'n_eff_turn':
            df_agg.drop(columns=c, inplace=True, errors='ignore')
else:
    # åˆ—è‡ªä½“ãŒç„¡ã„ã‚±ãƒ¼ã‚¹ã®ä¿é™º
    df_agg['n_eff_turn'] = 0.0


# éå»èµ°ã”ã¨ã«é€Ÿåº¦æ›²ç·šã‚’æ§‹ç¯‰ï¼ˆå‰åŠã§å®šç¾©ã—ãŸ pseudo_curve ã‚’ä½¿ç”¨ï¼‰
s0_spec = s0.copy()
for need in ['è·é›¢','èµ°ç ´ã‚¿ã‚¤ãƒ ç§’','ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']:
    if need not in s0_spec.columns: s0_spec[need] = np.nan
for opt in ['PCI','PCI3','RPCI','é€šé4è§’','é ­æ•°','èŠãƒ»ãƒ€']:
    if opt not in s0_spec.columns: s0_spec[opt] = np.nan

def _pos_ratio_4c(row) -> float:
    p = pd.to_numeric(row.get('é€šé4è§’', np.nan), errors='coerce')
    n = pd.to_numeric(row.get('é ­æ•°', np.nan), errors='coerce')
    if not np.isfinite(p) or not np.isfinite(n) or n<=1: return 0.5
    return float((n - p) / (n - 1))  # å…ˆè¡Œ=1.0, å¾Œæ–¹=0.0

s0_spec['_pos'] = s0_spec.apply(_pos_ratio_4c, axis=1)
s0_spec['_curve'] = s0_spec.apply(
    lambda r: pseudo_curve(
        pd.to_numeric(r['è·é›¢'], errors='coerce'),
        pd.to_numeric(r['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'], errors='coerce'),
        pd.to_numeric(r['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '], errors='coerce'),
        pd.to_numeric(r['PCI'], errors='coerce'),
        pd.to_numeric(r['PCI3'], errors='coerce'),
        pd.to_numeric(r['RPCI'], errors='coerce'),
        float(r['_pos'])
    ),
    axis=1
)

# ãƒ†ãƒ³ãƒ—ãƒ¬æ›²ç·šï¼ˆè·é›¢Â±100m & åŒSurfaceã®ä¸­å¤®å€¤ï¼‰
templ_curve, templ_info = build_template_curves(
    s0_spec[['é¦¬å','è·é›¢','èŠãƒ»ãƒ€','_curve']].copy(),
    int(TARGET_DISTANCE),
    str(TARGET_SURFACE),
    tol=int(templ_tol_m)
)


# å„é¦¬ã®DTWæœ€å°è·é›¢â†’ZåŒ–ï¼ˆå¤§ãã„ã»ã©é©åˆè‰¯ï¼‰
rows=[]
for name, g in s0_spec.groupby('é¦¬å'):
    dists=[]; gates=[]
    for v in g['_curve'].values:
        if isinstance(v, np.ndarray):
            dists.append(dtw_distance(v, templ_curve))
            gates.append(infer_gate_from_curve(v))
    DTWmin = float(np.nanmin(dists)) if dists else np.nan
    gate_hat = float(np.nanmedian([x for x in gates if np.isfinite(x)])) if gates else np.nan
    rows.append({'é¦¬å':name, 'DTWmin':DTWmin, 'SpecGate_horse': gate_hat})
df_spec = pd.DataFrame(rows)

if not df_spec.empty:
    mu = float(df_spec['DTWmin'].mean(skipna=True))
    sd = float(df_spec['DTWmin'].std(ddof=0, skipna=True))
    if not np.isfinite(sd) or sd==0.0: sd=1.0
    df_spec['SpecFitZ'] = -(df_spec['DTWmin'] - mu) / sd
else:
    df_spec = pd.DataFrame({'é¦¬å': df_agg['é¦¬å'], 'SpecFitZ': np.nan, 'SpecGate_horse': np.nan})

# ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆãƒ¬ãƒ¼ã‚¹å…¨ä½“ï¼‰ã®å‹ã¯å…¨å“¡åŒã˜å€¤ã¨ã—ã¦åˆ—ã‚’æŒãŸã›ã‚‹
df_spec['SpecGate_templ'] = templ_info.get('Gate', 1)

df_agg = df_agg.merge(df_spec, on='é¦¬å', how='left')
# æ•°å€¤ã‚²ãƒ¼ãƒˆ(0/1/2) â†’ ãƒ©ãƒ™ãƒ«ã¸
_gate_label = {0: 'æŒä¹…', 1: 'ä¸­åº¸', 2: 'ç¬ç™º'}
df_agg['SpecGate_horse']  = pd.to_numeric(df_agg['SpecGate_horse'], errors='coerce')  # å¿µã®ãŸã‚
df_agg['SpecGate_templ']  = pd.to_numeric(df_agg['SpecGate_templ'], errors='coerce')

df_agg['SpecGate_horse_lbl'] = df_agg['SpecGate_horse'].map(_gate_label)
df_agg['SpecGate_templ_lbl'] = df_agg['SpecGate_templ'].map(_gate_label)

# ===== èª¿æ•™ï¼ˆç‰©ç†ï¼‰â†’ PhysicsZ ã‚’ä½œã‚‹ =====
df_phys = pd.DataFrame()

if USE_PHYSICS:
    valid_trains = []

    # å…¥ã£ã¦ãã‚‹å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿ã€å¿…é ˆåˆ—ãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ã ã‘æ¡ç”¨
    for f, kind in [(wood_file, 'wood'), (hill_file, 'hill')]:
        if f is None:
            continue
        tdf = _read_train_xlsx(f, kind)  # æœŸå¾…åˆ—: é¦¬å, æ—¥ä»˜, _kind, _lap_sec, _intensity
        if isinstance(tdf, pd.DataFrame) and not tdf.empty:
            need = {'é¦¬å', 'æ—¥ä»˜', '_lap_sec'}
            if need.issubset(set(tdf.columns)):
                # ä½™è¨ˆãªåˆ—ã¯ã‚ã£ã¦ã‚‚è‰¯ã„ãŒã€æœ€ä½é™ã®åˆ—ã¯ãã‚ãˆã¦ãŠã
                keep = [c for c in ['é¦¬å','æ—¥ä»˜','_kind','_lap_sec','_intensity'] if c in tdf.columns]
                valid_trains.append(tdf[keep])

    if valid_trains:
        T = pd.concat(valid_trains, ignore_index=True)
        # ã“ã“ã§ KeyError ãŒå‡ºãªã„ã‚ˆã†ã«åˆ—å­˜åœ¨ã‚’å†ç¢ºèª
        if {'é¦¬å','æ—¥ä»˜'}.issubset(T.columns):
            T = T.dropna(subset=['é¦¬å','æ—¥ä»˜'])
            if not T.empty:
                df_phys = _derive_training_metrics(
                    train_df=T, s0_races=_df.copy(),
                    Crr_wood=Crr_wood, Crr_hill=Crr_hill,
                    CdA=CdA, rho=rho_air,
                    Pmax_wkg=Pmax_wkg, Emax_jkg=Emax_jkg,
                    half_life_days=int(half_life_train_days)
                )
with st.expander("èª¿æ•™ãƒ‘ãƒ¼ã‚¹çŠ¶æ³", expanded=False):
    st.write("woodãƒ•ã‚¡ã‚¤ãƒ«: ", wood_file.name if wood_file else None,
             " / hillãƒ•ã‚¡ã‚¤ãƒ«: ", hill_file.name if hill_file else None)
    try:
        trains_dbg = []
        if wood_file: trains_dbg.append(_read_train_xlsx(wood_file, 'wood'))
        if hill_file: trains_dbg.append(_read_train_xlsx(hill_file, 'hill'))
        T_dbg = pd.concat(trains_dbg, ignore_index=True) if trains_dbg else pd.DataFrame()

        st.write("æŠ½å‡ºè¡Œæ•°(èª¿æ•™):", len(T_dbg))
        if not T_dbg.empty:
            st.dataframe(T_dbg.head(10))
            st.write("åˆ—:", list(T_dbg.columns))
            st.write("èª¿æ•™ æ—¥ä»˜ç¯„å›²:", T_dbg['æ—¥ä»˜'].min(), "â†’", T_dbg['æ—¥ä»˜'].max())

            # ãƒãƒ¼ã‚¸ã§ãã‚‹åå‰ã®çªåˆ
            base_names = set(df_agg['é¦¬å'].astype(str))
            phys_names = set(T_dbg['é¦¬å'].astype(str))
            inter = sorted(base_names & phys_names)
            miss  = sorted(base_names - phys_names)
            st.write("å‡ºèµ°è¡¨âˆ©èª¿æ•™ äº¤å·®:", len(inter))
            if miss:
                st.write("æœªãƒãƒƒãƒï¼ˆå‡ºèµ°è¡¨ã«ã‚ã‚‹ãŒèª¿æ•™ã«ç„¡ã„ï¼‰ä¾‹:", miss[:10])
    except Exception as e:
        st.write("ãƒ‡ãƒãƒƒã‚°ä¸­ã«ä¾‹å¤–:", e)

    st.write("df_phys è¡Œæ•°:", 0 if df_phys.empty else len(df_phys))
    if not df_phys.empty:
        st.dataframe(df_phys.head(10))


# ç‰©ç†DFãŒç©ºãªã‚‰ãƒ€ãƒŸãƒ¼åˆ—ã‚’ç”¨æ„ï¼ˆä»¥é™ã® merge ã§ã‚³ã‚±ãªã„ã‚ˆã†ã«ï¼‰
if df_phys.empty:
    df_phys = pd.DataFrame({
        'é¦¬å': df_agg['é¦¬å'],
        'EAP': np.nan, 'PeakWkg': np.nan, 'EffReserve': np.nan, 'PhysicsZ': np.nan
    })
# ã‚†ã‚‹ã„çµåˆï¼ˆdifflibï¼‰ã§æ•‘æ¸ˆï¼šäº¤å·®ãŒå°‘ãªã„ã¨ãã ã‘
try:
    base_names = df_agg['é¦¬å'].astype(str).tolist()
    phys_names = df_phys['é¦¬å'].astype(str).tolist()
    inter = set(base_names) & set(phys_names)
    if len(inter) <= max(2, len(base_names)//4):
        import difflib
        # èª¿æ•™å´ â†’ å‡ºèµ°å´ ã¸ç½®æ›ãƒãƒƒãƒ—ã‚’ä½œã‚‹
        rep = {}
        for pn in phys_names:
            m = difflib.get_close_matches(pn, base_names, n=1, cutoff=0.90)
            if m:
                rep[pn] = m[0]
        if rep:
            df_phys = df_phys.assign(__join=df_phys['é¦¬å'].replace(rep)).drop(columns=['é¦¬å']).rename(columns={'__join':'é¦¬å'})
except Exception:
    pass

df_agg = df_agg.merge(df_phys, on='é¦¬å', how='left')
for c in ['PhysicsZ', 'PeakWkg', 'EAP']:
    df_agg[c] = pd.to_numeric(df_agg.get(c), errors='coerce')

# ===== RecencyZ / StabZ =====
base_for_recency = df_agg.get('WAvgZ', pd.Series(np.nan, index=df_agg.index)).fillna(df_agg.get('AvgZ', pd.Series(0.0, index=df_agg.index)))
df_agg['RecencyZ']=z_score(pd.to_numeric(base_for_recency, errors='coerce').fillna(0.0))
wstd_fill=pd.to_numeric(df_agg['WStd'], errors='coerce')
if not np.isfinite(wstd_fill).any(): wstd_fill=pd.Series(6.0, index=df_agg.index)
df_agg['StabZ']=z_score(-(wstd_fill.fillna(wstd_fill.median())))

# D) ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ è‡ªå·±å­¦ç¿’
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns and s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].notna().any():
    bt = _df.merge(s1[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
    bt_min=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True); bt_max=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True)
    span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
    bt['BT_norm']=((bt_max - bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])/span).clip(0,1)
    corr = np.corrcoef(bt['BT_norm'].fillna(0.0), bt['score_adj'].fillna(0.0))[0,1]
    if not np.isfinite(corr): corr=0.0
    w_bt = float(np.clip(corr, 0.0, 1.2)) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)
else:
    w_bt = 0.0 if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)

# ===== æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆæœªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™ï¼‰ =====
turn_gain = 1.0
pace_gain = float(pace_gain)
stab_weight = float(stab_weight)
dist_gain = 1.0

# === æ¬ æã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆFinalRawã®ç›´å‰ã§ä¸€åº¦ã ã‘ï¼‰ ===
df_agg['RecencyZ']    = pd.to_numeric(df_agg['RecencyZ'], errors='coerce').fillna(df_agg['RecencyZ'].median())
df_agg['StabZ']       = pd.to_numeric(df_agg['StabZ'],    errors='coerce').fillna(df_agg['StabZ'].median())
df_agg['TurnPrefPts'] = pd.to_numeric(df_agg['TurnPrefPts'], errors='coerce').fillna(0.0)
df_agg['DistTurnZ']   = pd.to_numeric(df_agg['DistTurnZ'],   errors='coerce').fillna(0.0)

# === 2æ­³æˆ¦ãƒ¢ãƒ¼ãƒ‰ã®é‡ã¿è£œæ­£ ===
if 'df_agg' in locals() and TWOYO_MODE:
    # å±¥æ­´ã®å½¢çŠ¶é©æ€§ã¯å¼±ã‚ã‚‹ï¼ˆãªã‘ã‚Œã°ç„¡è¦–ã•ã‚Œã‚‹ï¼‰
    for col in ('TurnPrefPts', 'DistTurnZ'):
        if col in df_agg.columns:
            df_agg[col] = 0.0
    # å®‰å®šæ€§ã‚¦ã‚§ã‚¤ãƒˆãŒå¤‰æ•°ãªã‚‰40%ã«å¼±ä½“åŒ–ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
    if 'stab_weight' in locals():
        try:
            stab_weight = float(stab_weight) * 0.4
        except Exception:
            pass

df_agg['PacePts']=0.0  # å¾Œã§MCã‹ã‚‰
# FinalRawï¼ˆåŸºç¤ï¼šRecency/Stab/Turn/Dist + ç‰¹æ€§ï¼‰
df_agg['FinalRaw'] = (
    df_agg['RecencyZ']
    + float(stab_weight) * df_agg['StabZ']
    + 1.0 * df_agg['TurnPrefPts']
    + 1.0 * df_agg['DistTurnZ'].fillna(0.0)
    + df_agg['SexPts'] + df_agg['StylePts'] + df_agg['AgePts'] + df_agg['WakuPts']
)

df_agg['FinalRaw_before_balance'] = df_agg['FinalRaw'].astype(float)

# æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆä¸­å¤®å€¤åŸºæº–ï¼‰
if 'æ–¤é‡' in df_agg.columns and pd.to_numeric(df_agg['æ–¤é‡'], errors='coerce').notna().any():
    kg = pd.to_numeric(df_agg['æ–¤é‡'], errors='coerce')
    kg_med = float(np.nanmedian(kg))
    df_agg['FinalRaw'] -= float(weight_coeff) * (kg - kg_med).fillna(0.0)

# BTã‚’åŠ ç‚¹
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns:
    btmap = s1.set_index('é¦¬å')['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].to_dict()
    btvals = df_agg['é¦¬å'].map(btmap)
    if pd.Series(btvals).notna().any():
        bts = pd.Series(btvals)
        bt_min=bts.min(skipna=True); bt_max=bts.max(skipna=True)
        span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
        BT_norm = ((bt_max - bts)/span).clip(0,1).fillna(0.0)
        df_agg['FinalRaw'] += w_bt * BT_norm

# â˜… ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯„ä¸ã‚’æœ€å¾Œã«åˆæˆ
# === 2æ­³æˆ¦: ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¯”é‡ã®ä¸‹é™ã‚’å¼•ãä¸Šã’ï¼ˆä»»æ„ï¼‰ ===
if 'spec_ratio' in locals() and TWOYO_MODE:
    try:
        spec_ratio = max(float(spec_ratio), 0.65)
    except Exception:
        pass
# â˜… ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯„ä¸ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¿‚æ•° Ã— é…åˆ†ï¼‰
df_agg['SpecFitZ'] = pd.to_numeric(df_agg['SpecFitZ'], errors='coerce')
df_agg['FinalRaw'] += spec_ratio * float(spectral_weight_ui) * df_agg['SpecFitZ'].fillna(0.0)

# â˜… ç‰©ç†å¯„ä¸ï¼ˆZ=50ã‚’0åŸºæº–, 10åˆ»ã¿ã§ä»–Zã¨ã‚¹ã‚±ãƒ¼ãƒ«åˆã‚ã›ï¼‰Ã— é…åˆ†
df_agg['PhysicsZ'] = pd.to_numeric(df_agg['PhysicsZ'], errors='coerce')
df_agg['FinalRaw'] += phys_ratio * ((df_agg['PhysicsZ'] - 50.0) / 10.0).fillna(0.0)

beta_pl = tune_beta(_df.copy()) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(mc_beta_manual)

# === 2æ­³æˆ¦: PLé‹­ã•ã®ç·©å’Œ ===
try:
    if TWOYO_MODE:
        beta_pl = max(0.9, float(beta_pl) * 0.85)
except Exception:
    pass

# === 2æ­³æˆ¦: ãƒšãƒ¼ã‚¹MCãƒˆã‚°ãƒ« ===
if USE_MC:
    # ===== ãƒšãƒ¼ã‚¹MCï¼ˆåå¯¾ç§°Gumbelã§åˆ†æ•£ä½æ¸›ï¼‰ =====
    mark_rule={
        'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
        'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
        'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
        'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    }
    mark_to_pts={'â—':2,'ã€‡':1,'â—‹':1,'â–³':0,'Ã—':-1}

    name_list=df_agg['é¦¬å'].tolist()
    P=np.zeros((len(name_list),4),float)
    for i, nm in enumerate(name_list):
        stl = df_agg.loc[df_agg['é¦¬å']==nm, 'è„šè³ª'].values
        stl = stl[0] if len(stl)>0 else ''
        if stl in STYLES:
            P[i, STYLES.index(stl)] = 1.0
        else:
            P[i,:]=0.25

    epi_alpha, epi_beta = 1.0, 0.6
    thr_hi, thr_mid, thr_slow = 0.52, 0.30, 0.18

    rng = np.random.default_rng(24601)
    draws = 10000
    Hn=len(name_list)
    sum_pts=np.zeros(Hn,float); pace_counter={'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
    for _ in range(draws//2):
        sampled = [np.argmax(P[i]) for i in range(Hn)]
        nige  = sum(1 for s in sampled if s==0)
        sengo = sum(1 for s in sampled if s==1)
        epi=(epi_alpha*nige + epi_beta*sengo)/max(1,Hn)
        if   epi>=thr_hi:   pace_t='ãƒã‚¤ãƒšãƒ¼ã‚¹'
        elif epi>=thr_mid:  pace_t='ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'
        elif epi>=thr_slow: pace_t='ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
        else:               pace_t='ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
        pace_counter[pace_t]+=2
        mk=mark_rule[pace_t]
        for i,s in enumerate(sampled):
            sum_pts[i]+=2*mark_to_pts[ mk[STYLES[s]] ]

    df_agg['PacePts']=sum_pts/max(1,draws)
    pace_type=max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'
else:
    # MCã‚’ä½¿ã‚ãªã„å ´åˆã¯ãƒšãƒ¼ã‚¹ç‚¹ã‚’å›ºå®š
    if 'df_agg' in locals():
        df_agg['PacePts'] = 0.0
    pace_type = 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'
    
# ===== ã‚¿ã‚¤ãƒ åˆ†å¸ƒ â†’ ç€é †MC =====
half_life_days = int(half_life_m * 30.4375) if half_life_m > 0 else 99999

time_model_pkg = fit_time_quantile_model(
    df_hist=_df.copy(),
    dist_turn_today_df=_dfturn.copy(),
    half_life_days=half_life_days,
    q_list=(0.2, 0.5, 0.8),
    param_grid=(80, 120, 160),
    random_state=24601
)

if time_model_pkg is not None:
    feats = time_model_pkg['feats']
    mu, sd = time_model_pkg['mu'], time_model_pkg['sd']
    models = time_model_pkg['models']

    todayX = build_today_design(
        horses_today=horses,
        s0_hist=s0,
        target_distance=int(TARGET_DISTANCE),
        target_surface=str(TARGET_SURFACE),
        dist_turn_today_df=_dfturn,
        feats=feats,
        pace_type=pace_type  # â† æ˜ç¤ºæ¸¡ã—
    )

    v = todayX[feats].astype(float).to_numpy()
    v = np.where(np.isfinite(v), v, time_model_pkg['col_means'])
    vs = (v - mu) / sd

    Q20 = models[0.2].predict(vs)
    Q50 = models[0.5].predict(vs)
    Q80 = models[0.8].predict(vs)

    z08 = 0.8416212335729143
    sigma_from_span = (Q80 - Q20) / (2.0 * z08)
    sigma_raw = np.maximum(sigma_from_span, 0.25)

    sigma_hat = time_model_pkg['sigma_hat']
    sigma = np.sqrt(0.5 * sigma_raw ** 2 + 0.5 * sigma_hat ** 2)

    pred_time = pd.DataFrame({
        'é¦¬å': todayX['é¦¬å'],
        'PredTime_s': Q50,
        'PredTime_p20': Q20,
        'PredTime_p80': Q80,
        'PredSigma_s': sigma
    })

    draws_mc = 120000
    rng_t = np.random.default_rng(13579)
    names = pred_time['é¦¬å'].tolist()
    mu_vec = pred_time['PredTime_s'].to_numpy(float)
    sig_vec = pred_time['PredSigma_s'].to_numpy(float)
    n = len(mu_vec)

    tau = 0.30 * float(np.nanmedian(sig_vec))
    E_id = rng_t.normal(size=(draws_mc, n)) * sig_vec[None, :]
    E_cm = rng_t.normal(size=(draws_mc, 1)) * tau
    T = mu_vec[None, :] + E_id + E_cm

    rk = np.argsort(T, axis=1)
    win = np.bincount(rk[:, 0], minlength=n)
    top3 = np.zeros(n, int)
    for k in range(3):
        top3 += np.bincount(rk[:, k], minlength=n)
    exp_rank = (rk.argsort(axis=1) + 1).mean(axis=0)

    pred_time['å‹ç‡%_TIME'] = (100.0 * win / draws_mc).round(2)
    pred_time['è¤‡å‹ç‡%_TIME'] = (100.0 * top3 / draws_mc).round(2)
    pred_time['æœŸå¾…ç€é †_TIME'] = np.round(exp_rank, 3)

    df_agg = df_agg.merge(pred_time, on='é¦¬å', how='left')
else:
    df_agg['PredTime_s'] = np.nan
    df_agg['PredTime_p20'] = np.nan
    df_agg['PredTime_p80'] = np.nan
    df_agg['PredSigma_s'] = np.nan
    df_agg['å‹ç‡%_TIME'] = np.nan
    df_agg['è¤‡å‹ç‡%_TIME'] = np.nan
    df_agg['æœŸå¾…ç€é †_TIME'] = np.nan

# ===== PhysS1ï¼ˆã‚³ãƒ¼ã‚¹å¹¾ä½•ï¼‰ã‚’ äºˆæ¸¬ã‚¿ã‚¤ãƒ å…¥ã‚Š ã§å†è¨ˆç®— â†’ å…¨é¦¬ã¸ä»˜ä¸ =====
try:
    # ãƒ¬ãƒ¼ã‚¹ä»£è¡¨ã‚¿ã‚¤ãƒ ï¼ˆå„é¦¬PredTimeä¸­å¤®å€¤ï¼‰
    race_pred_time = float(pd.to_numeric(df_agg.get('PredTime_s'), errors='coerce').median()) \
                     if 'PredTime_s' in df_agg.columns else np.nan

    # å¹¾ä½•ã®è§£æ±º
    lay_ok, rail_ok, geom, used_d, is_fb = resolve_course_geom(
        COURSE_ID,
        "èŠ" if TARGET_SURFACE == "èŠ" else "ãƒ€",
        int(TARGET_DISTANCE),
        LAYOUT,
        RAIL
    )

    # â† ã“ã“ã¯ try ã®ã€Œä¸­ã€ã«å…¥ã‚Œã‚‹ï¼ˆSyntaxError å›é¿ï¼‰
    if geom is None:
        raise ValueError(
            f"æœªå¯¾å¿œã®ã‚³ãƒ¼ã‚¹è¨­å®š: {COURSE_ID}/{TARGET_SURFACE}/{int(TARGET_DISTANCE)}m/{LAYOUT}-{RAIL}"
        )
    if is_fb:
        st.warning("course_geometry ã«ç™»éŒ²ãŒç„¡ã„è·é›¢ã®ãŸã‚ã€ç°¡æ˜“ã‚¸ã‚ªãƒ¡ãƒˆãƒªã§ PhysS1 ã‚’è¨ˆç®—ã—ã¾ã—ãŸï¼ˆä¿å®ˆçš„ãªæ¨å®šï¼‰ã€‚")

    # â˜… ã“ã“ã‹ã‚‰ PhysS1 æœ¬è¨ˆç®—ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšâ€œå¸¸ã«â€å®Ÿè¡Œ
    fld_size = int(len(horses)) if len(horses) > 0 else None
    races_df_today = []
    for _, r in horses.iterrows():
        gate_no = pd.to_numeric(r.get('ç•ª', np.nan), errors='coerce')
        races_df_today.append({
            'race_id': 'TODAY',
            'course_id': COURSE_ID,
            'surface': "èŠ" if TARGET_SURFACE == "èŠ" else "ãƒ€",
            'distance_m': int(TARGET_DISTANCE),
            'layout': lay_ok,
            'rail_state': rail_ok,
            'num_turns': 2,
            'final_time_sec': race_pred_time if np.isfinite(race_pred_time) else None,
            'gate_no': gate_no,
            'field_size': fld_size,
            'break_loss_sec': 0.0,
            'é¦¬å': r.get('é¦¬å', None)
        })
    races_df_today = pd.DataFrame(races_df_today)

    phys1 = add_phys_s1_features(
        races_df_today,
        group_cols=("race_id",),
        band_col=None,
        verbose=False
    ).rename(columns={
        'phys_corner_load': 'CornerLoadS1',
        'phys_start_cost': 'StartCostS1',
        'phys_finish_grade': 'FinishGradeS1',
        'phys_s1_score': 'PhysS1'
    })

    df_agg = df_agg.merge(
        phys1[['é¦¬å','CornerLoadS1','StartCostS1','FinishGradeS1','PhysS1']],
        on='é¦¬å', how='left'
    )
    df_agg['FinalRaw'] += float(PHYS_S1_GAIN) * pd.to_numeric(df_agg['PhysS1'], errors='coerce').fillna(0.0)

except Exception as e:
    st.warning(f"PhysS1ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    for k in ['CornerLoadS1','StartCostS1','FinishGradeS1','PhysS1']:
        df_agg[k] = np.nan



# PacePtsåæ˜ 
df_agg['PacePts'] = pd.to_numeric(df_agg['PacePts'], errors='coerce').fillna(0.0)
df_agg['FinalRaw'] += float(pace_gain) * df_agg['PacePts']


# === Bias-liteï¼ˆå ´Ã—è·é›¢å¸¯Ã—å›ã‚Šï¼šå†…ä¸­å¤–Ã—è„šè³ªã®è»½ã„è£œæ­£ï¼‰ ===
try:
    s0b = _df[['å ´å','è·é›¢','å›ã‚Š','æ ','è„šè³ª','ç¢ºå®šç€é †']].copy()
    s0b['è·é›¢å¸¯'] = (pd.to_numeric(s0b['è·é›¢'], errors='coerce')/200).round()*200
    s0b['band'] = pd.cut(pd.to_numeric(s0b['æ '], errors='coerce'),
                         bins=[0,3,5,8], labels=['å†…','ä¸­','å¤–'])
    s0b['win'] = (pd.to_numeric(s0b['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int)

    key = (COURSE_ID, int(TARGET_DISTANCE//200*200), TARGET_TURN)
    sub = s0b[(s0b['å ´å']==key[0]) & (s0b['è·é›¢å¸¯']==key[1]) & (s0b['å›ã‚Š']==key[2])].dropna(subset=['band','è„šè³ª'])
    if len(sub) >= 50:
        g = sub.groupby(['band','è„šè³ª'])['win'].agg(['mean','count']).reset_index()
        # Wilsonè£œæ­£ã§ä¸­å¿ƒåŒ–ã—ãŸå·®åˆ† â†’ Â±0.8pt ã®ç¯„å›²ã§åæ˜ 
        z=1.96
        p=g['mean'].to_numpy(float); n=g['count'].to_numpy(float)
        phat=(p + z*z/(2*n)) / (1 + z*z/n)
        adj = (phat - phat.mean())
        bias_map = {}
        for (_, row), val in zip(g.iterrows(), adj):
            bias_map[(row['band'], row['è„šè³ª'])] = float(np.clip(val*4.0, -0.8, 0.8))
        df_agg['BiasPts'] = [
            bias_map.get((
                'å†…' if (int(r.get('æ ',0)) in [1,2,3]) else ('ä¸­' if int(r.get('æ ',0)) in [4,5] else 'å¤–'),
                str(r.get('è„šè³ª',''))
            ), 0.0) for _, r in df_agg.iterrows()
        ]
        df_agg['FinalRaw'] += df_agg['BiasPts'].fillna(0.0)
except Exception:
    pass
# === Traffic-liteï¼ˆçŸ­1è§’Ã—å¤šé ­æ•°Ã—å¤–æ Ã—å‡ºè„šä¸å®‰ã®ç°¡æ˜“ãƒªã‚¹ã‚¯ï¼‰ ===
try:
    field = max(1, len(df_agg))
    short_1c = 1 if LAYOUT in ['å†…å›ã‚Š'] else (0 if LAYOUT in ['å¤–å›ã‚Š','ç›´ç·š'] else 0)
    k1, k2, k3, k4 = 0.06, 0.30, 0.10, 0.20  # ä¿‚æ•°ã¯è»½ã‚ï¼ˆéä¿¡æŠ‘åˆ¶ï¼‰
    risk = []
    for _, r in df_agg.iterrows():
        waku = pd.to_numeric(r.get('æ '), errors='coerce')
        stl  = str(r.get('è„šè³ª',''))
        base = k1*max(0, field-12) + k2*short_1c + (k3*max(0, (waku-5)/3) if np.isfinite(waku) else 0.0)
        esc  = k4*(1.0 if stl in ['é€ƒã’','å…ˆè¡Œ'] else 0.0)  # å‡ºè„šã§é€ƒã’å…ˆè¡Œã¯ç·©å’Œ
        risk.append(np.clip(base - esc, 0.0, 1.5))
    df_agg['TrafficPts'] = risk
    df_agg['FinalRaw'] -= df_agg['TrafficPts'].fillna(0.0)
except Exception:
    pass
# === è‡ªå‹•é‡ã¿èª¿æ•´ï¼ˆä¸ç¢ºå®Ÿæ€§ã§Mathå¯„ã‚Šã‚’å¼±ã‚ã€Spec/Physã‚’å°‘ã—è¶³ã™ï¼‰ ===
if USE_AUTO_BALANCER:
    try:
        # æ—¢å­˜ã®ãƒãƒ©ãƒ³ã‚µä¸­èº«ã¯ãã®ã¾ã¾æµç”¨
        wM, wP = 0.65, 0.35
        neff = pd.to_numeric(df_agg['n_eff_turn'], errors='coerce').fillna(0.0)
        sigma = pd.to_numeric(df_agg.get('PredSigma_s'), errors='coerce')
        sigma_med = float(np.nanmedian(sigma)) if np.isfinite(sigma).any() else 0.0
        u_M = np.clip((1.0/(1.0 + neff/3.0)) + (0.5 * (sigma / (sigma_med + 1e-9)).median(skipna=True)), 0.0, 0.30)

        spec_ratio_safe = float(spec_ratio)
        pace_amp = float(np.abs(df_agg.get('PacePts', 0)).median())
        u_P = np.clip(spec_ratio_safe*0.2 + pace_amp*0.1, 0.0, 0.20)

        wM_ = np.clip(wM - float(u_M), 0.30, 0.80)
        wP_ = 1.0 - wM_

        Z_math = pd.to_numeric(df_agg['FinalRaw'], errors='coerce').fillna(df_agg['FinalRaw'].median())
        Z_math = (Z_math - Z_math.mean()) / (Z_math.std() + 1e-9)

        # â˜…â˜… FIX: ã‚¹ãƒšã‚¯ãƒˆãƒ«å´ã«ã‚‚ UI ã®ä¿‚æ•°ã‚’å¿…ãšæ›ã‘ã‚‹
        Z_spec = (
            float(spectral_weight_ui)
            * pd.to_numeric(df_agg.get('SpecFitZ'), errors='coerce').fillna(0.0)
        )

        Z_phys = ((pd.to_numeric(df_agg.get('PhysicsZ'), errors='coerce') - 50.0) / 10.0).fillna(0.0)
        Z_ph   = (spec_ratio_safe * Z_spec + (1.0 - spec_ratio_safe) * Z_phys)

        Z_final = wM_ * Z_math + wP_ * Z_ph
        df_agg['FinalRaw'] = 50 + 10 * ((Z_final - np.nanmean(Z_final)) / (np.nanstd(Z_final) + 1e-9))
    except Exception:
        pass

# === 2æ­³æˆ¦: ç€é †MCãƒˆã‚°ãƒ« ===
if USE_MC:
    # ===== Monte Carlo (FinalRawãƒ™ãƒ¼ã‚¹ã®ç€é †ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) =====
    try:
        rng_mc = np.random.default_rng(20251102)
        sims = 500000  # å›æ•°ã¯å¥½ãã«å¢—ã‚„ã—ã¦OK

        names_mc = df_agg['é¦¬å'].tolist()
        score_mc = pd.to_numeric(df_agg['FinalRaw'], errors='coerce').to_numpy(float)
        n_h = len(names_mc)

        # Gumbelãƒã‚¤ã‚ºã‚’è¶³ã—ã¦é †ä½ã«ã™ã‚‹
        win_ct = np.zeros(n_h, int)
        top3_ct = np.zeros(n_h, int)
        rank_sum = np.zeros(n_h, float)

        for _ in range(sims):
            # ã‚¹ã‚³ã‚¢ã®å¤§ãã„æ–¹ãŒå¼·ã„å‰æ
            noise = rng_mc.gumbel(loc=0.0, scale=1.0, size=n_h)
            sample = score_mc + noise
            order = np.argsort(-sample)  # é™é †
            # 1ç€
            win_ct[order[0]] += 1
            # 3ç€ã¾ã§
            for k in range(min(3, n_h)):
                top3_ct[order[k]] += 1
            # å„é¦¬ã®é †ä½ã‚’è¶³ã™ï¼ˆæœŸå¾…ç€é †ç”¨ï¼‰
            inv = np.empty(n_h, int)
            inv[order] = np.arange(1, n_h+1)
            rank_sum += inv

        df_agg['å‹ç‡%_MC'] = (win_ct / sims * 100).round(2)
        df_agg['è¤‡å‹ç‡%_MC'] = (top3_ct / sims * 100).round(2)
        df_agg['æœŸå¾…ç€é †_MC'] = (rank_sum / sims).round(3)

    except Exception as e:
        st.warning(f"Monte Carloãƒ–ãƒ­ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {e}")
else:
    if 'df_agg' in locals():
        for c in ('å‹ç‡%_MC','è¤‡å‹ç‡%_MC','æœŸå¾…ç€é †_MC'):
            df_agg[c] = np.nan

# OFFã®ã¨ãã¯ FinalRaw_before_balance ã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆâ€»ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„ï¼‰


# ===== BTå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ï¼ˆé€±Ã—ã‚¯ãƒ©ã‚¹ãƒ­ãƒã‚¹ãƒˆå€¤ã‚‚ä½µè¨˜å¯ï¼‰=====
bt_base = _df[['é¦¬å','ç¢ºå®šç€é †','rid_hist','ãƒ¬ãƒ¼ã‚¹æ—¥','ClassKey','WeekKey','ScoreT_RB']].dropna(subset=['é¦¬å','ç¢ºå®šç€é †','rid_hist']).copy()
bt_base['ç¢ºå®šç€é †'] = pd.to_numeric(bt_base['ç¢ºå®šç€é †'], errors='coerce')
bt_base = bt_base[bt_base['ç¢ºå®šç€é †'] >= 1]

bt_base = (
    bt_base
    .groupby('rid_hist', group_keys=False)
    .filter(lambda g: len(g) >= 2 and g['ç¢ºå®šç€é †'].nunique() >= 2)
)
if bt_base.empty:
    # ã“ã“ã§æ­¢ã‚ãªã„ã€‚å¾Œæ®µã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å‡ç­‰ã«è½ã¨ã™
    pass
# æ™‚ç³»åˆ—é‡ã¿ï¼ˆä¾‹ï¼šãƒ­ãƒã‚¹ãƒˆã«å¯„ã›ã¦ã‚„ã‚„ç·©ã‚ï¼‰
bt_base['_w_time'] = 0.5 ** ((pd.Timestamp.today() - pd.to_datetime(bt_base['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0) / (half_life_m*30.4375 if half_life_m>0 else 180.0))

# è¿½åŠ ã§â€œé‡è¦ãƒ¬ãƒ¼ã‚¹â€ã«é‡ã¿ã‚’è¶³ã™ï¼ˆä¾‹ï¼šG1/G2ã«+20%ãªã©ï¼‰
def _class_boost(c):
    return 1.2 if c in ['G1','G2'] else (1.1 if c=='G3' else 1.0)
bt_base['_w_cls'] = bt_base['ClassKey'].map(_class_boost).fillna(1.0)

bt_base['_w'] = bt_base['_w_time'] * bt_base['_w_cls']

# ===== ãƒšã‚¢å‹æ•— â†’ BTå­¦ç¿’ =====
names_bt, N_bt, M_bt = _build_pairwise_from_ranks(
    bt_base, rid_col='rid_hist', name_col='é¦¬å', rank_col='ç¢ºå®šç€é †', weight_col='_w'
)
worth_bt = fit_bradley_terry(names_bt, N_bt, M_bt)  # Series(index=é¦¬å, value=worth)

# â˜… è¿½è¨˜: è¡Œåˆ—ãŒå®Ÿè³ªã‚¼ãƒ­ or å­¦ç¿’çµæœãŒç©ºãªã‚‰ã€å‡ç­‰é‡ã¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
try:
    total_comp = float(np.nansum(M_bt)) if isinstance(M_bt, np.ndarray) else 0.0
except Exception:
    total_comp = 0.0

if (worth_bt is None) or (len(worth_bt) == 0) or (not np.isfinite(total_comp) or total_comp <= 0):
    worth_bt = pd.Series(1.0, index=names_bt, dtype=float)

# ===== ä»Šæ—¥ã®å‹ç‡ï¼ˆBTï¼‰ =====
runners = horses['é¦¬å'].astype(str).tolist()

# å­¦ç¿’çµæœãŒç©º/ã»ã¼ã‚¼ãƒ­ã®ã¨ãã¯å‡ç­‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
try:
    total_comp = float(np.nansum(M_bt)) if isinstance(M_bt, np.ndarray) else 0.0
except Exception:
    total_comp = 0.0
if (worth_bt is None) or (len(worth_bt) == 0) or (not np.isfinite(total_comp) or total_comp <= 0):
    worth_bt = pd.Series(1.0, index=names_bt, dtype=float)

# ä»Šæ—¥ã®å‡ºèµ°é¦¬ã«å¯¾å¿œã™ã‚‹å¼·ã•ãƒ™ã‚¯ãƒˆãƒ«
w_today = pd.Series([worth_bt.get(nm, np.nan) for nm in runners], index=runners, dtype=float)

# ã™ã¹ã¦æ¬ æâ†’å‡ç­‰ã€æ¬ æâ†’ãƒãƒƒã‚¯ã‚ªãƒ•
if not np.isfinite(w_today).any():
    w_today = pd.Series(1.0, index=runners, dtype=float)
w_back = float(np.nanmean(w_today)) if np.isfinite(np.nanmean(w_today)) else (1.0 / max(len(runners), 1))
w_today = w_today.fillna(0.8 * w_back)

# æ­£è¦åŒ–
p_today = w_today / max(w_today.sum(), 1e-12)

# â˜… dictâ†’map ã§â€œå®‰å®šå½“ã¦è¾¼ã¿â€ï¼ˆNone æ··å…¥ã‚’é˜²ãï¼‰
bt_map = p_today.to_dict()
df_agg['å‹ç‡%_BT'] = (
    df_agg['é¦¬å'].astype(str).map(lambda n: bt_map.get(str(n), np.nan))
    .astype(float).mul(100.0).round(2)
)

# å¿µã®ãŸã‚ï¼šæ•°å€¤åŒ–ã‚’å¼·åˆ¶ï¼ˆStyler ã® "None" è¡¨ç¤ºå›é¿ï¼‰
df_agg['å‹ç‡%_BT'] = pd.to_numeric(df_agg['å‹ç‡%_BT'], errors='coerce')



# ===== å‹ç‡ï¼ˆPLè§£æè§£ï¼‰ï¼† Top3ï¼ˆGumbelåå¯¾ç§°ï¼‰ =====
calibrator = None
if do_calib and SK_ISO:
    dfh = _df.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid'] = _make_race_id_for_hist(dfh)
    X, Y = [], []
    for _, g in dfh.groupby('rid'):
        xs = g['score_adj'].astype(float).to_numpy()
        y  = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
        if len(xs) >= 2 and np.unique(y).size >= 2:
            p = np.exp(beta_pl * (xs - xs.max()))
            s = p.sum()
            if np.isfinite(s) and s > 0:
                p = np.clip(p / s, 1e-6, 1-1e-6)
                X.append(p); Y.append(y)
    if X:
        calibrator = IsotonicRegression(out_of_bounds='clip').fit(np.concatenate(X), np.concatenate(Y))

S = pd.to_numeric(df_agg['FinalRaw'], errors='coerce')
finite = np.isfinite(S)

p_win = np.zeros(len(S), float)
if finite.any():
    A = S[finite].to_numpy(float)
    m = A.mean(); s = A.std() + 1e-9
    z = (A - m) / s
    x = beta_pl * (z - z.max())
    ex = np.exp(x)
    p = ex / ex.sum()
    p_win[finite] = p
    if (~finite).any():
        p_win[~finite] = 1e-6
        p_win = p_win / p_win.sum()
else:
    p_win[:] = 1.0 / max(len(S), 1)

if calibrator is not None:
    p_win = safe_iso_predict(calibrator, p_win)

df_agg['å‹ç‡%_PL'] = (100 * p_win).round(2)

# Top3è¿‘ä¼¼ï¼ˆGumbelåå¯¾ç§°ï¼‰
if finite.any():
    A = S.copy()
    A[~finite] = A[finite].mean()
    m = A.mean(); s = A.std() + 1e-9
    abilities = ((A - m) / s).to_numpy(float)
else:
    abilities = np.zeros(len(S), float)

draws_top3 = 80000
rng3 = np.random.default_rng(13579)
G = rng3.gumbel(size=(draws_top3//2, len(abilities)))
U = np.vstack([beta_pl*abilities[None,:] + G, beta_pl*abilities[None,:] - G])
rank_idx = np.argsort(-U, axis=1)
counts = np.zeros(len(abilities), float)
for k in range(3):
    counts += np.bincount(rank_idx[:,k], minlength=len(abilities)).astype(float)
df_agg['è¤‡å‹ç‡%_PL'] = (100*(counts / draws_top3)).round(2)

# ===== H) AR100 =====
S = pd.to_numeric(df_agg['FinalRaw'], errors='coerce')
S = S.fillna(S.median())

# --- ä¸ç¢ºå®Ÿæ€§ã«åŸºã¥ã Z ç¸®ç´„ï¼ˆä¸ŠæŒ¯ã‚ŒæŠ‘åˆ¶ï¼‰ ---
sig = pd.to_numeric(df_agg.get('PredSigma_s'), errors='coerce')  # äºˆæ¸¬ã‚¿ã‚¤ãƒ ã®ä¸ç¢ºã‹ã•
sig_med = float(np.nanmedian(sig)) if np.isfinite(sig).any() else 0.0
neff = pd.to_numeric(df_agg['n_eff_turn'], errors='coerce').fillna(0.0)

# ç¸®ç´„ä¿‚æ•° u: 0< u â‰¤1
u = 1.0 / np.sqrt(
        1.0
        + (AR_SHRINK_SIGMA * (sig / (sig_med + 1e-9)))**2
        + (AR_SHRINK_NEFF  / (1.0 + neff))  # ãƒ‡ãƒ¼ã‚¿è–„ã„ã»ã©å¯„ä¸â†‘
    )
u = np.clip(u.fillna(1.0), 0.5, 1.0)

# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å†…Zï¼ˆç¸®ç´„å¾Œï¼‰
mu = float(S.mean()); sd = float(S.std(ddof=0) or 1.0)
z = (S - mu) / (sd + 1e-9)
z_eff = z * u

def logistic(x):
    return 1.0 / (1.0 + np.exp(-x))

if AR_MODE.startswith("å³æ ¼"):
    # 1) åŸºç¤ã‚¹ã‚³ã‚¢ï¼ˆerfã§å¼·åœ§ç¸®ï¼šä¸ŠãŒå‡ºã«ãã„ï¼‰
    base = 50.0 + AR_BASE_A * np.vectorize(math.erf)(
        z_eff / (np.sqrt(2.0) * AR_STRICT_S)
    )
    base = np.clip(base, 0.0, 99.999)

    # 2) å°¾éƒ¨ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆãƒˆãƒƒãƒ—ã ã‘ 100 ã«ç·©ã‚„ã‹ã«è¿‘ã¥ããƒ»é€£ç¶šé–¢æ•°ï¼‰
    #    sig ~ logistic((z - T)/Ï„) ã® p ä¹—ã§éå¸¸ã«å³ã—ã
    sigm = logistic((z_eff - TAIL_THRESH) / TAIL_TAU)
    tail_gain = (100.0 - base) * np.power(sigm, TAIL_POW)
    ar = np.clip(base + tail_gain, 0.0, 100.0)
    df_agg['AR100'] = ar

elif AR_MODE.startswith("ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹"):
    # å¾“æ¥Zâ†’0..100ï¼ˆä¸ŠãŒå‡ºã‚„ã™ã„ï¼‰
    ar = 50.0 + 10.0 * (S - mu) / (sd or 1.0)
    df_agg['AR100'] = ar.clip(0, 100)

else:
    # é †ä½ãƒ™ãƒ¼ã‚¹ï¼ˆä¸Šé™ã¯100ã®ã¾ã¾ã€ãŸã ã—ä¸Šä½å·®åˆ†ã¯Î³ã§åœ§ç¸®ï¼‰
    ranks = S.rank(method='average', pct=True).fillna(0.5).to_numpy(float)
    ranks_eff = np.power(ranks, float(AR_RANK_GAMMA))
    df_agg['AR100'] = np.interp(ranks_eff, [0.0, 1.0], [20.0, 100.0])

# --- ãƒãƒ³ãƒ‰ï¼ˆå³ã—ã‚ï¼‰ ---
def to_band(v):
    if not np.isfinite(v): return 'E'
    if v >= 96: return 'SS'
    if v >= 90: return 'S'
    if v >= 82: return 'A'
    if v >= 74: return 'B'
    if v >= 66: return 'C'
    return 'E'

df_agg['Band'] = df_agg['AR100'].map(to_band)


# ===== ãƒ†ãƒ¼ãƒ–ãƒ«æ•´å½¢ï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ä»˜ãï¼‰ =====
# â–¼ ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ—ã‚’è¿½åŠ ã—ãŸå®Œæˆç‰ˆ
_dfdisp = df_agg.copy().sort_values(['AR100','å‹ç‡%_PL'], ascending=[False, False]).reset_index(drop=True)
_dfdisp['é †ä½'] = np.arange(1, len(_dfdisp)+1)

# ---- ã‚¯ã‚¤ãƒƒã‚¯ã‚µãƒãƒªãƒ¼ï¼ˆä¸Šä½ã®è¦ç´„ï¼‰ ----
if '_dfdisp' not in globals() or _dfdisp.empty:
    st.error("é›†è¨ˆçµæœãŒç©ºã§ã™ã€‚å…¥åŠ›ã‚„ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# è¡¨ç¤ºãƒ©ãƒ™ãƒ«ï¼ˆæœ€å°é™ï¼‰
JP = {
    'é †ä½':'é †ä½','æ ':'æ ','ç•ª':'é¦¬ç•ª','é¦¬å':'é¦¬å','è„šè³ª':'è„šè³ª',
    'AR100':'AR100','Band':'è©•ä¾¡å¸¯',
    'å‹ç‡%_PL':'å‹ç‡%ï¼ˆPLï¼‰','è¤‡å‹ç‡%_PL':'è¤‡å‹ç‡%ï¼ˆPLï¼‰',
    'å‹ç‡%_TIME':'å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','è¤‡å‹ç‡%_TIME':'è¤‡å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','æœŸå¾…ç€é †_TIME':'æœŸå¾…ç€é †ï¼ˆã‚¿ã‚¤ãƒ ï¼‰',
    'PredTime_s':'äºˆæ¸¬ã‚¿ã‚¤ãƒ ä¸­å¤®å€¤[s]','PredTime_p20':'20%é€Ÿã„å´[s]','PredTime_p80':'80%é…ã„å´[s]','PredSigma_s':'ã‚¿ã‚¤ãƒ åˆ†æ•£Ïƒ[s]',
    'RecencyZ':'è¿‘èµ°Z','StabZ':'å®‰å®šæ€§Z','PacePts':'ãƒšãƒ¼ã‚¹Pts','TurnPrefPts':'å›ã‚ŠåŠ ç‚¹','DistTurnZ':'è·é›¢Ã—å›ã‚ŠZ',
    'SpecFitZ':'ã‚¹ãƒšã‚¯ãƒˆãƒ«é©åˆZ',
    'SpecGate_horse_lbl':'èµ°æ³•å‹(é¦¬)','SpecGate_templ_lbl':'èµ°æ³•å‹(ãƒ†ãƒ³ãƒ—ãƒ¬)',
    'PhysicsZ':'ç‰©ç†Z','PhysS1':'PhysS1','CornerLoadS1':'ã‚³ãƒ¼ãƒŠãƒ¼è·é‡','StartCostS1':'ã‚¹ã‚¿ãƒ¼ãƒˆæå¤±','FinishGradeS1':'ã‚´ãƒ¼ãƒ«å‰å‹¾é…',
    'å‹ç‡%_BT':'å‹ç‡%ï¼ˆBTï¼‰','å‹ç‡%_MC':'å‹ç‡%ï¼ˆMCï¼‰','è¤‡å‹ç‡%_MC':'è¤‡å‹ç‡%ï¼ˆMCï¼‰','æœŸå¾…ç€é †_MC':'æœŸå¾…ç€é †ï¼ˆMCï¼‰',
}

# ===== å°ï¼ˆèƒ½åŠ›ä¸Šä½é †ï¼‰=====
# ãƒ«ãƒ¼ãƒ«: â—1é ­, ã€‡1é ­, â–²1é ­, â˜†1é ­, â–³3é ­, æ®‹ã‚Šã¯ã€Œæ¶ˆã€
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ AR100 â†’ åŒå€¤ã¯ å‹ç‡%_PL â†’ RecencyZ ã®é †ã§ã‚¿ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¯
rank_key = ['AR100','å‹ç‡%_PL','RecencyZ']
for k in rank_key:
    if k not in _dfdisp.columns:
        _dfdisp[k] = np.nan
_dfdisp['_sort_tuple'] = list(
    zip(_dfdisp['AR100'].fillna(-1e9),
        _dfdisp['å‹ç‡%_PL'].fillna(-1e9),
        _dfdisp['RecencyZ'].fillna(-1e9))
)
_dfdisp = _dfdisp.sort_values('_sort_tuple', ascending=False).reset_index(drop=True)

marks = ['â—','ã€‡','â–²','â˜†','â–³','â–³','â–³']
å° = ['æ¶ˆ'] * len(_dfdisp)
for i, m in enumerate(marks):
    if i < len(å°):
        å°[i] = m
_dfdisp['å°'] = å°

# ä¸»è¦è¡¨ç¤ºåˆ—
disp_cols = [
    'é †ä½','å°','æ ','ç•ª','é¦¬å','è„šè³ª',
    'AR100','Band','å‹ç‡%_PL','è¤‡å‹ç‡%_PL',
    'å‹ç‡%_TIME','è¤‡å‹ç‡%_TIME','æœŸå¾…ç€é †_TIME',
    'PredTime_s','PredSigma_s',
    'SpecFitZ','SpecGate_horse_lbl','SpecGate_templ_lbl',
    'PhysicsZ','PhysS1','PacePts','TurnPrefPts','DistTurnZ'
]
disp_cols = [c for c in disp_cols if c in _dfdisp.columns]

table = _dfdisp[disp_cols].copy()
table = table.rename(columns=JP)

render_final_view(table)

# ===== è³¼å…¥æ¡ˆï¼ˆä¸Šä½6é ­ãƒœãƒƒã‚¯ã‚¹ / ä¸€é ­è»¸ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®é››å½¢ï¼‰=====
top6 = _dfdisp.head(6)['é¦¬å'].tolist()
head = _dfdisp.iloc[0]['é¦¬å'] if len(_dfdisp)>0 else None

with st.expander("ğŸ§¾ è²·ã„ç›®ã²ãªå½¢ï¼ˆä¸Šä½6é ­ãƒ™ãƒ¼ã‚¹ï¼‰", expanded=True):
    st.markdown(f"- ä¸Šä½6é ­ï¼ˆãƒœãƒƒã‚¯ã‚¹å€™è£œï¼‰: **{', '.join(top6)}**" if top6 else "- ä¸Šä½6é ­ãªã—")
    if head:
        st.markdown(f"- ä¸€é ­è»¸ï¼ˆè»¸å€™è£œï¼‰: **{head}**")

# ===== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆCSV / JSONï¼‰=====
export_cols = [c for c in _dfdisp.columns if c != '_sort_tuple']
res_csv = _dfdisp[export_cols].to_csv(index=False).encode('utf-8-sig')
st.download_button("â¬‡ï¸ çµæœCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=res_csv, file_name="result_rikeiba.csv", mime="text/csv")

# JSONï¼ˆè»½é‡ã‚µãƒãƒªãƒ¼ï¼‰
json_cols = ['å°','æ ','ç•ª','é¦¬å','è„šè³ª','AR100','å‹ç‡%_PL','è¤‡å‹ç‡%_PL','PredTime_s','PredSigma_s','SpecFitZ','PhysicsZ','PhysS1']
json_cols = [c for c in json_cols if c in _dfdisp.columns]
res_json = _dfdisp[json_cols].to_dict(orient='records')
st.download_button("â¬‡ï¸ çµæœJSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=json.dumps(res_json, ensure_ascii=False, indent=2).encode('utf-8'),
                   file_name="result_rikeiba.json", mime="application/json")

# ===== ã¡ã‚‡ã„å¯è¦–åŒ–ï¼ˆPlotlyç‰ˆ AR100 vs é¦¬ç•ªï¼‰=====
st.markdown("## ğŸ“ˆ AR100 ã¨ é¦¬ç•ªã®é–¢ä¿‚")
show_labels = st.checkbox("ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆé‡ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰", value=False)
df_plot = _dfdisp.copy()
if 'ç•ª' in df_plot.columns and 'é¦¬ç•ª' not in df_plot.columns:
    df_plot['é¦¬ç•ª'] = df_plot['ç•ª']
fig = plot_ar_scatter(
    df_plot,
    x_col="é¦¬ç•ª" if "é¦¬ç•ª" in df_plot.columns else "ç•ª",
    y_col="AR100",
    name_col="é¦¬å",
    waku_col="æ ",
    show_text=show_labels,
)
st.plotly_chart(fig, use_container_width=True)

st.markdown("#### å°ï¼ˆã‚³ãƒ”ãƒšç”¨ï¼‰")
marks_text = build_marks_text(
    _dfdisp,
    name_col="é¦¬å",
    mark_col="å°",
    ar_col="AR100",
    ar_label="AR",
    ar_only_for_top=True,
)
st.code(marks_text, language="text")

st.success("å®Œäº†ï¼šã‚¹ã‚³ã‚¢ç®—å‡ºãƒ»å°ä»˜ã‘ãƒ»å‡ºåŠ›ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")

