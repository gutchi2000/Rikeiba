# -*- coding: utf-8 -*-
# ç«¶é¦¬äºˆæƒ³ã‚¢ãƒ—ãƒªï¼ˆAUTOçµ±åˆç‰ˆ + ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼‰

import os, sys

# åŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å„ªå…ˆã—ã¦èª­ã‚ã‚‹ã‚ˆã†ã«ã™ã‚‹
BASE = os.path.dirname(os.path.abspath(__file__))
if BASE not in sys.path:
    sys.path.insert(0, BASE)

# -- å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆã“ã“ã§å®£è¨€ã—ã¦ãŠãï¼‰ --
from course_geometry import register_all_turf, get_course_geom
from physics_sprint1 import add_phys_s1_features  # â† å…ˆé ­ã§import

import streamlit as st

@st.cache_resource
def _boot_course_geom():
    register_all_turf()
    return True
_boot_course_geom()

# ï¼ˆå¿…è¦ãªã‚‰ï¼‰ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã¯ç„¡åŠ¹åŒ–ã—ã¦æ®‹ã™
if False:
    geom = get_course_geom(course_id="æ±äº¬", surface="èŠ", distance_m=1600, layout="å¤–å›ã‚Š", rail_state="A")
    # course_geometry ã«è¿½åŠ é–¢æ•°ãŒã‚ã‚‹ç’°å¢ƒã ã‘è©¦ã™
    try:
        import course_geometry as cg
        if hasattr(cg, "estimate_tci"):
            tci = cg.estimate_tci(geom)
    except Exception:
        pass

# â€» ã“ã“ã§ races_df ã«å¯¾ã—ã¦ add_phys_s1_features ã‚’å³æ™‚å®Ÿè¡Œã—ãªã„ã“ã¨ï¼
#   å®Ÿè¡Œã¯å¾ŒåŠã® UIï¼ˆğŸ§ª PhysS1 ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆï¼‰å†…ã§ã®ã¿è¡Œã„ã¾ã™ã€‚



# keiba_web_app.py å†’é ­ã® import ç¾¤ã®ç›´å¾Œ
import sys, os
BASE = os.path.dirname(os.path.abspath(__file__))
if BASE not in sys.path:
    sys.path.insert(0, BASE)

from course_geometry import register_all_turf, get_course_geom
from physics_sprint1 import add_phys_s1_features

import streamlit as st

@st.cache_resource
def _boot_course_geom():
    register_all_turf()
    return True
_boot_course_geom()
# ---- optional ----
try:
    import altair as alt
    ALT_AVAILABLE = True
except Exception:
    ALT_AVAILABLE = False

try:
    import streamlit.components.v1 as components
    COMPONENTS = True
except Exception:
    COMPONENTS = False

try:
    from sklearn.isotonic import IsotonicRegression
    SK_ISO = True
except Exception:
    SK_ISO = False

# ===== æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ =====
from matplotlib import font_manager
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = [
    'IPAexGothic','IPAGothic','Noto Sans CJK JP','Yu Gothic UI','Meiryo','Hiragino Sans','MS Gothic'
]

st.set_page_config(page_title="Rikeiba", layout="wide")

# ===== å°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
STYLES = ['é€ƒã’','å…ˆè¡Œ','å·®ã—','è¿½è¾¼']
_fw = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')

STYLE_ALIASES = {
    'è¿½ã„è¾¼ã¿':'è¿½è¾¼','è¿½è¾¼ã¿':'è¿½è¾¼','ãŠã„ã“ã¿':'è¿½è¾¼','ãŠã„è¾¼ã¿':'è¿½è¾¼',
    'ã•ã—':'å·®ã—','å·®è¾¼':'å·®ã—','å·®è¾¼ã¿':'å·®ã—',
    'ã›ã‚“ã“ã†':'å…ˆè¡Œ','å…ˆè¡Œ ':'å…ˆè¡Œ','å…ˆè¡Œã€€':'å…ˆè¡Œ',
    'ã«ã’':'é€ƒã’','é€ƒã’ ':'é€ƒã’','é€ƒã’ã€€':'é€ƒã’'
}

def normalize_style(s: str) -> str:
    s = str(s).replace('ã€€','').strip().translate(_fw)
    s = STYLE_ALIASES.get(s, s)
    return s if s in STYLES else ''

@st.cache_resource
def get_jp_font():
    for p in [
        'ipaexg.ttf',
        '/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf',
        '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc',
        '/System/Library/Fonts/Hiragino Sans W3.ttc',
        'C:/Windows/Fonts/meiryo.ttc',
    ]:
        if os.path.exists(p):
            try:
                font_manager.fontManager.addfont(p)
            except Exception:
                pass
            return font_manager.FontProperties(fname=p)
    return None

jp_font = get_jp_font()
if jp_font is not None:
    try:
        plt.rcParams['font.family'] = jp_font.get_name()
    except Exception:
        pass

# ===== ã‚¹ã‚¿ã‚¤ãƒ«é–¢ä¿‚ =====
WAKU_COL = {1:"#ffffff",2:"#000000",3:"#e6002b",4:"#1560bd",5:"#ffd700",6:"#00a04b",7:"#ff7f27",8:"#f19ec2"}

def _style_waku(s: pd.Series):
    out=[]
    for v in s:
        if pd.isna(v):
            out.append("")
        else:
            v=int(v); bg=WAKU_COL.get(v,"#fff"); fg="#000" if v==1 else "#fff"
            out.append(f"background-color:{bg}; color:{fg}; font-weight:700; text-align:center;")
    return out

# ===== é–¢æ•°ç¾¤ =====

def season_of(m: int) -> str:
    if 3<=m<=5: return 'æ˜¥'
    if 6<=m<=8: return 'å¤'
    if 9<=m<=11: return 'ç§‹'
    return 'å†¬'

def z_score(s: pd.Series) -> pd.Series:
    s = pd.to_numeric(s, errors='coerce')
    std = s.std(ddof=0)
    if not np.isfinite(std) or std==0: return pd.Series([50]*len(s), index=s.index)
    return 50 + 10*(s - s.mean())/std

def _parse_time_to_sec(x):
    if x is None or (isinstance(x,float) and np.isnan(x)): return np.nan
    s = str(x).strip()
    m = re.match(r'^(\d+):(\d+)\.(\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + float('0.'+m.group(3))
    m = re.match(r'^(\d+)[\.:](\d+)[\.:](\d+)$', s)
    if m: return int(m.group(1))*60 + int(m.group(2)) + int(m.group(3))/10
    try: return float(s)
    except: return np.nan

def _trim_name(x):
    try: return str(x).replace('\u3000',' ').strip()
    except: return str(x)

def w_std_unbiased(x, w, ddof=1):
    x=np.asarray(x,float); w=np.asarray(w,float)
    sw=w.sum()
    if not np.isfinite(sw) or sw<=0: return np.nan
    m=np.sum(w*x)/sw
    var=np.sum(w*(x-m)**2)/sw
    n_eff=(sw**2)/np.sum(w**2) if np.sum(w**2)>0 else 0
    if ddof and n_eff>ddof: var*= n_eff/(n_eff-ddof)
    return float(np.sqrt(max(var,0.0)))

def ndcg_by_race(frame: pd.DataFrame, scores, k: int=3) -> float:
    f = frame[['race_id','y']].copy().reset_index(drop=True)
    s = np.asarray(scores,float)
    if len(s)!=len(f): s=s[:len(f)]
    vals=[]
    for _, idx in f.groupby('race_id').groups.items():
        idx=np.asarray(list(idx),int)
        y_true=np.nan_to_num(f.loc[idx,'y'].astype(float).to_numpy(), nan=0.0)
        y_pred=np.nan_to_num(s[idx].astype(float), nan=0.0)
        m=len(idx)
        if m==0: continue
        if m==1:
            vals.append(1.0 if y_true[0]>0 else 0.0); continue
        kk=int(min(max(1,k),m))
        order=np.argsort(-y_pred)
        gains=(2.0**y_true[order]-1.0)
        discounts=1.0/np.log2(np.arange(2,m+2))
        dcg=float(np.sum(gains[:kk]*discounts[:kk]))
        order_best=np.argsort(-y_true)
        gains_best=(2.0**y_true[order_best]-1.0)
        idcg=float(np.sum(gains_best[:kk]*discounts[:kk]))
        vals.append(dcg/idcg if idcg>0 else 0.0)
    return float(np.mean(vals)) if vals else float('nan')

def safe_iso_predict(ir, p_vec: np.ndarray) -> np.ndarray:
    x = np.asarray(p_vec, float)
    x = np.nan_to_num(x, nan=1.0 / max(len(x), 1), posinf=1 - 1e-6, neginf=1e-6)
    x = np.clip(x, 1e-6, 1 - 1e-6)
    try:
        y = ir.predict(x)
        y = np.nan_to_num(y, nan=x.mean(), posinf=1 - 1e-6, neginf=1e-6)
        y = np.clip(y, 1e-6, 1 - 1e-6)
        s = y.sum()
        return (y / s) if s > 0 else x
    except Exception:
        return x

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ =====
st.sidebar.title("âš™ï¸ ãƒ‘ãƒ©ãƒ¡ã‚¿è¨­å®šï¼ˆAUTOçµ±åˆï¼‰")
MODE = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰", ["AUTOï¼ˆæ¨å¥¨ï¼‰","æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰"], index=0, horizontal=True)

with st.sidebar.expander("ğŸ”° åŸºæœ¬", expanded=True):
    lambda_part  = st.slider("å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹ Î»", 0.0, 1.0, 0.5, 0.05)
    grade_bonus  = st.slider("é‡è³å®Ÿç¸¾ãƒœãƒ¼ãƒŠã‚¹", 0, 20, 5)
    agari1_bonus = st.slider("ä¸ŠãŒã‚Š3F 1ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 10, 3)
    agari2_bonus = st.slider("ä¸ŠãŒã‚Š3F 2ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 5, 2)
    agari3_bonus = st.slider("ä¸ŠãŒã‚Š3F 3ä½ãƒœãƒ¼ãƒŠã‚¹", 0, 3, 1)

with st.sidebar.expander("æœ¬ãƒ¬ãƒ¼ã‚¹æ¡ä»¶", expanded=True):
    grade_opts = ["G1", "G2", "G3", "L", "OP", "3å‹ã‚¯ãƒ©ã‚¹"]
    TARGET_GRADE = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®æ ¼", grade_opts, index=grade_opts.index("OP"))
    TARGET_SURFACE  = st.selectbox("æœ¬ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´", ["èŠ","ãƒ€"], index=0)
    TARGET_DISTANCE = st.number_input("æœ¬ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ [m]", 1000, 3600, 1800, 100)
    TARGET_TURN     = st.radio("å›ã‚Š", ["å³","å·¦"], index=0, horizontal=True)
    
with st.sidebar.expander("ğŸ“ æœ¬ãƒ¬ãƒ¼ã‚¹å¹¾ä½•ï¼ˆã‚³ãƒ¼ã‚¹è¨­å®šï¼‰", expanded=True):
    VENUES = ["æœ­å¹Œ","å‡½é¤¨","ç¦å³¶","æ–°æ½Ÿ","æ±äº¬","ä¸­å±±","ä¸­äº¬","äº¬éƒ½","é˜ªç¥","å°å€‰"]
    COURSE_ID = st.selectbox("ç«¶é¦¬å ´", VENUES, index=VENUES.index("æ±äº¬"))

    LAYOUT_OPTS = {
        "æœ­å¹Œ":["å†…å›ã‚Š"], "å‡½é¤¨":["å†…å›ã‚Š"], "ç¦å³¶":["å†…å›ã‚Š"],
        "æ–°æ½Ÿ":["å†…å›ã‚Š","å¤–å›ã‚Š","ç›´ç·š"], "æ±äº¬":["å¤–å›ã‚Š"],
        "ä¸­å±±":["å†…å›ã‚Š","å¤–å›ã‚Š"], "ä¸­äº¬":["å¤–å›ã‚Š"],
        "äº¬éƒ½":["å†…å›ã‚Š","å¤–å›ã‚Š"], "é˜ªç¥":["å†…å›ã‚Š","å¤–å›ã‚Š"], "å°å€‰":["å†…å›ã‚Š"]
    }
    LAYOUT = st.selectbox("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ", LAYOUT_OPTS[COURSE_ID])
    RAIL = st.selectbox("ã‚³ãƒ¼ã‚¹åŒºåˆ†ï¼ˆA/B/C/Dï¼‰", ["A","B","C","D"], index=0)

    # â† ã“ã“ã§å ´ã«é€£å‹•ã—ã¦æ—¢å®šã®å›ã‚Šã‚’å‡ºã™
    DEFAULT_VENUE_TURN = {'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦','ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'}
    _turn_default = DEFAULT_VENUE_TURN.get(COURSE_ID, 'å³')
    TARGET_TURN = st.radio("å›ã‚Š", ["å³","å·¦"], index=(0 if _turn_default=="å³" else 1), horizontal=True)

    TODAY_BAND = st.select_slider("é€šéå¸¯åŸŸï¼ˆæš«å®šï¼‰", options=["å†…","ä¸­","å¤–"], value="ä¸­")


with st.sidebar.expander("ğŸ§® ç‰©ç†(Sprint1)ã®é‡ã¿", expanded=True):
    PHYS_S1_GAIN = st.slider("PhysS1åŠ ç‚¹ã®å¼·ã•", 0.0, 3.0, 1.0, 0.1)

with st.sidebar.expander("ğŸ›  å®‰å®šåŒ–/è£œæ­£", expanded=True):
    half_life_m  = st.slider("æ™‚ç³»åˆ—åŠæ¸›æœŸ(æœˆ)", 0.0, 12.0, 6.0, 0.5)
    stab_weight  = st.slider("å®‰å®šæ€§(å°ã•ã„ã»ã©â—)ã®ä¿‚æ•°", 0.0, 2.0, 0.7, 0.1)
    pace_gain    = st.slider("ãƒšãƒ¼ã‚¹é©æ€§ä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    weight_coeff = st.slider("æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åº¦(pts/kg)", 0.0, 4.0, 1.0, 0.1)
    
with st.sidebar.expander("ğŸ§© ç‰¹æ€§é‡ã¿ï¼ˆä»»æ„ï¼‰", expanded=False):
    # â”€â”€ æ€§åˆ¥ï¼ˆ0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    SEX_MALE  = st.slider("æ€§åˆ¥: ç‰¡ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")
    SEX_FEMA  = st.slider("æ€§åˆ¥: ç‰ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")
    SEX_GELD  = st.slider("æ€§åˆ¥: ã‚»ãƒ³ã®åŠ ç‚¹", 0.0, 2.0, 0.0, 0.01, format="%.2f")

    # â”€â”€ è„šè³ªï¼ˆ0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    STL_NIGE   = st.slider("è„šè³ª: é€ƒã’ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_SENKO  = st.slider("è„šè³ª: å…ˆè¡Œã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_SASHI  = st.slider("è„šè³ª: å·®ã—ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")
    STL_OIKOMI = st.slider("è„šè³ª: è¿½è¾¼ã®åŠ ç‚¹",  0.0, 2.0, 0.0, 0.01, format="%.2f")

    # â”€â”€ å¹´é½¢ï¼ˆãƒ”ãƒ¼ã‚¯ã¯æ•´æ•°ã®ã¾ã¾ / æ¸›è¡°å¼·ã•ã¯0.00ã€œ2.00ã€0.01åˆ»ã¿ï¼‰â”€â”€
    AGE_PEAK   = st.slider("å¹´é½¢ã®ãƒ”ãƒ¼ã‚¯ï¼ˆÂ±ã§æ¸›è¡°ï¼‰", 2, 8, 4)
    AGE_SLOPE  = st.slider("å¹´é½¢ã®æ¸›è¡°å¼·ã•", 0.0, 2.0, 0.5, 0.01, format="%.2f")

    # â”€â”€ æ ãƒã‚¤ã‚¢ã‚¹å¼·ã•ã‚‚0.00ã€œ2.00ã€0.01åˆ»ã¿ã«ï¼ˆæ–¹å‘ã¯ãã®ã¾ã¾ï¼‰â”€â”€
    WAKU_DIR   = st.radio("æ ãƒã‚¤ã‚¢ã‚¹æ–¹å‘", ["ãªã—","å†…æœ‰åˆ©","å¤–æœ‰åˆ©"], index=0, horizontal=True)
    WAKU_STR   = st.slider("æ ãƒã‚¤ã‚¢ã‚¹å¼·ã•", 0.0, 2.0, 1.0, 0.01, format="%.2f")

with st.sidebar.expander("ğŸ“ ç¢ºç‡æ ¡æ­£", expanded=False):
    do_calib = st.checkbox("ç­‰æ¸©å›å¸°ã§å‹ç‡ã‚’æ ¡æ­£", value=False)

with st.sidebar.expander("ğŸ› æ‰‹å‹•ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰", expanded=(MODE=="æ‰‹å‹•ï¼ˆä¸Šç´šè€…ï¼‰")):
    besttime_w_manual = st.slider("ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿(æ‰‹å‹•)", 0.0, 2.0, 1.0)
    dist_bw_m_manual  = st.slider("è·é›¢å¸¯ã®å¹…[æ‰‹å‹•]", 50, 600, 200, 25)
    mc_beta_manual    = st.slider("PLæ¸©åº¦Î²(æ‰‹å‹•)", 0.3, 5.0, 1.4, 0.1)

with st.sidebar.expander("ğŸ–¥ è¡¨ç¤º", expanded=False):
    FULL_TABLE_VIEW = st.checkbox("å…¨é ­è¡¨ç¤ºï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç„¡ã—ï¼‰", True)
    MAX_TABLE_HEIGHT = st.slider("æœ€å¤§é«˜ã•(px)", 800, 10000, 5000, 200)
    SHOW_CORNER = st.checkbox("4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã‚’è¡¨ç¤º", False)


if st.button("ğŸ§ª PhysS1 ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"):
    g = get_course_geom(COURSE_ID, "èŠ" if TARGET_SURFACE=="èŠ" else "ãƒ€", int(TARGET_DISTANCE), LAYOUT, RAIL)
    st.write("geom:", g)
    races_df_today_dbg = pd.DataFrame([{
        'race_id':'DBG','course_id':COURSE_ID,'surface':'èŠ',
        'distance_m':int(TARGET_DISTANCE),'layout':LAYOUT,'rail_state':RAIL,
        'band':TODAY_BAND,'num_turns':2
    }])
    try:
        out = add_phys_s1_features(races_df_today_dbg, group_cols=(), band_col="band", verbose=True)
        st.write("physåˆ—:", [c for c in out.columns if c.startswith("phys_")])
        st.dataframe(out)
    except Exception as e:
        st.error(f"PhysS1å¤±æ•—: {e}")

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
st.title("Rikeiba")
st.subheader("Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆsheet0=éå»èµ° / sheet1=å‡ºèµ°è¡¨ï¼‰")
excel_file = st.file_uploader("Excelï¼ˆ.xlsxï¼‰", type=['xlsx'], key="excel_up")
if excel_file is None:
    st.info("ã¾ãšExcelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

st.subheader("ï¼ˆä»»æ„ï¼‰èª¿æ•™ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
wood_file = st.file_uploader("ã‚¦ãƒƒãƒ‰ãƒãƒƒãƒ—èª¿æ•™ï¼ˆ.xlsxï¼‰", type=['xlsx'], key="wood_x")
hill_file = st.file_uploader("å‚è·¯èª¿æ•™ï¼ˆ.xlsxï¼‰", type=['xlsx'], key="hill_x")


@st.cache_data(show_spinner=False)
def load_excel_bytes(content: bytes):
    xls = pd.ExcelFile(io.BytesIO(content))
    s0 = pd.read_excel(xls, sheet_name=0)
    s1 = pd.read_excel(xls, sheet_name=1)
    return s0, s1

sheet0, sheet1 = load_excel_bytes(excel_file.getvalue())

# ===== èª¿æ•™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†æ­£è¦åŒ– =====
def _read_train_xlsx(file, kind: str) -> pd.DataFrame:
    """
    èª¿æ•™Excelï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯ï¼‰ã‹ã‚‰ä¸‹è¨˜åˆ—ã‚’æŠ½å‡ºã—ã¦çµ±ä¸€ã™ã‚‹:
      - é¦¬å(str), æ—¥ä»˜(datetime64), å ´æ‰€(str/ä»»æ„), _kind('wood'|'hill'),
        _intensity(str/ä»»æ„), _lap_sec(list[4] of float)
    ãƒ»Lap1..Lap4 or Time1..Time4ï¼ˆåŒºé–“ï¼‰å„ªå…ˆã€‚ãªã‘ã‚Œã° 4F/3F/2F/1Fï¼ˆç´¯è¨ˆï¼‰â†’å·®åˆ†åŒ–ã€‚
    ãƒ»ã€Œ12.1-11.8-â€¦ã€ç­‰ã®æ–‡å­—åˆ—ã‚‚è¨±å®¹ã€‚
    ãƒ»æœ«å°¾NaNã¯æœ€å¾Œã®æœ‰åŠ¹å€¤ã§å‰æ–¹è£œå®Œã€‚
    """
    import numpy as np, pandas as pd, io, re

    if file is None:
        return pd.DataFrame()

    try:
        xls = pd.ExcelFile(io.BytesIO(file.getvalue()))
    except Exception:
        return pd.DataFrame()

    def _norm_cols(d: pd.DataFrame) -> pd.DataFrame:
        return d.rename(columns=lambda c: str(c).strip())

    name_pat = r'é¦¬å|åå‰|å‡ºèµ°é¦¬|horse|Horse'
    date_pat = r'æ—¥ä»˜|å¹´æœˆæ—¥|èª¿æ•™æ—¥|æ—¥æ™‚|å®Ÿæ–½æ—¥|æ¸¬å®šæ—¥|è¨˜éŒ²æ—¥|date|Date'

    def _to_num_like(s):
        ser = pd.Series(s).astype(str).str.replace(',', '').str.replace('\u3000', ' ').str.strip()
        num = ser.str.extract(r'([-+]?\d+(?:\.\d+)?)', expand=False)
        return pd.to_numeric(num, errors='coerce')

    def _smart_parse_date(col: pd.Series) -> pd.Series:
        s = col
        if pd.api.types.is_integer_dtype(s) or pd.api.types.is_float_dtype(s):
            s = s.astype('Int64').astype(str).str.replace('<NA>', '')
        s = s.astype(str).str.strip()
        msk = s.str.match(r'^\d{8}$')
        out = pd.to_datetime(
            s.where(~msk, s.str.slice(0,4)+'-'+s.str.slice(4,6)+'-'+s.str.slice(6,8)),
            errors='coerce'
        )
        out = out.fillna(pd.to_datetime(s, errors='coerce'))
        return out

    def _split4_from_cum(t4, t3, t2, t1):
        vals = [np.nan, np.nan, np.nan, np.nan]
        vals[3] = t1
        if np.isfinite(t4) and np.isfinite(t3):
            vals[0] = t4 - t3
        elif np.isfinite(t4) and np.isfinite(t1):
            vals[0] = (t4 - t1) / 3.0
        if np.isfinite(t3) and np.isfinite(t2):
            vals[1] = t3 - t2
        elif np.isfinite(t3) and not np.isfinite(t2):
            base = t1 if np.isfinite(t1) else t3 / 3.0
            vals[1] = (t3 - base) / 2.0
        elif np.isfinite(t4) and np.isfinite(t1) and not np.isfinite(t3):
            vals[1] = (t4 - t1) / 3.0
        if np.isfinite(t2) and np.isfinite(t1):
            vals[2] = t2 - t1
        elif np.isfinite(t3) and not np.isfinite(t2):
            base = t1 if np.isfinite(t1) else t3 / 3.0
            vals[2] = (t3 - base) / 2.0
        elif np.isfinite(t4) and np.isfinite(t1):
            vals[2] = (t4 - t1) / 3.0

        arr = np.array(vals, float)
        if np.isnan(arr).any():
            good = np.where(np.isfinite(arr))[0]
            if good.size:
                last = arr[good[-1]]
                arr = np.where(np.isfinite(arr), arr, last)
        return arr

    def _parse_one(df_in: pd.DataFrame) -> pd.DataFrame:
        df = _norm_cols(df_in.copy())

        name_col = next((c for c in df.columns if re.search(name_pat, c, flags=re.I)), None)
        date_col = next((c for c in df.columns if re.search(date_pat, c, flags=re.I)), None)
        if name_col is None or date_col is None:
            return pd.DataFrame()

        df['é¦¬å'] = df[name_col].astype(str).str.replace('\u3000',' ').str.strip()
        df['æ—¥ä»˜'] = _smart_parse_date(df[date_col])

        seg_cols = []
        for i in range(1, 5):
            cand = [c for c in df.columns if re.search(fr'(^|\b)Lap{i}(\b|$)', c, flags=re.I)]
            if not cand:
                cand = [c for c in df.columns if re.search(fr'(^|\b)(L|Time){i}(\b|$)', c, flags=re.I)]
            seg_cols.append(cand)
        has_segment = all(len(x) > 0 for x in seg_cols)

        def _find_first(pats):
            for p in pats:
                cc = [c for c in df.columns if re.search(p, c, flags=re.I)]
                if cc:
                    return cc[0]
            return None

        c4 = _find_first([r'(^|\b)4\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'800\s*m', r'(^|[^0-9])4ï¼¦'])
        c3 = _find_first([r'(^|\b)3\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'600\s*m', r'(^|[^0-9])3ï¼¦'])
        c2 = _find_first([r'(^|\b)2\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'400\s*m', r'(^|[^0-9])2ï¼¦'])
        c1 = _find_first([r'(^|\b)1\s*[fï¼¦ï½†].{0,3}(æ™‚è¨ˆ|ï¾€ï½²ï¾‘|ç§’)?(\b|$)', r'200\s*m', r'(ï¾—ï½½ï¾„|æœ«|ä¸ŠãŒã‚Š).{0,3}1\s*[fï¼¦ï½†]'])
        has_cum = any([c4, c3, c2, c1])

        laps = np.full((len(df), 4), np.nan, float)

        if has_segment:
            for i in range(4):
                laps[:, i] = _to_num_like(df[seg_cols[i][0]]).to_numpy(float)
            if np.all(np.diff(laps, axis=1) < 0):
                t4, t3, t2, t1 = laps.T
                laps[:, 0] = t4 - t3
                laps[:, 1] = t3 - t2
                laps[:, 2] = t2 - t1
                laps[:, 3] = t1
        elif has_cum:
            T4 = _to_num_like(df[c4]) if c4 else pd.Series(np.nan, index=df.index)
            T3 = _to_num_like(df[c3]) if c3 else pd.Series(np.nan, index=df.index)
            T2 = _to_num_like(df[c2]) if c2 else pd.Series(np.nan, index=df.index)
            T1 = _to_num_like(df[c1]) if c1 else pd.Series(np.nan, index=df.index)
            arr = []
            for i in range(len(df)):
                arr.append(_split4_from_cum(T4.iloc[i], T3.iloc[i], T2.iloc[i], T1.iloc[i]))
            laps = np.vstack(arr)
        else:
            str_col = next(
                (c for c in df.columns if re.search(r'ãƒ©ãƒƒãƒ—|åŒºé–“|æ™‚è¨ˆ|ã‚¿ã‚¤ãƒ ', c) and df[c].astype(str).str.contains('-').any()),
                None
            )
            if str_col:
                def _parse_seq(s):
                    xs = re.findall(r'(\d+(?:\.\d+)?)', str(s))
                    xs = [float(x) for x in xs[:4]]
                    while len(xs) < 4:
                        xs.insert(0, np.nan)
                    return xs[-4:]
                laps = np.vstack(df[str_col].apply(_parse_seq).to_list()).astype(float)
            else:
                return pd.DataFrame()

        st_col = next((c for c in df.columns if re.search(r'å¼·å¼±|å†…å®¹|é¦¬ãªã‚Š|ä¸€æ¯|å¼·ã‚|ä»•æ›ã‘|è»½ã‚|æµã—', c)), None)
        intensity = df[st_col].astype(str) if st_col else pd.Series([''] * len(df), index=df.index)

        place_col = next((c for c in df.columns if re.search(r'å ´æ‰€|æ‰€å±|ãƒˆãƒ¬ã‚»ãƒ³|ç¾æµ¦|æ —æ±', c)), None)

        out = pd.DataFrame({
            'é¦¬å': df['é¦¬å'],
            'æ—¥ä»˜': df['æ—¥ä»˜'],
            'å ´æ‰€': (df[place_col].astype(str) if place_col else ""),
            '_kind': kind,
            '_intensity': intensity,
            '_lap_sec': list(laps)
        })

        mask = np.isfinite(laps).any(axis=1)
        out = out[mask].dropna(subset=['é¦¬å', 'æ—¥ä»˜'])
        return out

    frames = []
    for sh in xls.sheet_names:
        try:
            df0 = pd.read_excel(xls, sheet_name=sh, header=0)
            parsed = _parse_one(df0)
            if not parsed.empty:
                frames.append(parsed)
        except Exception:
            continue

    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()




# ===== ã‚³ãƒ¼ã‚¹æ–­é¢ï¼ˆå‚è·¯ã®å‚¾æ–œãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ =====
def _slope_profile(kind: str):
    # è¿”ã‚Šå€¤: list of (length_m, grade_perm) ã‚’è·é›¢é †ã«
    if kind == 'hill_ritto':
        # æ —æ±ï¼š300m 2.0â€°? â†’ 2.0%, ç¶šã„ã¦ 570m 3.5%, 100m 4.5%, 115m 1.25%
        # å˜ä½ã¯ã€Œ%ã€ï¼ 0.02 ãªã©
        return [(300, 0.020), (570, 0.035), (100, 0.045), (115, 0.0125)]
    elif kind == 'hill_miho':
        # ç¾æµ¦ï¼šä¸»è¨ˆæ¸¬800måŒºé–“ã®ä»£è¡¨å€¤ã¨ã—ã¦ 3.0% è¿‘å‚ã‚’ä¸€å®šã¨ã¿ãªã™ï¼ˆçµ‚ç«¯ä»˜è¿‘ã¯4.688%ï¼‰
        # 800mã‚’ 600m@3.0% + 200m@4.7% ã«
        return [(600, 0.030), (200, 0.04688)]
    else:
        return [(800, 0.000)]  # ãƒ•ãƒ©ãƒƒãƒˆ

def _intensity_gain(txt: str) -> float:
    s = str(txt)
    if re.search(r'ä¸€æ¯|å¼·ã‚', s): return 1.12
    if re.search(r'å¼·', s):       return 1.08
    if re.search(r'é¦¬ãªã‚Š', s):   return 0.94
    if re.search(r'è»½ã‚|æµã—', s):return 0.90
    return 1.00

def _seg_energy_wkg(v, a, g, grade, Crr, CdA, rho):
    # v[m/s], a[m/s2], ä½“é‡ã¯å¾Œã§æ›ã‘ã‚‹ã®ã§ W/kg ã¨ J/kg ã§è¿”ã™
    # æŠµæŠ—ï¼šè»¢ãŒã‚Š + é‡åŠ› + ç©ºåŠ›
    Fr = Crr * 9.80665 * np.cos(np.arctan(grade))            # â‰ˆ Crr*g
    Fg = g * grade                                           # g*sinÎ¸ â‰’ g*grade
    Fa = 0.5 * rho * CdA * v*v / 500.0                       # ä¿‚æ•°ç¸®å°ºï¼ˆç¾¤ã‚Œæµãƒ»å§¿å‹¢ç·©å’Œã®å¹³å‡çš„ä½æ¸›ï¼‰
    P_over_m = v*(Fr + Fg) + v*a                             # W/kg
    P_over_m += Fa*v/75.0                                    # ç©ºåŠ›å¯„ä¸ã‚’å¼±ã‚ã«ï¼ˆå®Ÿæ¸¬åˆã‚ã›ï¼‰
    return max(P_over_m, 0.0)

def _derive_training_metrics(train_df: pd.DataFrame,
                             s0_races: pd.DataFrame,
                             Crr_wood, Crr_hill, CdA, rho,
                             Pmax_wkg, Emax_jkg, half_life_days: int):
    """
    å„èª¿æ•™1æœ¬â†’ EAP[J/kg/m], PeakWkg[W/kg], EffReserve ã‚’è¨ˆç®—ã€‚
    é¦¬ä½“é‡ã¯ã€Œãã®èª¿æ•™æ—¥ã®ç›´è¿‘â€œå‰â€ãƒ¬ãƒ¼ã‚¹ã®é¦¬ä½“é‡ã€ã‚’å‚ç…§ï¼ˆãªã‘ã‚Œã°å…¨ä½“ä¸­å¤®å€¤ï¼‰ã€‚
    4FÃ—200mæƒ³å®šã€‚å‚è·¯ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«æ²¿ã£ã¦ grade ã‚’ä»˜ä¸ã€‚
    """
    if train_df.empty:
        return pd.DataFrame(columns=['é¦¬å','æ—¥ä»˜','EAP','PeakWkg','EffReserve','PhysicsZ'])

    # å‚ç…§ä½“é‡ãƒãƒƒãƒ—
    bw_map = {}
    if {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é¦¬ä½“é‡'}.issubset(s0_races.columns):
        tmp = s0_races[['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é¦¬ä½“é‡']].dropna().sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'])
        for name, g in tmp.groupby('é¦¬å'):
            bw_map[name] = list(zip(g['ãƒ¬ãƒ¼ã‚¹æ—¥'].to_numpy(), g['é¦¬ä½“é‡'].to_numpy()))
    bw_median = float(pd.to_numeric(s0_races.get('é¦¬ä½“é‡', pd.Series([480])), errors='coerce')
                      .median(skipna=True) or 480.0)

    out = []
    for _, r in train_df.iterrows():
        name = r['é¦¬å']
        day  = pd.to_datetime(r['æ—¥ä»˜'])

        laps = np.array(r['_lap_sec'], dtype=float)
        if laps.size != 4 or np.isnan(laps).all():
            continue

        # æ¬ æã¯ã€Œæœ€å¾Œã«è¦³æ¸¬ã§ããŸåŒºé–“ã€ã§å‰æ–¹è£œå®Œ
        laps = np.where(np.isfinite(laps), laps, np.nan)
        if np.isnan(laps).any():
            good = np.where(np.isfinite(laps))[0]
            if good.size == 0:
                continue
            fill_val = float(laps[good[-1]])
            laps = np.where(np.isfinite(laps), laps, fill_val)

        # èª¿æ•™æ—¥ã®ç›´å‰ãƒ¬ãƒ¼ã‚¹ã®ä½“é‡ï¼ˆç„¡ã‘ã‚Œã°å…¨ä½“ä¸­å¤®å€¤ï¼‰
        bw = bw_median
        if name in bw_map:
            prev = [w for (d, w) in bw_map[name] if pd.to_datetime(d) <= day]
            if prev:
                bw = float(pd.to_numeric(prev[-1], errors='coerce') or bw_median)

        # é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ï¼ˆ200mã”ã¨ï¼‰
        d = 200.0
        v = d / laps
        a = np.diff(v, prepend=v[0]) / laps

        # ã‚³ãƒ¼ã‚¹ç¨®åˆ¥ï¼ˆå‚è·¯/ã‚¦ãƒƒãƒ‰ï¼‰ã¨å‹¾é…
        kind  = str(r.get('_kind', 'wood'))
        place = str(r.get('å ´æ‰€', ''))
        if kind == 'hill':
            is_miho  = bool(re.search(r'ç¾æµ¦|miho', place, flags=re.I))
            prof_key = 'hill_miho' if is_miho else 'hill_ritto'
            prof = _slope_profile(prof_key)

            grades = []
            remain = 800.0
            for L, G in prof:
                take = min(L, remain)
                nseg = int(round(take / 200.0))
                grades += [G] * max(1, nseg)
                remain -= take
                if remain <= 0:
                    break
            if not grades:
                grades = [0.03] * 4
            while len(grades) < 4:
                grades.append(grades[-1])
            grade = np.array(grades[:4], float)
            Crr = Crr_hill
        else:
            grade = np.zeros(4, float)
            Crr   = Crr_wood

        # å¼·å¼±ã«ã‚ˆã‚‹ä¿‚æ•°
        gain = _intensity_gain(r.get('_intensity', ''))

        # åŒºé–“ã‚ãŸã‚Šã®å‡ºåŠ›å¯†åº¦ï¼ˆW/kgï¼‰
        P = np.array([_seg_energy_wkg(v[i], a[i], 9.80665, grade[i], Crr, CdA, rho) * gain
                      for i in range(4)], float)

        # æŒ‡æ¨™
        PeakWkg    = float(P.max())
        work_jkg   = float(np.sum(P * laps))               # J/kg
        EAP        = float(work_jkg / 800.0)               # J/kg/m
        EffReserve = float(max(0.0, Emax_jkg - work_jkg)) / Emax_jkg  # 0..1

        out.append({'é¦¬å': name, 'æ—¥ä»˜': day,
                    'EAP': EAP, 'PeakWkg': PeakWkg, 'EffReserve': EffReserve})

    df = pd.DataFrame(out)
    if df.empty:
        return df

    # ç›´è¿‘é‡ã¿ä»˜ã‘ â†’ PhysicsZ
    df = df.sort_values(['é¦¬å','æ—¥ä»˜'])
    today = pd.Timestamp.today()
    df['_w'] = 0.5 ** ((today - df['æ—¥ä»˜']).dt.days.clip(lower=0) / float(max(1, half_life_days)))

    agg = (df.groupby('é¦¬å')
             .apply(lambda g: pd.Series({
                 'EAP':        np.average(g['EAP'],        weights=g['_w']),
                 'PeakWkg':    np.average(g['PeakWkg'],    weights=g['_w']),
                 'EffReserve': np.average(g['EffReserve'], weights=g['_w']),
             }))
             .reset_index())

    # ã€Œå°ã•ã„ã»ã©è‰¯ã„ã€EAP ã‚’åè»¢ã—ã¦ZåŒ–ï¼ˆå¹³å‡50, Ïƒ10ï¼‰
    agg['PhysicsCore'] = -pd.to_numeric(agg['EAP'], errors='coerce')
    mu = float(agg['PhysicsCore'].mean())
    sd = float(agg['PhysicsCore'].std(ddof=0) or 1.0)
    agg['PhysicsZ'] = (agg['PhysicsCore'] - mu) / sd * 10 + 50

    return agg[['é¦¬å','EAP','PeakWkg','EffReserve','PhysicsZ']]


# ===== åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè»½é‡ï¼‰ =====

def _norm_col(s: str) -> str:
    s=str(s).strip(); s=re.sub(r'\s+','',s)
    return s.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼…','0123456789%')).replace('ï¼ˆ','(').replace('ï¼‰',')')

def _auto_pick(df, pats):
    cmap={c:_norm_col(c) for c in df.columns}
    for c, n in cmap.items():
        for p in pats:
            if re.search(p, n, flags=re.I):
                return c
    return None

def _map_ui(df, patterns, required, title, key_prefix):
    cols = list(df.columns)
    auto = {k: _auto_pick(df, pats) for k, pats in patterns.items()}
    with st.expander(f"åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼š{title}", expanded=False):
        mapping = {}
        for k, _ in patterns.items():
            options = ['<æœªé¸æŠ>'] + cols
            default = auto.get(k) if auto.get(k) in cols else '<æœªé¸æŠ>'
            mapping[k] = st.selectbox(k, options, index=options.index(default), key=f"map:{key_prefix}:{k}")
    for k in required:
        if mapping.get(k, '<æœªé¸æŠ>') == '<æœªé¸æŠ>':
            st.error(f"{title} å¿…é ˆåˆ—ãŒæœªé¸æŠ: {k}")
            st.stop()
    return {k: (v if v != '<æœªé¸æŠ>' else None) for k, v in mapping.items()}

# ã“ã“ã« PCI / PCI3 / Ave-3F ã‚‚æ‹¾ã†ã‚­ãƒ¼ã‚’è¿½åŠ 
PAT_S0 = {
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'ãƒ¬ãƒ¼ã‚¹æ—¥':[r'ãƒ¬ãƒ¼ã‚¹æ—¥|æ—¥ä»˜(?!S)|å¹´æœˆæ—¥|æ–½è¡Œæ—¥|é–‹å‚¬æ—¥'],
    'ç«¶èµ°å':[r'ç«¶èµ°å|ãƒ¬ãƒ¼ã‚¹å|åç§°'],
    'ã‚¯ãƒ©ã‚¹å':[r'ã‚¯ãƒ©ã‚¹å|æ ¼|æ¡ä»¶|ãƒ¬ãƒ¼ã‚¹ã‚°ãƒ¬ãƒ¼ãƒ‰'],
    'é ­æ•°':[r'é ­æ•°|å‡ºèµ°é ­æ•°'],
    'ç¢ºå®šç€é †':[r'ç¢ºå®šç€é †|ç€é †(?!ç‡)'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ':[r'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ |ä¸ŠãŒã‚Š3F|ä¸Š3F'],
    'ä¸Š3Fé †ä½':[r'ä¸ŠãŒã‚Š3Fé †ä½|ä¸Š3Fé †ä½'],
    'é€šé4è§’':[r'é€šé.*4è§’|4è§’.*é€šé|ç¬¬4ã‚³ãƒ¼ãƒŠãƒ¼|4è§’é€šéé †'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’':[r'èµ°ç ´ã‚¿ã‚¤ãƒ .*ç§’|èµ°ç ´ã‚¿ã‚¤ãƒ |ã‚¿ã‚¤ãƒ $'],
    'è·é›¢':[r'è·é›¢'],
    'èŠãƒ»ãƒ€':[r'èŠ.?ãƒ».?ãƒ€|èŠãƒ€|ã‚³ãƒ¼ã‚¹|é¦¬å ´ç¨®åˆ¥|Surface'],
    'é¦¬å ´':[r'é¦¬å ´(?!.*æŒ‡æ•°)|é¦¬å ´çŠ¶æ…‹'],
    'å ´å':[r'å ´å|å ´æ‰€|ç«¶é¦¬å ´|é–‹å‚¬(åœ°|å ´|å ´æ‰€)'],
    'PCI':[r'\bPCI(?!G)|ï¼°ï¼£ï¼©'],
    'PCI3':[r'\bPCI3\b|ï¼°ï¼£ï¼©3'],
    'Ave-3F':[r'Ave[-_]?3F|å¹³å‡.*3F'],
}
REQ_S0 = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']

MAP_S0 = {k: _auto_pick(sheet0, v) for k,v in PAT_S0.items()}
missing = [k for k in REQ_S0 if MAP_S0.get(k) is None]
if missing:
    MAP_S0 = _map_ui(sheet0, PAT_S0, REQ_S0, 'sheet0ï¼ˆéå»èµ°ï¼‰', 's0')

s0 = pd.DataFrame()
for k, col in MAP_S0.items():
    if col and col in sheet0.columns:
        s0[k]=sheet0[col]

s0['ãƒ¬ãƒ¼ã‚¹æ—¥']=pd.to_datetime(s0['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
for c in ['é ­æ•°','ç¢ºå®šç€é †','æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡','ä¸Š3Fé †ä½','é€šé4è§’','è·é›¢']:
    if c in s0: s0[c]=pd.to_numeric(s0[c], errors='coerce')
if 'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’' in s0: s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’']=s0['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'].apply(_parse_time_to_sec)
if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in s0: s0['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']=s0['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)
for col in ['PCI', 'PCI3', 'Ave-3F']:
    if col in s0.columns:
        s0[col] = pd.to_numeric(s0[col], errors='coerce')
if 'è·é›¢' in s0.columns:
    s0['è·é›¢'] = pd.to_numeric(s0['è·é›¢'], errors='coerce')

# ã‚·ãƒ¼ãƒˆ1
PAT_S1={
    'é¦¬å':[r'é¦¬å|åå‰|å‡ºèµ°é¦¬'],
    'æ ':[r'æ |æ ç•ª'],
    'ç•ª':[r'é¦¬ç•ª|ç•ª'],
    'æ€§åˆ¥':[r'æ€§åˆ¥'],
    'å¹´é½¢':[r'å¹´é½¢|é¦¬é½¢'],
    'æ–¤é‡':[r'æ–¤é‡'],
    'é¦¬ä½“é‡':[r'é¦¬ä½“é‡|ä½“é‡'],
    'è„šè³ª':[r'è„šè³ª'],
    'å‹ç‡':[r'å‹ç‡(?!.*ç‡)|\bå‹ç‡\b'],
    'é€£å¯¾ç‡':[r'é€£å¯¾ç‡|é€£å¯¾'],
    'è¤‡å‹ç‡':[r'è¤‡å‹ç‡|è¤‡å‹'],
    'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ':[r'ãƒ™ã‚¹ãƒˆ.*ã‚¿ã‚¤ãƒ |Best.*Time|ï¾ï¾ï½½ï¾„.*ï¾€ï½²ï¾‘|ã‚¿ã‚¤ãƒ .*(æœ€é€Ÿ|ãƒ™ã‚¹ãƒˆ)'],
}
REQ_S1=['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢']
MAP_S1 = {k: _auto_pick(sheet1, v) for k,v in PAT_S1.items()}
miss1=[k for k in REQ_S1 if MAP_S1.get(k) is None]
if miss1:
    MAP_S1=_map_ui(sheet1, PAT_S1, REQ_S1, 'sheet1ï¼ˆå‡ºèµ°è¡¨ï¼‰', 's1')

s1=pd.DataFrame()
for k, col in MAP_S1.items():
    if col and col in sheet1.columns:
        s1[k]=sheet1[col]

for c in ['æ ','ç•ª','æ–¤é‡','é¦¬ä½“é‡']:
    if c in s1: s1[c]=pd.to_numeric(s1[c], errors='coerce')
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ' in s1: s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']=s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ '].apply(_parse_time_to_sec)

# å…ˆé ­ç©ºè¡Œ/ç©ºé¦¬åé™¤å»
s1 = s1.replace(r'^\s*$', np.nan, regex=True).dropna(how='all')
if 'é¦¬å' in s1:
    s1['é¦¬å']=s1['é¦¬å'].astype(str).str.replace('\u3000',' ').str.strip()
    s1=s1[s1['é¦¬å'].ne('')]
s1=s1.reset_index(drop=True)

# è„šè³ªã‚¨ãƒ‡ã‚£ã‚¿
if 'è„šè³ª' not in s1.columns: s1['è„šè³ª']=''
if 'æ–¤é‡' not in s1.columns: s1['æ–¤é‡']=np.nan
if 'é¦¬ä½“é‡' not in s1.columns: s1['é¦¬ä½“é‡']=np.nan

st.subheader("é¦¬ä¸€è¦§ï¼ˆå¿…è¦ãªã‚‰è„šè³ª/æ–¤é‡/ä½“é‡ã‚’èª¿æ•´ï¼‰")

def auto_style_from_history(df: pd.DataFrame, n_recent=5, hl_days=180):
    # å¿…é ˆåˆ—ãŒãªã‘ã‚Œã°ç©ºã§è¿”ã™ï¼ˆè½ã¡ãªã„ã‚ˆã†ã«ï¼‰
    need = {'é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'}
    if not need.issubset(df.columns):
        return pd.DataFrame({'é¦¬å': [], 'æ¨å®šè„šè³ª': []})

    # ä»»æ„åˆ—ï¼ˆä¸Š3Fé †ä½ï¼‰ã¯å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰é¸ã¶
    base_cols = ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’']
    if 'ä¸Š3Fé †ä½' in df.columns:
        base_cols.append('ä¸Š3Fé †ä½')

    t = (
        df[base_cols]
        .dropna(subset=['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','é ­æ•°','é€šé4è§’'])
        .copy()
    )

    # ä¸¦ã¹æ›¿ãˆã¨æœ€è¿‘nä»¶
    t = t.sort_values(['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥'], ascending=[True, False])
    t['_rn'] = t.groupby('é¦¬å').cumcount() + 1
    t = t[t['_rn'] <= n_recent].copy()

    today = pd.Timestamp.today()
    t['_days'] = (today - pd.to_datetime(t['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0).fillna(9999)
    t['_w'] = 0.5 ** (t['_days'] / float(hl_days))

    # 4è§’ä½ç½®â†’å…ˆè¡Œåº¦ï¼ˆ0ã€œ1ï¼‰
    denom = (pd.to_numeric(t['é ­æ•°'], errors='coerce') - 1).replace(0, np.nan)
    pos_ratio = (pd.to_numeric(t['é€šé4è§’'], errors='coerce') - 1) / denom
    pos_ratio = pos_ratio.clip(0, 1).fillna(0.5)

    # ä¸ŠãŒã‚Šé †ä½ãŒã‚ã‚Œã°çµ‚ã„è„šå¯„ä¸ã€ãªã‘ã‚Œã°0
    if 'ä¸Š3Fé †ä½' in t.columns:
        ag = pd.to_numeric(t['ä¸Š3Fé †ä½'], errors='coerce')
        close = ((3.5 - ag) / 3.5).clip(0, 1).fillna(0.0)
    else:
        close = pd.Series(0.0, index=t.index)

    # ãƒ­ã‚¸ãƒƒãƒˆ
    b = {'é€ƒã’': -1.2, 'å…ˆè¡Œ': 0.6, 'å·®ã—': 0.3, 'è¿½è¾¼': -0.7}
    t['L_é€ƒã’'] = b['é€ƒã’'] + 1.6*(1 - pos_ratio) - 1.2*close
    t['L_å…ˆè¡Œ'] = b['å…ˆè¡Œ'] + 1.1*(1 - pos_ratio) - 0.1*close
    t['L_å·®ã—'] = b['å·®ã—'] + 1.1*(pos_ratio)     + 0.9*close
    t['L_è¿½è¾¼'] = b['è¿½è¾¼'] + 1.6*(pos_ratio)     + 0.5*close

    # é¦¬ã”ã¨é‡ã¿å¹³å‡ â†’ softmax â†’ æœ€é »è„šè³ª
    rows = []
    for name, g in t.groupby('é¦¬å'):
        w = g['_w'].to_numpy(); sw = w.sum()
        if sw <= 0: 
            continue
        vec = np.array([
            float((g['L_é€ƒã’']*w).sum()/sw),
            float((g['L_å…ˆè¡Œ']*w).sum()/sw),
            float((g['L_å·®ã—']*w).sum()/sw),
            float((g['L_è¿½è¾¼']*w).sum()/sw),
        ])
        vec = vec - vec.max()
        p = np.exp(vec); p /= p.sum()
        rows.append([name, STYLES[int(np.argmax(p))]])
    return pd.DataFrame(rows, columns=['é¦¬å','æ¨å®šè„šè³ª'])

pred_style = auto_style_from_history(s0.copy())

s1['è„šè³ª']=s1['è„šè³ª'].map(normalize_style)
if not pred_style.empty:
    s1=s1.merge(pred_style, on='é¦¬å', how='left')
    s1['è„šè³ª']=s1['è„šè³ª'].where(s1['è„šè³ª'].astype(str).str.strip().ne(''), s1['æ¨å®šè„šè³ª'])
    s1.drop(columns=['æ¨å®šè„šè³ª'], inplace=True)

H = (lambda n: int(min(MAX_TABLE_HEIGHT, 38 + 35*max(1,int(n)) + 28)) if FULL_TABLE_VIEW else 460)
edit = st.data_editor(
    s1[['æ ','ç•ª','é¦¬å','æ€§åˆ¥','å¹´é½¢','è„šè³ª','æ–¤é‡','é¦¬ä½“é‡']].copy(),
    column_config={
        'è„šè³ª': st.column_config.SelectboxColumn('è„šè³ª', options=STYLES),
        'æ–¤é‡': st.column_config.NumberColumn('æ–¤é‡', min_value=45, max_value=65, step=0.5),
        'é¦¬ä½“é‡': st.column_config.NumberColumn('é¦¬ä½“é‡', min_value=300, max_value=600, step=1),
    },
    use_container_width=True,
    num_rows='static',
    height=H(len(s1)),
    hide_index=True,
)
horses = edit.copy()

# ===== å…¥åŠ›ãƒã‚§ãƒƒã‚¯ =====
problems=[]
for c in ['é¦¬å','ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','é ­æ•°','ç¢ºå®šç€é †']:
    if c not in s0.columns: problems.append(f"sheet0 å¿…é ˆåˆ—ãŒä¸è¶³: {c}")
if 'é€šé4è§’' in s0.columns and 'é ­æ•°' in s0.columns:
    tmp=s0[['é€šé4è§’','é ­æ•°']].dropna()
    if len(tmp)>0 and ((tmp['é€šé4è§’']<1)|(tmp['é€šé4è§’']>tmp['é ­æ•°'])).any():
        problems.append('sheet0 é€šé4è§’ãŒé ­æ•°ãƒ¬ãƒ³ã‚¸å¤–')
if problems:
    st.warning("å…¥åŠ›ãƒã‚§ãƒƒã‚¯:\n- "+"\n- ".join(problems))

# ===== ãƒãƒ¼ã‚¸ =====
for dup in ['æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']:
    s0.drop(columns=[dup], errors='ignore', inplace=True)
df = s0.merge(horses[['é¦¬å','æ ','ç•ª','æ€§åˆ¥','å¹´é½¢','æ–¤é‡','é¦¬ä½“é‡','è„šè³ª']], on='é¦¬å', how='left')

# ===== 1èµ°ã‚¹ã‚³ã‚¢ =====
CLASS_PTS={'G1':10,'G2':8,'G3':6,'ãƒªã‚¹ãƒ†ãƒƒãƒ‰':5,'ã‚ªãƒ¼ãƒ—ãƒ³ç‰¹åˆ¥':4}

def normalize_grade_text(x: str|None) -> str|None:
    if x is None or (isinstance(x,float) and np.isnan(x)): return None
    s=str(x).translate(_fw)
    s=(s.replace('ï¼§','G').replace('ï¼ˆ','(').replace('ï¼‰',')')
        .replace('â… ','I').replace('â…¡','II').replace('â…¢','III'))
    s=re.sub(r'G\s*III','G3',s,flags=re.I)
    s=re.sub(r'G\s*II','G2',s,flags=re.I)
    s=re.sub(r'G\s*I','G1',s,flags=re.I)
    s=re.sub(r'ï¼ªï¼°ï¼®','Jpn',s,flags=re.I)
    s=re.sub(r'JPN','Jpn',s,flags=re.I)
    s=re.sub(r'Jpn\s*III','Jpn3',s,flags=re.I)
    s=re.sub(r'Jpn\s*II','Jpn2',s,flags=re.I)
    s=re.sub(r'Jpn\s*I','Jpn1',s,flags=re.I)
    m=re.search(r'(?:G|Jpn)\s*([123])', s, flags=re.I)
    return f"G{m.group(1)}" if m else None

def class_points(row) -> int:
    g=normalize_grade_text(row.get('ã‚¯ãƒ©ã‚¹å')) if 'ã‚¯ãƒ©ã‚¹å' in row else None
    if not g and 'ç«¶èµ°å' in row: g=normalize_grade_text(row.get('ç«¶èµ°å'))
    if g in CLASS_PTS: return CLASS_PTS[g]
    name=str(row.get('ã‚¯ãƒ©ã‚¹å',''))+' '+str(row.get('ç«¶èµ°å',''))
    if re.search(r'3\s*å‹', name): return 3
    if re.search(r'2\s*å‹', name): return 2
    if re.search(r'1\s*å‹', name): return 1
    if re.search(r'æ–°é¦¬|æœªå‹åˆ©', name): return 1
    if re.search(r'ã‚ªãƒ¼ãƒ—ãƒ³', name): return 4
    if re.search(r'ãƒªã‚¹ãƒ†ãƒƒãƒ‰|L\b', name, flags=re.I): return 5
    return 1

if 'ãƒ¬ãƒ¼ã‚¹æ—¥' not in df.columns:
    st.error('ãƒ¬ãƒ¼ã‚¹æ—¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'); st.stop()

_df = df.copy()

def calc_score(r):
    gpt = class_points(r)
    # åŸºæœ¬ï¼šç€é †é€†è»¢ãƒã‚¤ãƒ³ãƒˆ + å‡ºèµ°ãƒœãƒ¼ãƒŠã‚¹Î»
    base = gpt * (pd.to_numeric(r['é ­æ•°'], errors='coerce') + 1 - pd.to_numeric(r['ç¢ºå®šç€é †'], errors='coerce'))
    base = float(base) if np.isfinite(base) else 0.0
    base += float(lambda_part) * gpt

    # é‡è³ãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼é€£å‹•ï¼‰
    gtxt = normalize_grade_text(r.get('ã‚¯ãƒ©ã‚¹å')) or normalize_grade_text(r.get('ç«¶èµ°å'))
    bonus_grade = int(grade_bonus) if gtxt in ['G1','G2','G3'] else 0

    # ä¸ŠãŒã‚Šãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼é€£å‹•ï¼‰
    ao = pd.to_numeric(r.get('ä¸Š3Fé †ä½', np.nan), errors='coerce')
    if ao == 1:    bonus_agari = int(agari1_bonus)
    elif ao == 2:  bonus_agari = int(agari2_bonus)
    elif ao == 3:  bonus_agari = int(agari3_bonus)
    else:          bonus_agari = 0

    return base + bonus_grade + bonus_agari

_df['score_raw'] = _df.apply(calc_score, axis=1)

if _df['score_raw'].max()==_df['score_raw'].min():
    _df['score_norm']=50.0
else:
    _df['score_norm'] = (_df['score_raw'] - _df['score_raw'].min()) / (_df['score_raw'].max()-_df['score_raw'].min())*100


now = pd.Timestamp.today()
half_life_m_default = 6.0
_df['_days_ago']=(now - _df['ãƒ¬ãƒ¼ã‚¹æ—¥']).dt.days
_df['_w'] = 0.5 ** (_df['_days_ago'] / (half_life_m*30.4375 if half_life_m>0 else half_life_m_default*30.4375))

# ===== A) ãƒ¬ãƒ¼ã‚¹å†…ãƒ‡ãƒ•ãƒ¬ãƒ¼ãƒˆ =====
def _make_race_id_for_hist(dfh: pd.DataFrame) -> pd.Series:
    return pd.to_datetime(dfh['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d').fillna('00000000') + '_' + dfh['ç«¶èµ°å'].astype(str).fillna('')

_df['rid_hist'] = _make_race_id_for_hist(_df)
med = _df.groupby('rid_hist')['score_norm'].transform('median')
_df['score_adj'] = _df['score_norm'] - med

# ===== å³/å·¦å›ã‚Šï¼ˆæ¨å®šï¼‰ =====
DEFAULT_VENUE_TURN = {'æœ­å¹Œ':'å³','å‡½é¤¨':'å³','ç¦å³¶':'å³','æ–°æ½Ÿ':'å·¦','æ±äº¬':'å·¦','ä¸­å±±':'å³','ä¸­äº¬':'å·¦','äº¬éƒ½':'å³','é˜ªç¥':'å³','å°å€‰':'å³'}
def infer_turn_row(row):
    # ã¾ãšå ´åã§åˆ¤å®š
    venue = str(row.get('å ´å','')).strip()
    if venue in DEFAULT_VENUE_TURN:
        return DEFAULT_VENUE_TURN[venue]
    # æ¬¡ã«ç«¶èµ°åã‹ã‚‰æ¨å®šï¼ˆå¾“æ¥äº’æ›ï¼‰
    name = str(row.get('ç«¶èµ°å',''))
    for v, t in DEFAULT_VENUE_TURN.items():
        if v in name:
            return t
    return np.nan
if 'å›ã‚Š' not in _df.columns:
    _df['å›ã‚Š'] = _df.apply(infer_turn_row, axis=1)

# ===== B) Î²è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° =====
def tune_beta(df_hist: pd.DataFrame, betas=np.linspace(0.6, 2.4, 19)) -> float:
    dfh = df_hist.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid']=_make_race_id_for_hist(dfh)
    def logloss(beta: float):
        tot=0.0; n=0
        for _, g in dfh.groupby('rid'):
            x=g['score_adj'].astype(float).to_numpy()
            y=(pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int).to_numpy()
            if len(x)<2: continue
            p=np.exp(beta*(x-x.max())); s=p.sum();
            if s<=0 or not np.isfinite(s): continue
            p/=s; p=np.clip(p,1e-9,1-1e-9)
            tot+=-np.mean(y*np.log(p) + (1-y)*np.log(1-p)); n+=1
        return tot/max(n,1)
    return float(min(betas, key=logloss))

# === ã‚¿ã‚¤ãƒ åˆ†ä½å›å¸°ï¼ˆGBRï¼‰ + åŠ é‡CV ===
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GroupKFold

def _recent_weight_from_dates(dates: pd.Series, half_life_days: float) -> np.ndarray:
    days = (pd.Timestamp.today() - pd.to_datetime(dates, errors='coerce')).dt.days
    days = days.clip(lower=0).fillna(9999)
    H = max(1.0, float(half_life_days))
    return (0.5 ** (days / H)).to_numpy(float)

def _weighted_mu_sigma(X: np.ndarray, w: np.ndarray):
    w = w / max(w.sum(), 1e-12)
    mu = (w[:, None] * X).sum(axis=0)
    sd = np.sqrt(np.maximum((w[:, None] * (X - mu) ** 2).sum(axis=0), 1e-12))
    return mu, sd

def _standardize_with_mu_sigma(X: np.ndarray, mu: np.ndarray, sd: np.ndarray):
    return (X - mu) / sd

def _build_time_features(df_hist: pd.DataFrame, dist_turn_df: pd.DataFrame | None = None):
    need = {'èµ°ç ´ã‚¿ã‚¤ãƒ ç§’', 'è·é›¢', 'æ–¤é‡', 'èŠãƒ»ãƒ€', 'ãƒ¬ãƒ¼ã‚¹æ—¥', 'é¦¬å'}
    if not need.issubset(df_hist.columns):
        return None
    d = df_hist.copy()
    d = d.dropna(subset=list(need))
    if d.empty:
        return None
    feats = ['è·é›¢', 'æ–¤é‡', 'is_dirt']
    d['is_dirt'] = d['èŠãƒ»ãƒ€'].astype(str).str.contains('ãƒ€').astype(int)
    for c in ['PCI', 'PCI3', 'Ave-3F', 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']:
        if c in d.columns:
            feats.append(c)
    if 'PCI' in d.columns:
        d['è·é›¢xPCI'] = d['è·é›¢'] * d['PCI']; feats.append('è·é›¢xPCI')
    if 'PCI3' in d.columns and 'PCI' in d.columns:
        d['PCIgap'] = d['PCI3'] - d['PCI']; feats.append('PCIgap')
    if dist_turn_df is not None and 'DistTurnZ' in dist_turn_df.columns:
        dt = dist_turn_df[['é¦¬å', 'DistTurnZ']].drop_duplicates()
        d = d.merge(dt, on='é¦¬å', how='left')
        feats.append('DistTurnZ')
    X = d[feats].astype(float).to_numpy()
    y = pd.to_numeric(d['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'], errors='coerce').to_numpy(float)
    groups = d['é¦¬å'].astype(str).to_numpy()
    dates = pd.to_datetime(d['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')
    col_means = np.nanmean(X, axis=0)
    X = np.where(np.isfinite(X), X, col_means)
    return {'frame': d, 'X': X, 'y': y, 'feats': feats, 'groups': groups, 'dates': dates, 'col_means': col_means}

def fit_time_quantile_model(df_hist: pd.DataFrame, dist_turn_today_df: pd.DataFrame,
                            half_life_days: float, q_list=(0.2, 0.5, 0.8),
                            param_grid=(80, 120, 160), random_state=42):
    built = _build_time_features(df_hist, dist_turn_today_df)
    if built is None:
        return None

    X = built['X']; y = built['y']; groups = built['groups']; dates = built['dates']
    feats = built['feats']; col_means = built['col_means']

    # æ™‚ç³»åˆ—é‡ã¿ & æ¨™æº–åŒ–
    w = _recent_weight_from_dates(dates, half_life_days=max(1.0, float(half_life_days)))
    mu, sd = _weighted_mu_sigma(X, w)
    Xs = _standardize_with_mu_sigma(X, mu, sd)

    # GroupKFold ã§æœ¨æœ¬æ•°é¸å®š
    n_groups = int(len(np.unique(groups)))
    n_splits = max(2, min(5, n_groups))
    best_param, best_rmse = None, 1e18

    if n_groups >= 2 and Xs.shape[0] >= 20:
        gkf = GroupKFold(n_splits=n_splits)
        for n_est in param_grid:
            rmse_fold = []
            for tr, va in gkf.split(Xs, y, groups=groups):
                Xt, Xv = Xs[tr], Xs[va]
                yt, yv = y[tr], y[va]
                wt, wv = w[tr], w[va]
                med = GradientBoostingRegressor(loss='quantile', alpha=0.5,
                                                n_estimators=n_est, max_depth=3,
                                                random_state=random_state)
                med.fit(Xt, yt, sample_weight=wt)
                pred = med.predict(Xv)
                rmse = np.sqrt(np.average((yv - pred) ** 2, weights=wv))
                rmse_fold.append(rmse)
            score = float(np.mean(rmse_fold)) if rmse_fold else 1e18
            if score < best_rmse:
                best_rmse, best_param = score, n_est
    else:
        best_param = sorted(param_grid)[len(param_grid)//2]

    if best_param is None:
        best_param = sorted(param_grid)[len(param_grid)//2]

    models = {}
    for q in q_list:
        m = GradientBoostingRegressor(loss='quantile', alpha=q,
                                      n_estimators=best_param, max_depth=3,
                                      random_state=random_state)
        m.fit(Xs, y, sample_weight=w)
        models[q] = m

    resid = y - models[0.5].predict(Xs)
    sigma_hat = float(np.sqrt(np.maximum(np.average(resid ** 2, weights=w), 1e-6)))

    return {
        'feats': feats, 'mu': mu, 'sd': sd, 'col_means': col_means,
        'models': models, 'n_estimators': int(best_param), 'sigma_hat': sigma_hat
    }

def _field_pci_from_pace(pace_type: str) -> float:
    return {'ãƒã‚¤ãƒšãƒ¼ã‚¹': 46.0, 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹': 50.0, 'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 53.0, 'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': 56.0}.get(str(pace_type), 50.0)

def build_today_design(horses_today: pd.DataFrame, s0_hist: pd.DataFrame,
                       target_distance: int, target_surface: str,
                       dist_turn_today_df: pd.DataFrame, feats: list[str],
                       pace_type: str):
    # éå»ã®æŒ‡æ•°ã®æ™‚é–“æ¸›è¡°å¹³å‡ã‚’ä½œã‚‹
    rec_w = None
    if not s0_hist.empty and 'ãƒ¬ãƒ¼ã‚¹æ—¥' in s0_hist:
        rec_w = 0.5 ** ((pd.Timestamp.today() - pd.to_datetime(s0_hist['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce')).dt.days.clip(lower=0) / 180.0)

    def _wmean(s, w):
        s = pd.to_numeric(s, errors='coerce')
        w = pd.to_numeric(w, errors='coerce')
        m = np.nansum(s * w); sw = np.nansum(w)
        return float(m / sw) if sw > 0 else np.nan

    pci_wmean, pci3_wmean, ave3_wmean, a3_wmedian = {}, {}, {}, {}
    if 'PCI' in s0_hist.columns:
        pci_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI'], g['_w'])).to_dict()
    if 'PCI3' in s0_hist.columns:
        pci3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['PCI3'], g['_w'])).to_dict()
    if 'Ave-3F' in s0_hist.columns:
        ave3_wmean = s0_hist.assign(_w=rec_w).groupby('é¦¬å').apply(lambda g: _wmean(g['Ave-3F'], g['_w'])).to_dict()
    if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in s0_hist.columns:
        a3_wmedian = s0_hist.groupby('é¦¬å')['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '].median().to_dict()

    dtt = dist_turn_today_df[['é¦¬å', 'DistTurnZ']].drop_duplicates() if ('DistTurnZ' in dist_turn_today_df.columns) \
        else pd.DataFrame({'é¦¬å': horses_today['é¦¬å'], 'DistTurnZ': np.nan})
    H = horses_today.merge(dtt, on='é¦¬å', how='left')

    rows = []
    pci_field = _field_pci_from_pace(pace_type)  # â˜… æ˜ç¤ºå¼•æ•°ã§å—ã‘å–ã‚‹

    for _, r in H.iterrows():
        name = str(r['é¦¬å'])
        x = {}
        x['è·é›¢'] = float(target_distance)
        x['æ–¤é‡'] = float(r.get('æ–¤é‡', np.nan))
        x['is_dirt'] = 1.0 if str(target_surface).startswith('ãƒ€') else 0.0

        if 'PCI' in feats:
            x['PCI'] = float(pci_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI']): x['PCI'] = pci_field
        if 'PCI3' in feats:
            x['PCI3'] = float(pci3_wmean.get(name, np.nan))
            if not np.isfinite(x['PCI3']): x['PCI3'] = (x.get('PCI', pci_field) + 1.0)
        if 'Ave-3F' in feats:
            x['Ave-3F'] = float(ave3_wmean.get(name, np.nan))
        if 'ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ' in feats:
            x['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '] = float(a3_wmedian.get(name, np.nan))
        if 'è·é›¢xPCI' in feats:
            x['è·é›¢xPCI'] = x['è·é›¢'] * x.get('PCI', pci_field)
        if 'PCIgap' in feats:
            x['PCIgap'] = x.get('PCI3', x.get('PCI', pci_field) + 1.0) - x.get('PCI', pci_field)
        if 'DistTurnZ' in feats:
            x['DistTurnZ'] = float(r.get('DistTurnZ', np.nan))

        rows.append({'é¦¬å': name, **x})
    return pd.DataFrame(rows)


# ===== E) è·é›¢ãƒãƒ³ãƒ‰å¹… è‡ªå‹• =====

def auto_h_m(x_all: np.ndarray) -> float:
    x = pd.to_numeric(pd.Series(x_all), errors='coerce').dropna().to_numpy(float)
    if x.size<3: return 300.0
    q75,q25=np.percentile(x,75),np.percentile(x,25)
    iqr=q75-q25
    sigma=np.std(x)
    bw=0.9*min(sigma, iqr/1.34)*(x.size**(-1/5))
    return float(np.clip(bw, 120.0, 600.0))

# ===== è·é›¢Ã—å›ã‚Š è¿‘å‚åŒ–ï¼ˆNWã€score_adjä½¿ç”¨ï¼‰ =====

def kish_neff(w: np.ndarray) -> float:
    w=np.asarray(w,float); sw=w.sum(); s2=np.sum(w**2)
    return float((sw*sw)/s2) if s2>0 else 0.0

def nw_mean(x, y, w, h):
    x=np.asarray(x,float); y=np.asarray(y,float); w=np.asarray(w,float)
    if len(x)==0: return np.nan
    K=np.exp(-0.5*(x/max(1e-9,h))**2) * w
    sK=K.sum()
    return float((K*y).sum()/sK) if sK>0 else np.nan

# æº–å‚™
hist_for_turn = _df[['é¦¬å','è·é›¢','å›ã‚Š','score_adj','_w']].dropna(subset=['é¦¬å','è·é›¢','score_adj','_w']).copy()

# hé¸å®š
h_auto = auto_h_m(hist_for_turn['è·é›¢'].to_numpy())

# æŒ‡å®šè·é›¢ãƒ»å›ã‚Šã§ã®æ¨å®š
def dist_turn_profile(name: str, df_hist: pd.DataFrame, target_d: int, target_turn: str, h_m: float, opp_turn_w: float=0.5):
    g=df_hist[df_hist['é¦¬å'].astype(str).str.strip()==str(name)].copy()
    if g.empty: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    w0 = g['_w'].to_numpy(float) * np.where(g['å›ã‚Š'].astype(str)==str(target_turn), 1.0, float(opp_turn_w))
    x  = g['è·é›¢'].to_numpy(float)
    y  = g['score_adj'].to_numpy(float)
    msk=np.isfinite(x)&np.isfinite(y)&np.isfinite(w0)
    x,y,w0=x[msk],y[msk],w0[msk]
    if x.size==0: return {'DistTurnZ':np.nan,'n_eff_turn':0.0,'BestDist_turn':np.nan,'DistTurnZ_best':np.nan}
    z_hat = nw_mean(x-float(target_d), y, w0, h_m)
    w_eff = kish_neff(np.exp(-0.5*((x-float(target_d))/max(1e-9,h_m))**2)*w0)
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    ds=np.arange(int(target_d-600), int(target_d+600)+1, 100)
    best_d,best_val=np.nan,-1e18
    for d0 in ds:
        z=nw_mean(x-float(d0), y, w0, h_m)
        if np.isfinite(z) and z>best_val:
            best_val=float(z); best_d=int(d0)
    return {
        'DistTurnZ': float(z_hat) if np.isfinite(z_hat) else np.nan,
        'n_eff_turn': float(w_eff),
        'BestDist_turn': float(best_d) if np.isfinite(best_d) else np.nan,
        'DistTurnZ_best': float(best_val) if np.isfinite(best_val) else np.nan
    }

# ===== ã‚¹ãƒšã‚¯ãƒˆãƒ«: æ›²ç·šç”Ÿæˆ / DTW / ãƒ†ãƒ³ãƒ—ãƒ¬æ§‹ç¯‰ =====
def pseudo_curve(dist_m: float, time_s: float, a3_s: float, pci: float, pci3: float, rpci: float, pos_ratio: float,
                 n_points: int = 128) -> np.ndarray:
    """
    è·é›¢ãƒ»ç·ã‚¿ã‚¤ãƒ ãƒ»ä¸ŠãŒã‚Š3Fãªã©ã‹ã‚‰æ“¬ä¼¼é€Ÿåº¦æ›²ç·šï¼ˆæ­£è¦åŒ–ï¼‰ã‚’åˆæˆã€‚
    - è·é›¢: m, ã‚¿ã‚¤ãƒ : s
    - pos_ratio: å…ˆè¡Œåº¦ï¼ˆ1=å…ˆè¡Œã€0=å¾Œæ–¹ï¼‰
    """
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not np.isfinite(dist_m) or not np.isfinite(time_s) or time_s <= 0:
        return np.full(n_points, np.nan)

    # ãƒ™ãƒ¼ã‚¹é€Ÿåº¦ï¼ˆå¹³å‡ï¼‰
    v_avg = dist_m / time_s

    # PCI ç³»ã®å½¢çŠ¶å¯„ä¸
    pci = np.clip(pci if np.isfinite(pci) else 50.0, 35.0, 65.0)
    pci3 = pci3 if np.isfinite(pci3) else (pci + 1.0)
    rpci = rpci if np.isfinite(rpci) else 0.0  # ä»»æ„åˆ—

    x = np.linspace(0, 1, n_points)

    # å…ˆè¡Œ/å·®ã—ã«ã‚ˆã‚‹åºç›¤/çµ‚ç›¤ã‚¦ã‚§ã‚¤ãƒˆ
    lead = np.clip(pos_ratio if np.isfinite(pos_ratio) else 0.5, 0.0, 1.0)
    w_front = 0.8 + 0.6*(lead - 0.5)    # å…ˆè¡Œã»ã©åºç›¤â†‘
    w_back  = 0.8 - 0.6*(lead - 0.5)

    # PCIâ†’ä¸­ç›¤ã®ã€Œç·©ã¿/ç· ã¾ã‚Šã€ã‚’ã‚¬ã‚¦ã‚¹å½¢ã§ä»˜ä¸
    mid_center = 0.55
    mid_width  = 0.20
    mid_bump   = (50.0 - pci) / 25.0  # ãƒã‚¤ãƒšãƒ¼ã‚¹(PCIä½)ã ã¨ä¸­ç›¤è½ã¡è¾¼ã¿
    shape_mid  = 1.0 - mid_bump * np.exp(-0.5*((x - mid_center)/mid_width)**2)

    # ä¸ŠãŒã‚Š3Fï¼ˆçµ‚ç›¤ï¼‰ã«ã‚ˆã‚‹æœ«è„šå¯„ä¸
    if np.isfinite(a3_s) and a3_s > 0:
        # ã‚³ãƒ¼ã‚¹å…¨ä½“é€Ÿåº¦ã«å¯¾ã™ã‚‹çµ‚ç›¤é€Ÿåº¦æ¯”ï¼ˆç²—ã„è¿‘ä¼¼ï¼‰
        last_600 = 600.0
        if dist_m > last_600:
            v_last = last_600 / a3_s
            ratio  = np.clip(v_last / v_avg, 0.5, 1.5)
        else:
            ratio = 1.0
    else:
        ratio = 1.0 + 0.10*((pci3 - pci)/5.0)  # PCI3>PCI ãªã‚‰æœ«ä¸Šã’

    # å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è»½ã„èµ·ä¼ï¼ˆFFTç›¸å½“ã®ç‰¹å¾´ã‚’å°‘ã—æŒãŸã›ã‚‹ï¼‰
    low = 0.04*np.sin(2*np.pi*1*x)
    mid = 0.03*np.sin(2*np.pi*2*x + 1.3)
    high= 0.02*np.sin(2*np.pi*4*x + 0.7)
    wav = 1.0 + low + mid + high

    # åˆæˆ
    base = v_avg * (w_front*(1 - x) + w_back*x)   # ç›´ç·šè£œé–“ã®å‹¾é…
    v = base * shape_mid * (1.0 + (ratio - 1.0)*x**1.2) * wav

    # æ­£è¦åŒ–ï¼ˆé¢ç©=1ã«è¿‘ã¥ã‘ã‚‹ï¼‰
    v = np.maximum(v, 1e-8)
    v = v / np.trapz(v, x)
    return v

def dtw_distance(a: np.ndarray, b: np.ndarray) -> float:
    """ã‚·ãƒ³ãƒ—ãƒ«DTWï¼ˆO(N^2)ã€NaNã¯å¤§ãã„ç½°å‰‡ï¼‰"""
    if not (isinstance(a, np.ndarray) and isinstance(b, np.ndarray)):
        return np.inf
    if len(a)==0 or len(b)==0 or np.isnan(a).all() or np.isnan(b).all():
        return np.inf
    A = np.nan_to_num(a, nan=1e6)
    B = np.nan_to_num(b, nan=1e6)
    n, m = len(A), len(B)
    D = np.full((n+1, m+1), np.inf)
    D[0,0] = 0.0
    for i in range(1, n+1):
        ai = A[i-1]
        for j in range(1, m+1):
            cost = abs(ai - B[j-1])
            D[i,j] = cost + min(D[i-1,j], D[i,j-1], D[i-1,j-1])
    return float(D[n,m])

def build_template_curves(df_curves: pd.DataFrame, target_dist: int, target_surface: str,
                          tol: int = 100, n_points: int = 128):
    """
    åŒè·é›¢å¸¯(Â±tol) & åŒSurface ã® '_curve' ã‚’é›†ã‚ã¦ä¸­å¤®å€¤ãƒ†ãƒ³ãƒ—ãƒ¬ã‚’ä½œã‚‹ã€‚
    ã¤ã„ã§ã«FFTå¸¯åŸŸã‹ã‚‰ãƒ¬ãƒ¼ã‚¹å‹ Gate ã‚’æ¨å®šï¼ˆ0=æŒä¹…,1=ä¸­åº¸,2=ç¬ç™ºï¼‰ã€‚
    """
    if df_curves.empty or '_curve' not in df_curves.columns:
        return np.full(n_points, 1.0/n_points), {'Gate': 1}

    dd = df_curves.copy()
    dd['è·é›¢'] = pd.to_numeric(dd['è·é›¢'], errors='coerce')
    dd = dd[dd['è·é›¢'].between(target_dist - tol, target_dist + tol, inclusive='both')]
    dd = dd[dd['èŠãƒ»ãƒ€'].astype(str).str.startswith(str(target_surface))]

    curves = [c for c in dd['_curve'].values if isinstance(c, np.ndarray)]
    if not curves:
        return np.full(n_points, 1.0/n_points), {'Gate': 1}

    # é•·ã•ã‚’åˆã‚ã›ã¦ä¸­å¤®å€¤
    L = min(min(len(c) for c in curves), n_points)
    mat = np.vstack([np.interp(np.linspace(0,1,L), np.linspace(0,1,len(c)), c) for c in curves])
    templ = np.median(mat, axis=0)

    # FFTå¸¯åŸŸæ¯”ã‹ã‚‰ Gate ã‚’ç°¡æ˜“åˆ¤å®š
    f = np.fft.rfft(templ - templ.mean())
    pow_spec = np.abs(f)**2
    # ä½/ä¸­/é«˜ã®ç°¡æ˜“å¸¯åŸŸæ¯”
    n = len(pow_spec)
    low = pow_spec[:max(2, n//6)].sum()
    mid = pow_spec[max(2, n//6):max(4, n//3)].sum()
    high= pow_spec[max(4, n//3):].sum()

    # é«˜å‘¨æ³¢(ç¬ç™º)ãŒå¼·ã‘ã‚Œã°2ã€ä½å‘¨æ³¢(æŒä¹…)ãŒå¼·ã‘ã‚Œã°0ã€ãã®ä»–1
    if high > max(low, mid) * 1.15:
        gate = 2
    elif low > max(mid, high) * 1.15:
        gate = 0
    else:
        gate = 1

    return templ, {'Gate': gate}

def infer_gate_from_curve(curve: np.ndarray, n_points: int = 128):
    """1æœ¬ã®é€Ÿåº¦æ›²ç·šã‹ã‚‰ 0=æŒä¹… / 1=ä¸­åº¸ / 2=ç¬ç™º ã‚’FFTå¸¯åŸŸæ¯”ã§æ¨å®š"""
    if not isinstance(curve, np.ndarray) or curve.size == 0 or np.isnan(curve).all():
        return np.nan
    L = min(len(curve), n_points)
    x = np.interp(np.linspace(0,1,L), np.linspace(0,1,len(curve)), curve)
    x = x - np.nanmean(x)
    x = np.nan_to_num(x, nan=0.0)
    f = np.fft.rfft(x)
    ps = np.abs(f)**2
    n = len(ps)
    low  = ps[:max(2, n//6)].sum()
    mid  = ps[max(2, n//6):max(4, n//3)].sum()
    high = ps[max(4, n//3):].sum()
    if high > max(low, mid) * 1.15:
        return 2  # ç¬ç™º
    elif low > max(mid, high) * 1.15:
        return 0  # æŒä¹…
    else:
        return 1  # ä¸­åº¸

# ===== é¦¬ã”ã¨é›†è¨ˆ =====
agg=[]
for name, g in _df.groupby('é¦¬å'):
    avg=g['score_norm'].mean()
    std=g['score_norm'].std(ddof=0)
    wavg = np.average(g['score_norm'], weights=g['_w']) if g['_w'].sum()>0 else np.nan
    wstd = w_std_unbiased(g['score_norm'], g['_w'], ddof=1) if len(g)>=2 else np.nan
    agg.append({'é¦¬å':_trim_name(name),'AvgZ':avg,'Stdev':std,'WAvgZ':wavg,'WStd':wstd,'Nrun':len(g)})

df_agg=pd.DataFrame(agg)
if df_agg.empty:
    st.error('éå»èµ°ã®é›†è¨ˆãŒç©ºã§ã™ã€‚'); st.stop()

# WStdã®åºŠ
wstd_nontrivial=df_agg.loc[df_agg['Nrun']>=2,'WStd']
def_std=float(wstd_nontrivial.median()) if wstd_nontrivial.notna().any() else 6.0
min_floor=max(1.0, def_std*0.6)
df_agg['WStd']=df_agg['WStd'].fillna(def_std)
df_agg.loc[df_agg['WStd']<min_floor,'WStd']=min_floor

# ä»Šæ—¥æƒ…å ±
for df_ in [df_agg, horses]:
    if 'é¦¬å' in df_.columns: df_['é¦¬å']=df_['é¦¬å'].map(_trim_name)

if 'è„šè³ª' in horses.columns: horses['è„šè³ª']=horses['è„šè³ª'].map(normalize_style)

cols_to_merge = ['é¦¬å','æ ','ç•ª','è„šè³ª','æ€§åˆ¥','å¹´é½¢']
cols_to_merge = [c for c in cols_to_merge if c in horses.columns]
df_agg = df_agg.merge(horses[cols_to_merge], on='é¦¬å', how='left')

# === æ–°è¦: ç‰¹æ€§Ptsï¼ˆæ€§åˆ¥ãƒ»è„šè³ªãƒ»å¹´é½¢ãƒ»æ ï¼‰ ===
idx = df_agg.index

# æ€§åˆ¥
sex_map = {'ç‰¡': SEX_MALE, 'ç‰': SEX_FEMA, 'ã‚»': SEX_GELD, 'é¨™': SEX_GELD, 'ã›ã‚“': SEX_GELD}
sex_series = df_agg['æ€§åˆ¥'] if 'æ€§åˆ¥' in df_agg.columns else pd.Series(['']*len(idx), index=idx)
df_agg['SexPts'] = sex_series.astype(str).map(sex_map).fillna(0.0).astype(float)

# è„šè³ª
style_map = {'é€ƒã’': STL_NIGE, 'å…ˆè¡Œ': STL_SENKO, 'å·®ã—': STL_SASHI, 'è¿½è¾¼': STL_OIKOMI}
style_series = df_agg['è„šè³ª'] if 'è„šè³ª' in df_agg.columns else pd.Series(['']*len(idx), index=idx)
df_agg['StylePts'] = style_series.astype(str).map(style_map).fillna(0.0).astype(float)

# å¹´é½¢ï¼ˆãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®è·é›¢ã§æ¸›ç‚¹ï¼‰
age_series = pd.to_numeric(df_agg['å¹´é½¢'] if 'å¹´é½¢' in df_agg.columns else pd.Series([np.nan]*len(idx), index=idx), errors='coerce')
df_agg['AgePts'] = (-float(AGE_SLOPE) * (age_series - int(AGE_PEAK)).abs()).fillna(0.0)

# æ ï¼ˆå†…å¤–ãƒã‚¤ã‚¢ã‚¹ï¼‰
w = pd.to_numeric(df_agg['æ '] if 'æ ' in df_agg.columns else pd.Series([np.nan]*len(idx), index=idx), errors='coerce')
centered = (4.5 - w) / 3.5   # æ 1=+1, æ 8=-1ï¼ˆå†…ãŒæ­£ï¼‰
if WAKU_DIR == "å†…æœ‰åˆ©":
    waku_raw = centered
elif WAKU_DIR == "å¤–æœ‰åˆ©":
    waku_raw = -centered
else:
    waku_raw = pd.Series(0.0, index=idx)
df_agg['WakuPts'] = (float(WAKU_STR) * pd.to_numeric(waku_raw)).fillna(0.0)

# === å³/å·¦é›†è¨ˆï¼ˆscore_adjã®é‡ã¿å¹³å‡ï¼‰ ===
turn_base = (
    _df[['é¦¬å','å›ã‚Š','score_adj','_w']]
    .dropna(subset=['é¦¬å','å›ã‚Š','score_adj','_w'])
    .copy()
)

def _wavg_score(g: pd.DataFrame) -> float:
    w = pd.to_numeric(g['_w'], errors='coerce').to_numpy(float)
    x = pd.to_numeric(g['score_adj'], errors='coerce').to_numpy(float)
    sw = np.nansum(w)
    return float(np.nansum(w * x) / sw) if sw > 0 else np.nan

# å³å›ã‚Š
right = (
    turn_base[turn_base['å›ã‚Š'].astype(str) == 'å³']
    .groupby('é¦¬å', as_index=False)
    .apply(lambda g: pd.Series({'RightZ': _wavg_score(g)}))
    .reset_index(drop=True)
)

# å·¦å›ã‚Š
left = (
    turn_base[turn_base['å›ã‚Š'].astype(str) == 'å·¦']
    .groupby('é¦¬å', as_index=False)
    .apply(lambda g: pd.Series({'LeftZ': _wavg_score(g)}))
    .reset_index(drop=True)
)

# å‡ºèµ°æœ¬æ•°ã‚«ã‚¦ãƒ³ãƒˆ
counts = (
    turn_base.assign(_one=1)
    .pivot_table(index='é¦¬å', columns='å›ã‚Š', values='_one', aggfunc='sum', fill_value=0)
    .rename(columns={'å³': 'nR', 'å·¦': 'nL'})
    .reset_index()
)

# ã¾ã¨ã‚ã¦çµåˆ
turn_pref = (
    pd.merge(right, left, on='é¦¬å', how='outer')
    .merge(counts, on='é¦¬å', how='left')
    .fillna({'nR': 0, 'nL': 0})
)

# æ¬ æä¿é™º
for c in ['RightZ','LeftZ','nR','nL']:
    if c not in turn_pref.columns:
        turn_pref[c] = np.nan if c in ['RightZ','LeftZ'] else 0

# ä»¥é™ã¯å…ƒã®è¨ˆç®—ã®ã¾ã¾
turn_pref['TurnGap'] = (turn_pref['RightZ'].fillna(0) - turn_pref['LeftZ'].fillna(0))
turn_pref['n_eff_turn'] = (turn_pref['nR'].fillna(0) + turn_pref['nL'].fillna(0)).clip(lower=0)
conf = np.clip(turn_pref['n_eff_turn'] / 3.0, 0.0, 1.0)
turn_pref['TurnPrefPts'] = np.clip(turn_pref['TurnGap'] / 1.5, -1.0, 1.0) * conf

df_agg = df_agg.merge(
    turn_pref[['é¦¬å','RightZ','LeftZ','TurnGap','n_eff_turn','TurnPrefPts']],
    on='é¦¬å',
    how='left'
)



# è·é›¢Ã—å›ã‚Šï¼ˆè‡ªå‹•hï¼‰
rows=[]
for nm in df_agg['é¦¬å'].astype(str):
    prof=dist_turn_profile(nm, hist_for_turn, int(TARGET_DISTANCE), str(TARGET_TURN), h_auto, opp_turn_w=0.5)
    rows.append({'é¦¬å':nm, **prof})
_dfturn = pd.DataFrame(rows)
df_agg = df_agg.merge(_dfturn, on='é¦¬å', how='left')

# ===== ã“ã“ã‹ã‚‰ ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼ˆFFT+DTWï¼‰ã‚’æœ¬ç·šã«çµ±åˆ =====
with st.sidebar.expander("ğŸ“¡ ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨­å®š", expanded=True):
    spectral_weight_ui = st.slider("ã‚¹ãƒšã‚¯ãƒˆãƒ«é©åˆä¿‚æ•°", 0.0, 3.0, 1.0, 0.1)
    templ_tol_m = st.slider("ãƒ†ãƒ³ãƒ—ãƒ¬è·é›¢è¨±å®¹å¹…(Â±m)", 50, 400, 100, 25)

# ===== ç‰©ç†ï¼ˆèª¿æ•™ï¼‰ãƒ–ãƒ­ãƒƒã‚¯ =====
with st.sidebar.expander("ğŸ‡ ç‰©ç†ï¼ˆèª¿æ•™ï¼‰", expanded=True):
    USE_PHYSICS = st.checkbox("ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã†ï¼ˆèª¿æ•™Ã—åŠ›å­¦ï¼‰", True)
    # ã‚¹ãƒšã‚¯ãƒˆãƒ« : ç‰©ç† ã®æ¯”ç‡ï¼ˆåˆæˆã®â€œé…åˆ†â€ï¼‰
    spec_phys_ratio = st.slider("ã‚¹ãƒšã‚¯ãƒˆãƒ« : ç‰©ç† ã®æ¯”ç‡", 0.0, 1.0, 0.6, 0.05)
    spec_ratio = float(spec_phys_ratio)
    phys_ratio = 1.0 - spec_ratio

    # ä»»æ„ã®åˆæœŸå€¤ï¼ˆåŠ¹ããŒè‰¯ã„å®Ÿæˆ¦å€¤ï¼‰
    Crr_wood = st.number_input("Crrï¼ˆè»¢ãŒã‚ŠæŠµæŠ—ï¼‰: ã‚¦ãƒƒãƒ‰", 0.0, 0.06, 0.020, 0.001, help="æ¨å¥¨: 0.020")
    Crr_hill = st.number_input("Crrï¼ˆè»¢ãŒã‚ŠæŠµæŠ—ï¼‰: å‚è·¯", 0.0, 0.06, 0.014, 0.001, help="æ¨å¥¨: 0.014")
    CdA      = st.number_input("CdAï¼ˆç©ºåŠ›ãƒ•ãƒ­ãƒ³ãƒˆ[mÂ²]ï¼‰", 0.2, 1.6, 0.80, 0.05, help="æ¨å¥¨: 0.8")
    rho_air  = st.number_input("ç©ºæ°—å¯†åº¦ Ï[kg/mÂ³]", 0.8, 1.5, 1.20, 0.01)
    Pmax_wkg = st.number_input("æœ€å¤§ç™ºæ®å‡ºåŠ› Pmax[W/kg]", 10.0, 30.0, 20.0, 0.5)
    Emax_jkg = st.number_input("å¯ç”¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ Emax[J/kg/800m]", 600.0, 4000.0, 1800.0, 50.0)
    half_life_train_days = st.slider("èª¿æ•™å¯„ä¸ã®åŠæ¸›æœŸï¼ˆæ—¥ï¼‰", 3, 60, 18, 1)


# éå»èµ°ã”ã¨ã«é€Ÿåº¦æ›²ç·šã‚’æ§‹ç¯‰ï¼ˆå‰åŠã§å®šç¾©ã—ãŸ pseudo_curve ã‚’ä½¿ç”¨ï¼‰
s0_spec = s0.copy()
for need in ['è·é›¢','èµ°ç ´ã‚¿ã‚¤ãƒ ç§’','ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ ']:
    if need not in s0_spec.columns: s0_spec[need] = np.nan
for opt in ['PCI','PCI3','RPCI','é€šé4è§’','é ­æ•°','èŠãƒ»ãƒ€']:
    if opt not in s0_spec.columns: s0_spec[opt] = np.nan

def _pos_ratio_4c(row) -> float:
    p = pd.to_numeric(row.get('é€šé4è§’', np.nan), errors='coerce')
    n = pd.to_numeric(row.get('é ­æ•°', np.nan), errors='coerce')
    if not np.isfinite(p) or not np.isfinite(n) or n<=1: return 0.5
    return float((n - p) / (n - 1))  # å…ˆè¡Œ=1.0, å¾Œæ–¹=0.0

s0_spec['_pos'] = s0_spec.apply(_pos_ratio_4c, axis=1)
s0_spec['_curve'] = s0_spec.apply(
    lambda r: pseudo_curve(
        pd.to_numeric(r['è·é›¢'], errors='coerce'),
        pd.to_numeric(r['èµ°ç ´ã‚¿ã‚¤ãƒ ç§’'], errors='coerce'),
        pd.to_numeric(r['ä¸ŠãŒã‚Š3Fã‚¿ã‚¤ãƒ '], errors='coerce'),
        pd.to_numeric(r['PCI'], errors='coerce'),
        pd.to_numeric(r['PCI3'], errors='coerce'),
        pd.to_numeric(r['RPCI'], errors='coerce'),
        float(r['_pos'])
    ),
    axis=1
)

# ãƒ†ãƒ³ãƒ—ãƒ¬æ›²ç·šï¼ˆè·é›¢Â±100m & åŒSurfaceã®ä¸­å¤®å€¤ï¼‰
templ_curve, templ_info = build_template_curves(
    s0_spec[['é¦¬å','è·é›¢','èŠãƒ»ãƒ€','_curve']].copy(),
    int(TARGET_DISTANCE),
    str(TARGET_SURFACE),
    tol=int(templ_tol_m)
)


# å„é¦¬ã®DTWæœ€å°è·é›¢â†’ZåŒ–ï¼ˆå¤§ãã„ã»ã©é©åˆè‰¯ï¼‰
rows=[]
for name, g in s0_spec.groupby('é¦¬å'):
    dists=[]; gates=[]
    for v in g['_curve'].values:
        if isinstance(v, np.ndarray):
            dists.append(dtw_distance(v, templ_curve))
            gates.append(infer_gate_from_curve(v))
    DTWmin = float(np.nanmin(dists)) if dists else np.nan
    gate_hat = float(np.nanmedian([x for x in gates if np.isfinite(x)])) if gates else np.nan
    rows.append({'é¦¬å':name, 'DTWmin':DTWmin, 'SpecGate_horse': gate_hat})
df_spec = pd.DataFrame(rows)

if not df_spec.empty:
    mu = float(df_spec['DTWmin'].mean(skipna=True))
    sd = float(df_spec['DTWmin'].std(ddof=0, skipna=True))
    if not np.isfinite(sd) or sd==0.0: sd=1.0
    df_spec['SpecFitZ'] = -(df_spec['DTWmin'] - mu) / sd
else:
    df_spec = pd.DataFrame({'é¦¬å': df_agg['é¦¬å'], 'SpecFitZ': np.nan, 'SpecGate_horse': np.nan})

# ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆãƒ¬ãƒ¼ã‚¹å…¨ä½“ï¼‰ã®å‹ã¯å…¨å“¡åŒã˜å€¤ã¨ã—ã¦åˆ—ã‚’æŒãŸã›ã‚‹
df_spec['SpecGate_templ'] = templ_info.get('Gate', 1)

df_agg = df_agg.merge(df_spec, on='é¦¬å', how='left')
# æ•°å€¤ã‚²ãƒ¼ãƒˆ(0/1/2) â†’ ãƒ©ãƒ™ãƒ«ã¸
_gate_label = {0: 'æŒä¹…', 1: 'ä¸­åº¸', 2: 'ç¬ç™º'}
df_agg['SpecGate_horse']  = pd.to_numeric(df_agg['SpecGate_horse'], errors='coerce')  # å¿µã®ãŸã‚
df_agg['SpecGate_templ']  = pd.to_numeric(df_agg['SpecGate_templ'], errors='coerce')

df_agg['SpecGate_horse_lbl'] = df_agg['SpecGate_horse'].map(_gate_label)
df_agg['SpecGate_templ_lbl'] = df_agg['SpecGate_templ'].map(_gate_label)

# ===== èª¿æ•™ï¼ˆç‰©ç†ï¼‰â†’ PhysicsZ ã‚’ä½œã‚‹ =====
df_phys = pd.DataFrame()

if USE_PHYSICS:
    valid_trains = []

    # å…¥ã£ã¦ãã‚‹å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿ã€å¿…é ˆåˆ—ãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ã ã‘æ¡ç”¨
    for f, kind in [(wood_file, 'wood'), (hill_file, 'hill')]:
        if f is None:
            continue
        tdf = _read_train_xlsx(f, kind)  # æœŸå¾…åˆ—: é¦¬å, æ—¥ä»˜, _kind, _lap_sec, _intensity
        if isinstance(tdf, pd.DataFrame) and not tdf.empty:
            need = {'é¦¬å', 'æ—¥ä»˜', '_lap_sec'}
            if need.issubset(set(tdf.columns)):
                # ä½™è¨ˆãªåˆ—ã¯ã‚ã£ã¦ã‚‚è‰¯ã„ãŒã€æœ€ä½é™ã®åˆ—ã¯ãã‚ãˆã¦ãŠã
                keep = [c for c in ['é¦¬å','æ—¥ä»˜','_kind','_lap_sec','_intensity'] if c in tdf.columns]
                valid_trains.append(tdf[keep])

    if valid_trains:
        T = pd.concat(valid_trains, ignore_index=True)
        # ã“ã“ã§ KeyError ãŒå‡ºãªã„ã‚ˆã†ã«åˆ—å­˜åœ¨ã‚’å†ç¢ºèª
        if {'é¦¬å','æ—¥ä»˜'}.issubset(T.columns):
            T = T.dropna(subset=['é¦¬å','æ—¥ä»˜'])
            if not T.empty:
                df_phys = _derive_training_metrics(
                    train_df=T, s0_races=_df.copy(),
                    Crr_wood=Crr_wood, Crr_hill=Crr_hill,
                    CdA=CdA, rho=rho_air,
                    Pmax_wkg=Pmax_wkg, Emax_jkg=Emax_jkg,
                    half_life_days=int(half_life_train_days)
                )
with st.expander("èª¿æ•™ãƒ‘ãƒ¼ã‚¹çŠ¶æ³", expanded=False):
    st.write("woodãƒ•ã‚¡ã‚¤ãƒ«: ", wood_file.name if wood_file else None,
             " / hillãƒ•ã‚¡ã‚¤ãƒ«: ", hill_file.name if hill_file else None)
    try:
        trains_dbg = []
        if wood_file: trains_dbg.append(_read_train_xlsx(wood_file, 'wood'))
        if hill_file: trains_dbg.append(_read_train_xlsx(hill_file, 'hill'))
        T_dbg = pd.concat(trains_dbg, ignore_index=True) if trains_dbg else pd.DataFrame()

        st.write("æŠ½å‡ºè¡Œæ•°(èª¿æ•™):", len(T_dbg))
        if not T_dbg.empty:
            st.dataframe(T_dbg.head(10))
            st.write("åˆ—:", list(T_dbg.columns))
            st.write("èª¿æ•™ æ—¥ä»˜ç¯„å›²:", T_dbg['æ—¥ä»˜'].min(), "â†’", T_dbg['æ—¥ä»˜'].max())

            # ãƒãƒ¼ã‚¸ã§ãã‚‹åå‰ã®çªåˆ
            base_names = set(df_agg['é¦¬å'].astype(str))
            phys_names = set(T_dbg['é¦¬å'].astype(str))
            inter = sorted(base_names & phys_names)
            miss  = sorted(base_names - phys_names)
            st.write("å‡ºèµ°è¡¨âˆ©èª¿æ•™ äº¤å·®:", len(inter))
            if miss:
                st.write("æœªãƒãƒƒãƒï¼ˆå‡ºèµ°è¡¨ã«ã‚ã‚‹ãŒèª¿æ•™ã«ç„¡ã„ï¼‰ä¾‹:", miss[:10])
    except Exception as e:
        st.write("ãƒ‡ãƒãƒƒã‚°ä¸­ã«ä¾‹å¤–:", e)

    st.write("df_phys è¡Œæ•°:", 0 if df_phys.empty else len(df_phys))
    if not df_phys.empty:
        st.dataframe(df_phys.head(10))


# ç‰©ç†DFãŒç©ºãªã‚‰ãƒ€ãƒŸãƒ¼åˆ—ã‚’ç”¨æ„ï¼ˆä»¥é™ã® merge ã§ã‚³ã‚±ãªã„ã‚ˆã†ã«ï¼‰
if df_phys.empty:
    df_phys = pd.DataFrame({
        'é¦¬å': df_agg['é¦¬å'],
        'EAP': np.nan, 'PeakWkg': np.nan, 'EffReserve': np.nan, 'PhysicsZ': np.nan
    })
# ã‚†ã‚‹ã„çµåˆï¼ˆdifflibï¼‰ã§æ•‘æ¸ˆï¼šäº¤å·®ãŒå°‘ãªã„ã¨ãã ã‘
try:
    base_names = df_agg['é¦¬å'].astype(str).tolist()
    phys_names = df_phys['é¦¬å'].astype(str).tolist()
    inter = set(base_names) & set(phys_names)
    if len(inter) <= max(2, len(base_names)//4):
        import difflib
        # èª¿æ•™å´ â†’ å‡ºèµ°å´ ã¸ç½®æ›ãƒãƒƒãƒ—ã‚’ä½œã‚‹
        rep = {}
        for pn in phys_names:
            m = difflib.get_close_matches(pn, base_names, n=1, cutoff=0.90)
            if m:
                rep[pn] = m[0]
        if rep:
            df_phys = df_phys.assign(__join=df_phys['é¦¬å'].replace(rep)).drop(columns=['é¦¬å']).rename(columns={'__join':'é¦¬å'})
except Exception:
    pass

df_agg = df_agg.merge(df_phys, on='é¦¬å', how='left')
for c in ['PhysicsZ', 'PeakWkg', 'EAP']:
    df_agg[c] = pd.to_numeric(df_agg.get(c), errors='coerce')

# ===== RecencyZ / StabZ =====
base_for_recency = df_agg.get('WAvgZ', pd.Series(np.nan, index=df_agg.index)).fillna(df_agg.get('AvgZ', pd.Series(0.0, index=df_agg.index)))
df_agg['RecencyZ']=z_score(pd.to_numeric(base_for_recency, errors='coerce').fillna(0.0))
wstd_fill=pd.to_numeric(df_agg['WStd'], errors='coerce')
if not np.isfinite(wstd_fill).any(): wstd_fill=pd.Series(6.0, index=df_agg.index)
df_agg['StabZ']=z_score(-(wstd_fill.fillna(wstd_fill.median())))

# D) ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ é‡ã¿ è‡ªå·±å­¦ç¿’
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns and s1['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].notna().any():
    bt = _df.merge(s1[['é¦¬å','ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’']], on='é¦¬å', how='left')
    bt_min=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].min(skipna=True); bt_max=bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].max(skipna=True)
    span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
    bt['BT_norm']=((bt_max - bt['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'])/span).clip(0,1)
    corr = np.corrcoef(bt['BT_norm'].fillna(0.0), bt['score_adj'].fillna(0.0))[0,1]
    if not np.isfinite(corr): corr=0.0
    w_bt = float(np.clip(corr, 0.0, 1.2)) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)
else:
    w_bt = 0.0 if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(besttime_w_manual)

# ===== æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆæœªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™ï¼‰ =====
turn_gain = 1.0
pace_gain = float(pace_gain)
stab_weight = float(stab_weight)
dist_gain = 1.0

# === æ¬ æã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆFinalRawã®ç›´å‰ã§ä¸€åº¦ã ã‘ï¼‰ ===
df_agg['RecencyZ']    = pd.to_numeric(df_agg['RecencyZ'], errors='coerce').fillna(df_agg['RecencyZ'].median())
df_agg['StabZ']       = pd.to_numeric(df_agg['StabZ'],    errors='coerce').fillna(df_agg['StabZ'].median())
df_agg['TurnPrefPts'] = pd.to_numeric(df_agg['TurnPrefPts'], errors='coerce').fillna(0.0)
df_agg['DistTurnZ']   = pd.to_numeric(df_agg['DistTurnZ'],   errors='coerce').fillna(0.0)

df_agg['PacePts']=0.0  # å¾Œã§MCã‹ã‚‰
# FinalRawï¼ˆåŸºç¤ï¼šRecency/Stab/Turn/Dist + ç‰¹æ€§ï¼‰
df_agg['FinalRaw'] = (
    df_agg['RecencyZ']
    + float(stab_weight) * df_agg['StabZ']
    + 1.0 * df_agg['TurnPrefPts']
    + 1.0 * df_agg['DistTurnZ'].fillna(0.0)
    + df_agg['SexPts'] + df_agg['StylePts'] + df_agg['AgePts'] + df_agg['WakuPts']
)

# æ–¤é‡ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆä¸­å¤®å€¤åŸºæº–ï¼‰
if 'æ–¤é‡' in df_agg.columns and pd.to_numeric(df_agg['æ–¤é‡'], errors='coerce').notna().any():
    kg = pd.to_numeric(df_agg['æ–¤é‡'], errors='coerce')
    kg_med = float(np.nanmedian(kg))
    df_agg['FinalRaw'] -= float(weight_coeff) * (kg - kg_med).fillna(0.0)

# BTã‚’åŠ ç‚¹
if 'ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’' in s1.columns:
    btmap = s1.set_index('é¦¬å')['ãƒ™ã‚¹ãƒˆã‚¿ã‚¤ãƒ ç§’'].to_dict()
    btvals = df_agg['é¦¬å'].map(btmap)
    if pd.Series(btvals).notna().any():
        bts = pd.Series(btvals)
        bt_min=bts.min(skipna=True); bt_max=bts.max(skipna=True)
        span=(bt_max-bt_min) if (pd.notna(bt_min) and pd.notna(bt_max) and bt_max>bt_min) else 1.0
        BT_norm = ((bt_max - bts)/span).clip(0,1).fillna(0.0)
        df_agg['FinalRaw'] += w_bt * BT_norm

# â˜… ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯„ä¸ã‚’æœ€å¾Œã«åˆæˆ
# â˜… ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯„ä¸ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¿‚æ•° Ã— é…åˆ†ï¼‰
df_agg['SpecFitZ'] = pd.to_numeric(df_agg['SpecFitZ'], errors='coerce')
df_agg['FinalRaw'] += spec_ratio * float(spectral_weight_ui) * df_agg['SpecFitZ'].fillna(0.0)

# â˜… ç‰©ç†å¯„ä¸ï¼ˆZ=50ã‚’0åŸºæº–, 10åˆ»ã¿ã§ä»–Zã¨ã‚¹ã‚±ãƒ¼ãƒ«åˆã‚ã›ï¼‰Ã— é…åˆ†
df_agg['PhysicsZ'] = pd.to_numeric(df_agg['PhysicsZ'], errors='coerce')
df_agg['FinalRaw'] += phys_ratio * ((df_agg['PhysicsZ'] - 50.0) / 10.0).fillna(0.0)

# ===== ãƒšãƒ¼ã‚¹MCï¼ˆåå¯¾ç§°Gumbelã§åˆ†æ•£ä½æ¸›ï¼‰ =====
mark_rule={
    'ãƒã‚¤ãƒšãƒ¼ã‚¹':      {'é€ƒã’':'â–³','å…ˆè¡Œ':'â–³','å·®ã—':'â—','è¿½è¾¼':'ã€‡'},
    'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'ã€‡','è¿½è¾¼':'â–³'},
    'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹': {'é€ƒã’':'ã€‡','å…ˆè¡Œ':'â—','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
    'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':    {'é€ƒã’':'â—','å…ˆè¡Œ':'ã€‡','å·®ã—':'â–³','è¿½è¾¼':'Ã—'},
}
mark_to_pts={'â—':2,'ã€‡':1,'â—‹':1,'â–³':0,'Ã—':-1}

name_list=df_agg['é¦¬å'].tolist()
P=np.zeros((len(name_list),4),float)
for i, nm in enumerate(name_list):
    stl = df_agg.loc[df_agg['é¦¬å']==nm, 'è„šè³ª'].values
    stl = stl[0] if len(stl)>0 else ''
    if stl in STYLES:
        P[i, STYLES.index(stl)] = 1.0
    else:
        P[i,:]=0.25

epi_alpha, epi_beta = 1.0, 0.6
thr_hi, thr_mid, thr_slow = 0.52, 0.30, 0.18

beta_pl = tune_beta(_df.copy()) if MODE=="AUTOï¼ˆæ¨å¥¨ï¼‰" else float(mc_beta_manual)

rng = np.random.default_rng(24601)
draws = 4000
Hn=len(name_list)
sum_pts=np.zeros(Hn,float); pace_counter={'ãƒã‚¤ãƒšãƒ¼ã‚¹':0,'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹':0,'ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0,'ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹':0}
for _ in range(draws//2):
    sampled = [np.argmax(P[i]) for i in range(Hn)]
    nige  = sum(1 for s in sampled if s==0)
    sengo = sum(1 for s in sampled if s==1)
    epi=(epi_alpha*nige + epi_beta*sengo)/max(1,Hn)
    if   epi>=thr_hi:   pace_t='ãƒã‚¤ãƒšãƒ¼ã‚¹'
    elif epi>=thr_mid:  pace_t='ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'
    elif epi>=thr_slow: pace_t='ã‚„ã‚„ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
    else:               pace_t='ã‚¹ãƒ­ãƒ¼ãƒšãƒ¼ã‚¹'
    pace_counter[pace_t]+=2
    mk=mark_rule[pace_t]
    for i,s in enumerate(sampled):
        sum_pts[i]+=2*mark_to_pts[ mk[STYLES[s]] ]

df_agg['PacePts']=sum_pts/max(1,draws)
pace_type=max(pace_counter, key=lambda k: pace_counter[k]) if sum(pace_counter.values())>0 else 'ãƒŸãƒ‰ãƒ«ãƒšãƒ¼ã‚¹'

# ===== ã‚¿ã‚¤ãƒ åˆ†å¸ƒ â†’ ç€é †MC =====
half_life_days = int(half_life_m * 30.4375) if half_life_m > 0 else 99999

time_model_pkg = fit_time_quantile_model(
    df_hist=_df.copy(),
    dist_turn_today_df=_dfturn.copy(),
    half_life_days=half_life_days,
    q_list=(0.2, 0.5, 0.8),
    param_grid=(80, 120, 160),
    random_state=24601
)

if time_model_pkg is not None:
    feats = time_model_pkg['feats']
    mu, sd = time_model_pkg['mu'], time_model_pkg['sd']
    models = time_model_pkg['models']

    todayX = build_today_design(
    horses_today=horses,
    s0_hist=s0,
    target_distance=int(TARGET_DISTANCE),
    target_surface=str(TARGET_SURFACE),
    dist_turn_today_df=_dfturn,
    feats=feats,
    pace_type=pace_type  # â† æ˜ç¤ºæ¸¡ã—
)


    v = todayX[feats].astype(float).to_numpy()
    v = np.where(np.isfinite(v), v, time_model_pkg['col_means'])
    vs = (v - mu) / sd

    Q20 = models[0.2].predict(vs)
    Q50 = models[0.5].predict(vs)
    Q80 = models[0.8].predict(vs)

    z08 = 0.8416212335729143
    sigma_from_span = (Q80 - Q20) / (2.0 * z08)
    sigma_raw = np.maximum(sigma_from_span, 0.25)

    sigma_hat = time_model_pkg['sigma_hat']
    sigma = np.sqrt(0.5 * sigma_raw ** 2 + 0.5 * sigma_hat ** 2)

    pred_time = pd.DataFrame({
        'é¦¬å': todayX['é¦¬å'],
        'PredTime_s': Q50,
        'PredTime_p20': Q20,
        'PredTime_p80': Q80,
        'PredSigma_s': sigma
    })

    draws_mc = 12000
    rng_t = np.random.default_rng(13579)
    names = pred_time['é¦¬å'].tolist()
    mu_vec = pred_time['PredTime_s'].to_numpy(float)
    sig_vec = pred_time['PredSigma_s'].to_numpy(float)
    n = len(mu_vec)

    tau = 0.30 * float(np.nanmedian(sig_vec))
    E_id = rng_t.normal(size=(draws_mc, n)) * sig_vec[None, :]
    E_cm = rng_t.normal(size=(draws_mc, 1)) * tau
    T = mu_vec[None, :] + E_id + E_cm

    rk = np.argsort(T, axis=1)
    win = np.bincount(rk[:, 0], minlength=n)
    top3 = np.zeros(n, int)
    for k in range(3):
        top3 += np.bincount(rk[:, k], minlength=n)
    exp_rank = (rk.argsort(axis=1) + 1).mean(axis=0)

    pred_time['å‹ç‡%_TIME'] = (100.0 * win / draws_mc).round(2)
    pred_time['è¤‡å‹ç‡%_TIME'] = (100.0 * top3 / draws_mc).round(2)
    pred_time['æœŸå¾…ç€é †_TIME'] = np.round(exp_rank, 3)

    df_agg = df_agg.merge(pred_time, on='é¦¬å', how='left')
else:
    df_agg['PredTime_s'] = np.nan
    df_agg['PredTime_p20'] = np.nan
    df_agg['PredTime_p80'] = np.nan
    df_agg['PredSigma_s'] = np.nan
    df_agg['å‹ç‡%_TIME'] = np.nan
    df_agg['è¤‡å‹ç‡%_TIME'] = np.nan
    df_agg['æœŸå¾…ç€é †_TIME'] = np.nan

# ===== PhysS1ï¼ˆã‚³ãƒ¼ã‚¹å¹¾ä½•ï¼‰ã‚’ äºˆæ¸¬ã‚¿ã‚¤ãƒ å…¥ã‚Š ã§å†è¨ˆç®— â†’ å…¨é¦¬ã¸ä»˜ä¸ =====
try:
    # ãƒ¬ãƒ¼ã‚¹æƒ³å®šã®ä»£è¡¨ã‚¿ã‚¤ãƒ ï¼ˆå„é¦¬ã®PredTimeä¸­å¤®å€¤ï¼‰
    race_pred_time = float(pd.to_numeric(df_agg['PredTime_s'], errors='coerce').median()) \
                     if 'PredTime_s' in df_agg.columns else np.nan

    races_df_today = pd.DataFrame([{
        'race_id': 'TODAY',
        'course_id': COURSE_ID,
        'surface': 'èŠ' if TARGET_SURFACE == 'èŠ' else 'ãƒ€',
        'distance_m': int(TARGET_DISTANCE),
        'layout': LAYOUT,
        'rail_state': RAIL,
        'band': TODAY_BAND,
        'num_turns': 2,
        # â˜… ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼šäºˆæ¸¬ã‚¿ã‚¤ãƒ ã®ä»£è¡¨å€¤ã‚’æ¸¡ã™ï¼ˆç„¡ã‘ã‚Œã°æ¬ æã®ã¾ã¾OKï¼‰
        'final_time_sec': race_pred_time if np.isfinite(race_pred_time) else None,
    }])

    phys1 = add_phys_s1_features(
        races_df_today,
        group_cols=(),      # 1è¡Œãªã®ã§OK
        band_col="band",
        verbose=False
    )

    phys_cols = {
        'phys_corner_load':'CornerLoadS1',
        'phys_start_cost':'StartCostS1',
        'phys_finish_grade':'FinishGradeS1',
        'phys_s1_score':'PhysS1'
    }
    pv = phys1.rename(columns=phys_cols).iloc[0]
    for k in phys_cols.values():
        df_agg[k] = float(pv[k])

    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®å¼·ã•ã§åŠ ç‚¹
    df_agg['FinalRaw'] += float(PHYS_S1_GAIN) * df_agg['PhysS1'].fillna(0.0)

except Exception as e:
    st.warning(f"PhysS1ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    for k in ['CornerLoadS1','StartCostS1','FinishGradeS1','PhysS1']:
        df_agg[k] = np.nan

# PacePtsåæ˜ 
df_agg['PacePts'] = pd.to_numeric(df_agg['PacePts'], errors='coerce').fillna(0.0)
df_agg['FinalRaw'] += float(pace_gain) * df_agg['PacePts']

# ===== å‹ç‡ï¼ˆPLè§£æè§£ï¼‰ï¼† Top3ï¼ˆGumbelåå¯¾ç§°ï¼‰ =====
calibrator = None
if do_calib and SK_ISO:
    dfh = _df.dropna(subset=['score_adj','ç¢ºå®šç€é †']).copy()
    dfh['rid'] = _make_race_id_for_hist(dfh)
    X, Y = [], []
    for _, g in dfh.groupby('rid'):
        xs = g['score_adj'].astype(float).to_numpy()
        y  = (pd.to_numeric(g['ç¢ºå®šç€é †'], errors='coerce') == 1).astype(int).to_numpy()
        if len(xs) >= 2 and np.unique(y).size >= 2:
            p = np.exp(beta_pl * (xs - xs.max()))
            s = p.sum()
            if np.isfinite(s) and s > 0:
                p = np.clip(p / s, 1e-6, 1-1e-6)
                X.append(p); Y.append(y)
    if X:
        calibrator = IsotonicRegression(out_of_bounds='clip').fit(np.concatenate(X), np.concatenate(Y))

S = pd.to_numeric(df_agg['FinalRaw'], errors='coerce')
finite = np.isfinite(S)

p_win = np.zeros(len(S), float)
if finite.any():
    A = S[finite].to_numpy(float)
    m = A.mean(); s = A.std() + 1e-9
    z = (A - m) / s
    x = beta_pl * (z - z.max())
    ex = np.exp(x)
    p = ex / ex.sum()
    p_win[finite] = p
    if (~finite).any():
        p_win[~finite] = 1e-6
        p_win = p_win / p_win.sum()
else:
    p_win[:] = 1.0 / max(len(S), 1)

if calibrator is not None:
    p_win = safe_iso_predict(calibrator, p_win)

df_agg['å‹ç‡%_PL'] = (100 * p_win).round(2)

# Top3è¿‘ä¼¼ï¼ˆGumbelåå¯¾ç§°ï¼‰
if finite.any():
    A = S.copy()
    A[~finite] = A[finite].mean()
    m = A.mean(); s = A.std() + 1e-9
    abilities = ((A - m) / s).to_numpy(float)
else:
    abilities = np.zeros(len(S), float)

draws_top3 = 8000
rng3 = np.random.default_rng(13579)
G = rng3.gumbel(size=(draws_top3//2, len(abilities)))
U = np.vstack([beta_pl*abilities[None,:] + G, beta_pl*abilities[None,:] - G])
rank_idx = np.argsort(-U, axis=1)
counts = np.zeros(len(abilities), float)
for k in range(3):
    counts += np.bincount(rank_idx[:,k], minlength=len(abilities)).astype(float)
df_agg['è¤‡å‹ç‡%_PL'] = (100*(counts / draws_top3)).round(2)

# ===== H) AR100: åˆ†ä½å†™åƒ =====
ranks = S.rank(method='average', pct=True).fillna(0.5)
qx = np.array([0.00,0.10,0.25,0.50,0.75,0.90,0.97,1.00])
qy = np.array([40 , 45 , 55 , 65 , 72 , 80 , 90 , 98 ])
df_agg['AR100'] = np.interp(ranks.to_numpy(float), qx, qy)

def to_band(v):
    if not np.isfinite(v): return 'E'
    if v>=90: return 'SS'
    if v>=80: return 'S'
    if v>=70: return 'A'
    if v>=60: return 'B'
    if v>=50: return 'C'
    return 'E'

df_agg['Band'] = df_agg['AR100'].map(to_band)

# ===== ãƒ†ãƒ¼ãƒ–ãƒ«æ•´å½¢ï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ä»˜ãï¼‰ =====
# â–¼ ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ—ã‚’è¿½åŠ ã—ãŸå®Œæˆç‰ˆ
_dfdisp = df_agg.copy().sort_values(['AR100','å‹ç‡%_PL'], ascending=[False, False]).reset_index(drop=True)
_dfdisp['é †ä½'] = np.arange(1, len(_dfdisp)+1)

def _fmt_int(x):
    try:
        return '' if pd.isna(x) else f"{int(x)}"
    except:
        return ''

show_cols = [
    'é †ä½','æ ','ç•ª','é¦¬å','è„šè³ª',
    'AR100','Band',
    'å‹ç‡%_PL','è¤‡å‹ç‡%_PL',
    'å‹ç‡%_TIME','è¤‡å‹ç‡%_TIME','æœŸå¾…ç€é †_TIME',
    'PredTime_s','PredTime_p20','PredTime_p80','PredSigma_s',
    'RecencyZ','StabZ','PacePts','TurnPrefPts','DistTurnZ',
    'SpecFitZ','SpecGate_horse_lbl','SpecGate_templ_lbl',
    'PhysicsZ','PeakWkg','EAP','CornerLoadS1','StartCostS1','FinishGradeS1','PhysS1',
]


JP = {
    'é †ä½':'é †ä½','æ ':'æ ','ç•ª':'é¦¬ç•ª','é¦¬å':'é¦¬å','è„šè³ª':'è„šè³ª',
    'AR100':'AR100','Band':'è©•ä¾¡å¸¯',
    'å‹ç‡%_PL':'å‹ç‡%ï¼ˆPLï¼‰','è¤‡å‹ç‡%_PL':'è¤‡å‹ç‡%ï¼ˆPLï¼‰',
    'å‹ç‡%_TIME':'å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','è¤‡å‹ç‡%_TIME':'è¤‡å‹ç‡%ï¼ˆã‚¿ã‚¤ãƒ ï¼‰','æœŸå¾…ç€é †_TIME':'æœŸå¾…ç€é †ï¼ˆã‚¿ã‚¤ãƒ ï¼‰',
    'PredTime_s':'äºˆæ¸¬ã‚¿ã‚¤ãƒ ä¸­å¤®å€¤[s]','PredTime_p20':'20%é€Ÿã„å´[s]','PredTime_p80':'80%é…ã„å´[s]','PredSigma_s':'ã‚¿ã‚¤ãƒ åˆ†æ•£Ïƒ[s]',
    'RecencyZ':'è¿‘èµ°Z','StabZ':'å®‰å®šæ€§Z','PacePts':'ãƒšãƒ¼ã‚¹Pts','TurnPrefPts':'å›ã‚ŠåŠ ç‚¹','DistTurnZ':'è·é›¢Ã—å›ã‚ŠZ',
    'SpecFitZ':'ã‚¹ãƒšã‚¯ãƒˆãƒ«é©åˆZ',
    'SpecGate_horse':'èµ°æ³•å‹(0=æŒä¹…,1=ä¸­åº¸,2=ç¬ç™º)',      # â† è¿½åŠ 
    'SpecGate_templ':'æƒ³å®šãƒ¬ãƒ¼ã‚¹å‹(ãƒ†ãƒ³ãƒ—ãƒ¬)'             # â† è¿½åŠ 
}
JP.update({
    'SpecGate_horse_lbl': 'èµ°æ³•å‹',
    'SpecGate_templ_lbl': 'æƒ³å®šãƒ¬ãƒ¼ã‚¹å‹(ãƒ†ãƒ³ãƒ—ãƒ¬)',
    'PhysicsZ':'ç‰©ç†Z',
    'PeakWkg':'ãƒ”ãƒ¼ã‚¯W/kg',
    'EAP':'EAP[J/kg/m]'
})


_dfdisp_view = _dfdisp[show_cols].rename(columns=JP)

fmt = {
    JP['AR100']:'{:.1f}',
    JP['å‹ç‡%_PL']:'{:.2f}',
    JP['è¤‡å‹ç‡%_PL']:'{:.2f}',
    JP['RecencyZ']:'{:.2f}',
    JP['StabZ']:'{:.2f}',
    JP['PacePts']:'{:.2f}',
    JP['TurnPrefPts']:'{:.2f}',
    JP['DistTurnZ']:'{:.2f}',
    JP['PredTime_s']:'{:.3f}',
    JP['PredTime_p20']:'{:.3f}',
    JP['PredTime_p80']:'{:.3f}',
    JP['PredSigma_s']:'{:.3f}',
    JP['SpecFitZ']:'{:.2f}',
}

num_fmt = {
    JP['æ ']: _fmt_int,
    JP['ç•ª']: _fmt_int,
}
fmt.update({
    JP['PhysicsZ']:'{:.2f}',
    JP['PeakWkg']:'{:.2f}',
    JP['EAP']:'{:.3f}',
})
num_fmt.update(fmt)

styled = (
    _dfdisp_view
      .style
      .apply(_style_waku, subset=[JP['æ ']])
      .format(num_fmt, na_rep="")
)

st.markdown("### æœ¬å‘½ãƒªã‚¹ãƒˆï¼ˆAUTOçµ±åˆï¼‹ã‚¹ãƒšã‚¯ãƒˆãƒ«ï¼‰")
st.dataframe(styled, use_container_width=True, height=H(len(_dfdisp_view)))

# ä¸Šä½æŠœç²‹ï¼ˆ6é ­ï¼‰
head_cols = ['é †ä½','æ ','ç•ª','é¦¬å','AR100','Band','å‹ç‡%_PL','å‹ç‡%_TIME','PredTime_s','PredSigma_s','PacePts','SpecFitZ','PhysicsZ','PeakWkg','EAP','CornerLoadS1','StartCostS1','FinishGradeS1','PhysS1']
base = _dfdisp.rename(columns=JP) if '_dfdisp' in globals() else _dfdisp_view
cols_jp = [JP[c] if c in JP else c for c in head_cols]
head_view = base[cols_jp].head(6).copy()

st.markdown("#### ä¸Šä½æŠœç²‹")
st.dataframe(
    head_view.style.format({
        JP['æ ']: _fmt_int,
        JP['ç•ª']: _fmt_int,
        JP['AR100']:'{:.1f}',
        JP['å‹ç‡%_PL']:'{:.2f}',
        JP['å‹ç‡%_TIME']:'{:.2f}',
        JP['PacePts']:'{:.2f}',
        JP['PredTime_s']:'{:.3f}',
        JP['PredSigma_s']:'{:.3f}',
        JP['SpecFitZ']:'{:.2f}',
    }),
    use_container_width=True, height=H(len(head_view))
)

# è¦‹é€ã‚Šç›®å®‰
if not (_dfdisp['AR100'] >= 70).any():
    st.warning('ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã¯ã€Œè¦‹é€ã‚Šã€ï¼šAä»¥ä¸Šï¼ˆAR100â‰¥70ï¼‰ãŒä¸åœ¨ã€‚')
else:
    lead = _dfdisp.iloc[0]

    def _fmt_int_str(x):
        import pandas as pd, numpy as np
        try:
            v = pd.to_numeric(x)
            return "" if pd.isna(v) else f"{int(v)}"
        except Exception:
            return ""

    def _fmt_float(x, n):
        import pandas as pd
        try:
            v = float(x)
            return f"{v:.{n}f}"
        except Exception:
            return "â€”"

    waku   = _fmt_int_str(lead.get('æ '))
    umaban = _fmt_int_str(lead.get('ç•ª'))
    win    = _fmt_float(lead.get('å‹ç‡%_PL'), 2)
    ar100  = _fmt_float(lead.get('AR100'), 1)

    # ã©ã¡ã‚‰ã‹ç•ªå·ãŒç©ºãªã‚‰ãƒã‚¤ãƒ•ãƒ³çœç•¥
    num_part = f"{waku}-{umaban}".strip("-")
    title = f"**{num_part} {lead.get('é¦¬å','')}**" if num_part else f"**{lead.get('é¦¬å','')}**"

    st.info(f"æœ¬å‘½å€™è£œï¼š{title} / å‹ç‡{win}% / AR100 {ar100}")

# 4è§’å›³ï¼ˆä»»æ„ï¼‰
if SHOW_CORNER:
    try:
        from matplotlib.patches import Wedge, Rectangle, Circle
        fig, ax = plt.subplots(figsize=(6,4))
        xs={'é€ƒã’':0.1,'å…ˆè¡Œ':0.3,'å·®ã—':0.7,'è¿½è¾¼':0.9}
        for _,r in _dfdisp.iterrows():
            x=xs.get(r.get('è„šè³ª',''),0.5); y=float(r.get('AR100',50))/100
            ax.scatter(x,y)
            ax.annotate(str(r.get('ç•ª','')), (x,y), xytext=(3,3), textcoords='offset points', fontsize=8)
        ax.set_xlim(0,1); ax.set_ylim(0,1); ax.set_xlabel('è„šè³ªå´'); ax.set_ylabel('AR100æ­£è¦åŒ–')
        ax.grid(alpha=.3)
        st.pyplot(fig)
    except Exception as e:
        st.caption(f"4è§’ãƒã‚¸ã‚·ãƒ§ãƒ³å›³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼š{e}")

# è¨ºæ–­ã‚¿ãƒ–ï¼ˆæ ¡æ­£/NDCGãªã©ï¼‰
with st.expander('ğŸ“ˆ è¨ºæ–­ï¼ˆæ ¡æ­£ãƒ»NDCGï¼‰', expanded=False):
    try:
        df_tmp=_df[['ãƒ¬ãƒ¼ã‚¹æ—¥','ç«¶èµ°å','score_adj','ç¢ºå®šç€é †']].dropna().copy()
        df_tmp['race_id']=pd.to_datetime(df_tmp['ãƒ¬ãƒ¼ã‚¹æ—¥'], errors='coerce').dt.strftime('%Y%m%d') + '_' + df_tmp['ç«¶èµ°å'].astype(str)
        df_tmp['y']=(pd.to_numeric(df_tmp['ç¢ºå®šç€é †'], errors='coerce')==1).astype(int)
        pr=[]
        for rid, g in df_tmp.groupby('race_id'):
            s=g['score_adj'].astype(float).to_numpy(); p=np.exp(beta_pl*(s-s.max())); p/=p.sum(); pr.append(p)
        p_raw=np.concatenate(pr) if pr else np.array([])
        ndcg=ndcg_by_race(df_tmp[['race_id','y']], p_raw, k=3)
        st.caption(f"NDCG@3ï¼ˆæœªæ ¡æ­£softmaxã®æ“¬ä¼¼ï¼‰: {ndcg:.4f}")
    except Exception:
        pass
    if calibrator is None and do_calib:
        st.warning('æ ¡æ­£å™¨ã®å­¦ç¿’ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚')
    elif calibrator is not None:
        st.success('ç­‰æ¸©å›å¸°ã§å‹ç‡ã‚’æ ¡æ­£ä¸­ã€‚')

st.markdown("""
<small>
- æœ¬ç‰ˆã¯ **AUTOãƒ¢ãƒ¼ãƒ‰** ãŒæ¨™æº–ã§ã™ã€‚æ‰‹å‹•ã¯ã€ŒğŸ› æ‰‹å‹•ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰ã€ã‚’å±•é–‹ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚<br>
- **score_adj** ã‚’åŸºæº–ã«ã€è·é›¢Ã—å›ã‚Šãƒ»å³å·¦ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ«é©åˆã‚’çµ±åˆã—ã€PLâ†’Top3â†’AR100 ã¸é€£çµã—ã¾ã—ãŸã€‚<br>
- ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¯ **FFTã®å¸¯åŸŸåˆ¤å®š** ã¨ **DTWé©åˆZ** ã‚’ä½¿ç”¨ã€‚ãƒ†ãƒ³ãƒ—ãƒ¬ã¯åŒè·é›¢å¸¯ãƒ»åŒSurfaceã®ä¸­å¤®å€¤ã€‚<br>
</small>
""", unsafe_allow_html=True)

# ===== â‘¢ å…¬é–‹ç”¨JSONï¼šæ‰‹å…¥åŠ›â†’AR100æ¡ç”¨ã§æ›¸ãå‡ºã— =====
st.markdown("## â‘¢ å…¬é–‹ç”¨JSONï¼ˆæ‰‹å…¥åŠ› â†’ AR100å¾—ç‚¹ã§æ›¸ãå‡ºã—ï¼‰")

with st.expander("ğŸ“ å…¬é–‹ãƒ¡ã‚¿å…¥åŠ›", expanded=True):
    # â€» é–‹å‚¬æ—¥ã¯ã‚µã‚¤ãƒˆå´ã§å¿…è¦ã«ãªã‚‹ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä»Šæ—¥ã‚’å…¥ã‚Œã¦ãŠã
    PUB_DATE   = st.date_input("é–‹å‚¬æ—¥ï¼ˆã‚µã‚¤ãƒˆè¡¨ç¤ºã«ä½¿ç”¨ï¼‰", value=pd.Timestamp.today().date(), key="pub_date2")
    FILE_NAME  = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆ.json çœç•¥å¯ï¼‰", value="rikeiba_picks", key="pub_fname2")
    RACE_NAME  = st.text_input("ãƒ¬ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šç§‹è¯è³(G1), å¯Œå£«S(G2)ï¼‰", key="pub_rname2")
    SURFACE_TX = st.radio("é¦¬å ´", ["èŠ", "ãƒ€ãƒ¼ãƒˆ"], horizontal=True, key="pub_surface2")
    DIST_M     = st.number_input("è·é›¢ [m]", min_value=1000, max_value=3600, value=2000, step=100, key="pub_dist2")
    # ä»»æ„ï¼šé¦¬å ´çŠ¶æ…‹ã¯ç©ºã§ã‚‚OK
    GOING_TX   = st.selectbox("é¦¬å ´çŠ¶æ…‹ï¼ˆä»»æ„ï¼‰", ["", "è‰¯", "ç¨é‡", "é‡", "ä¸è‰¯"], index=1, key="pub_going2")
    # ä»»æ„ï¼šrace_idï¼ˆç©ºã§ã‚‚OKï¼‰
    RACE_ID_TX = st.text_input("ãƒ¬ãƒ¼ã‚¹IDï¼ˆä»»æ„ãƒ»ç©ºã§å¯ï¼‰", value="", key="pub_rid2")

# ä¸Šä½6é ­ã‚’ AR100 ã§æ›¸ãå‡ºã—ï¼ˆâ— ã€‡ â–² â–³ â–³ â–³ï¼‰
MARKS6 = ["â—", "ã€‡", "â–²", "â–³", "â–³", "â–³"]

btn = st.button("ğŸ“¤ JSONã‚’æ›¸ãå‡ºã™ï¼ˆAR100ã§å¾—ç‚¹å‡ºåŠ›ï¼‰", use_container_width=True)
if btn:
    import os, re, json
    from datetime import datetime

    # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    problems = []
    if not str(RACE_NAME).strip():
        problems.append("ãƒ¬ãƒ¼ã‚¹åãŒæœªå…¥åŠ›ã§ã™ã€‚")
    if not DIST_M:
        problems.append("è·é›¢[m]ãŒæœªå…¥åŠ›ã§ã™ã€‚")
    if problems:
        st.error(" / ".join(problems))
        st.stop()

    # ãƒ•ã‚¡ã‚¤ãƒ«åæ•´å½¢
    fname = str(FILE_NAME).strip()
    if not fname:
        fname = "rikeiba_picks"
    # åŠè§’ãƒ»å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã«å¯„ã›ã‚‹
    fname = re.sub(r"[^\w\-\.\(\)]+", "_", fname)
    if not fname.lower().endswith(".json"):
        fname += ".json"

    # ä¸Šä½6é ­ï¼ˆ_dfdisp ã¯ä¸Šã®é›†è¨ˆã§ä½œã£ã¦ã‚ã‚‹æƒ³å®šï¼‰
    if '_dfdisp' not in globals() or _dfdisp.empty:
        st.error("å‡ºèµ°è¡¨ã®é›†è¨ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ_dfdisp ãŒç©ºï¼‰ã€‚å…ˆã«Excelã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
        st.stop()

    top = _dfdisp[['é¦¬å','AR100']].head(6).copy()
    if top.empty:
        st.error("ä¸Šä½6é ­ã®æŠ½å‡ºã«å¤±æ•—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºï¼‰ã€‚")
        st.stop()

    # picks ã‚’ AR100 æ¡ç”¨ã§ä½œæˆ
    picks = []
    for i in range(len(top)):
        row = top.iloc[i]
        picks.append({
            "horse": str(row['é¦¬å']),
            "mark": MARKS6[i],
            # â† ã“ã“ãŒãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒã‚¤ãƒ³ãƒˆï¼šscore ã« AR100 ã‚’æ¡ç”¨ï¼ˆå°æ•°1æ¡ï¼‰
            "score": round(float(row['AR100']), 1) if pd.notna(row['AR100']) else None
        })

    # track ã¯ã€ŒèŠ / ãƒ€ãƒ¼ãƒˆã€ã®ã¿ï¼ˆã‚µã‚¤ãƒˆå´ã¯ 'ãƒ€' ã‚’å«ã‚ã°ãƒ€ãƒ¼ãƒˆã¨åˆ¤å®šã§ãã‚‹ï¼‰
    track_text = "èŠ" if SURFACE_TX == "èŠ" else "ãƒ€ãƒ¼ãƒˆ"

    # å˜æ—¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚µã‚¤ãƒˆã¯å˜æ—¥/ç´¯ç©ã©ã¡ã‚‰ã‚‚è‡ªå‹•å¯¾å¿œï¼‰
    payload = {
        "date": str(PUB_DATE),           # ä¾‹: "2025-10-20"
        "brand": "Rikeiba",
        "races": [{
            "race_id": RACE_ID_TX.strip() or None,
            "race_name": RACE_NAME.strip(),
            "track": track_text,         # ä¾‹: "èŠ" or "ãƒ€ãƒ¼ãƒˆ"
            "distance": int(DIST_M),
            "going": GOING_TX or "",
            "picks": picks
        }]
    }

    # ä¿å­˜
    os.makedirs("public_exports", exist_ok=True)
    out_path = os.path.join("public_exports", fname)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    st.success(f"JSONã‚’æ›¸ãå‡ºã—ã¾ã—ãŸ: {out_path}")
    st.code(json.dumps(payload, ensure_ascii=False, indent=2), language="json")
    st.caption("â€» ãã®ã¾ã¾ commit & push ã™ã‚Œã°ã€Actions â†’ Netlify ã§ã‚µã‚¤ãƒˆã«åæ˜ ã•ã‚Œã¾ã™ã€‚")
